{"pages":[],"posts":[{"title":"CMSC 5724 Data Mining and Knowledge Discovery - Lecture 01","text":"Classification Decision Trees Generalization Theorem Proof Classification 这里的 U 跟 X 一样，是各个 attribute : A1… Ad 的 Cartesian product（笛卡尔乘积），同时也是 instance space（实例空间）；Y 则是 label space（标签空间）。作为 attribute 的 Ai 如下图所示就是 age、education、occupation，他们共同组成了实例空间。而 loan default 就是 label，yes / no 就是 -1 / 1，组成了标签空间。 此时也可以定义 ( instance, label ) - pair（pair有时也叫Object），即 ( x, y ) ∈ X × Y，这里的 x 是向量，即样本空间的一行，如下图所示就是 e.g. age = 28、education = high school、occupation = self-employed。X 跟 Y 的笛卡尔乘积代表了所有的可能性，也就是 D，这在后面会讲到。 其实究其原因，就是我们需要去将这个 D 进行分类，因为 D 的真正分类我们是不知道的，我们就需要通过样本，通过训练集，通过模型，去预测实际的情况。D 也就类似于一个真理，这个真理只有上帝知道，我们能做的就是去通过各种方法找到真理（也有可能只是逼近）。 由于我们只知道向量x，也就是上图中的每一行，为了知道 D，也就是真理，也就是所有的可能性，我们需要 classifier，它通常也叫 hypothesis 或者 concept，定义为 h : X -&gt; Y。它可以理解为一个 function，将 X 中的每一个元素映射到 Y 中，就像是用 Y 的 -1 / 1 给它们打上了标签。这个过程通常是潜藏的（latent），我们看不见的。 根据上述，我们定义 h : classifier，则我们预测 label 的结果是 h(x)。但这是有可能有错误的，我们将预测的错误结果写作 err_D(h)，我们要做的就是尽可能最小化 err_D(h)。 为了更好地预测 D，我们设置了训练集 S。同时我们希望我们生成的 h 的 err_D(h) 尽可能地小。然而这个 err_D(h) 你只能尽可能让它小，你是控制不了的，你能控制的是 err_S(h)，即样本误差。 分子为 objects 的数量，这里的 objects 是一个样本对，即一个样本对应一个标签，它属于样本集 S，同时 s.t.（subject to） 代表受后面的约束，该约束条件为预测的结果跟实际结果不一致，即 h(x) ≠ y。 Q &amp; AQ : “independently” is so important because errS(h) will be close enough to errD(h)? A : related to large number law 老师表示下节课会谈 Q：why could we minimize err_D(h) just given training set? why not err_S(h) A : both of them should be considered, but err_S(h) may be secondary. Q : can we think that D has all the answers we need for (x, y) and that’s why we are trying to get close to it. A : yes, that’s exactly. Decision TreeHunt’s algorithm形式上，我们将决策树 T 定义为完整的二叉树，其中 每个叶节点都带有一个类标签：yes / no 每个内部节点 u : 有两个子节点 有一个 attribute Au 的 predicate —— Pu（也可以理解为一个判断语句） 当 classification 使用决策树 T 时：给定一个 向量x，我们会对每个节点 u 做如下事情 如果 u 是一个叶子节点，返回 u 的 label 如果 向量x 满足 Pu，那么去到左子节点，否则，去到右子节点 我们回到上面那个例子 从根节点开始，遇到的每一个判断条件，即 predicate，走向不同的分叉，最后得到 yes / no。 T ：决策树 u ：T 里的节点 S ：训练集 S(u) 定义： 如果 u 是根节点，那么 S(u) 为 S 假设 S(u) 已经被定义，同时假设 u 是一个有着 v1、v2作为孩子节点的内部节点 S(v1) 定义为满足 S 的对象集 S(v2) 则为 S(u) \\ S(v1)，意为在剩下的子集中查找 我们说S(u) split into S(v1) 和 S(v2) 定义 Pu 的方式决定于 Au 如果 Au 是 ordinal 有序的（学历也可认为是一种有序）：Pu 定义为 Au &gt;= a ∈ dom(Au) 如果 Au 是 nominal 名类的（如职业名称）：Pu 定义为 Au ∈ R ⊆ dom(Au)（R为dom(Au)的子集） Quality of a SplitGini index 基尼系数 其实就是 yes 的百分比与 no 的百分比，当 py = 1，pn = 0 的时候达到 maximum purity，此时 Gini = 0；当 py = 0.5，pn = 0.5时，达到 maximum imputity，此时 Gini = 0.5。Gini 的范围在0到0.5之间。 Principle : the “purer” a set is, the better. 同时，我们定义 split 的基尼系数 选择方法令 P 为全世界的一组人，假设没有其他属性 。 我们想学习一个分类器，给定一个随机的人，可以预测他/她是否喝酒。 为此，我们给定训练集 R⊆P。使用 Hunt’s 算法，我们从 R 中获得决策树 T。很容易看出 T 仅具有一个叶子。 令 c 为该叶子的类值（即c =yes / no）。 然后，对于 P 中的每个对象，我们将其类别值预测为c。 c 的哪个值对 P 有用？ 这应该与P中有多少人属于“yes”类，以及有多少人属于“no”类有关。 具体来说，让 为了使预测 P 中的随机人的错误最小化，如果 πy &gt; πn，则应将 c 设置为 yes，否则将其设置为 no。 举个例子，假设 πy= 0.7 和 πn= 0.3。 然后，如果将 c 设置为yes，我们将有70％的时间是正确的。 另一方面，如果将 c 设置为 no，则只有30％的时间是正确的。 但是，我们不知道 πy 和 πn 的实际值。 因此，我们依靠 R 来猜测这两个值之间的关系。 如果 R 中的 yes 对象多于 no 对象，我们猜 πy&gt;πn，因此将 c 设置为 yes； 否则，我们将 c 设置为 no。 这正是 Hunt’s 算法的作用。 如何确保我们获得良好的猜测？ 显然，我们需要 R 的数量大。 这非常直观：如果没有足够的训练数据，则没有办法建立可靠的决策树。 在这种情况下，训练数据没有统计意义。 Generalization Theorem 假设决策树 h 使用 b 个 bits 来表达（因为别人不知道你的决策树是如何编码的，所以必须根据要满足的某些编码约定来表示它，编码方案说明了你的所有约定，以便其他任何人都可以查看该方案并跟你完全相同的方式对任何 h 进行编码，b 取决于你的编码方式，可以看做是上界的长度，可以取任意数字，但是不能是无限数字，因为这样机器无法输出） 我们给定一个概率大小为0到1之间的值 δ，比如是 0.1%（失败），那么它至少以 (1 - δ) = 99.9%（成功）的概率保持以下的不等式： 这里的err_D(h) 即 generalization error 是我需要去约束的，我们使用能够控制的 err_S(h) 来约束。显然，D 的总量是比 S 要大的，因为 D 代表的是实际、是全部，S 只是我们找的训练集，因此要做好训练集不够完整的准备，有可能约束不了 err_D(h)，需要加第二项来弥补缺少的信息。 假设 err_S(h) 为0，b 为 2 × 10的6次方，S 为 10的6次方，显然这个训练集 S 太小，会导致后面加的值大于 1，然而 Generalization Theorem 不应该大于1（？）。 如果要使用很大数量的 b 也可以，前提是你的 S 要足够大，如下例 通过选择对的决策树，与足够大的 S，会有很小的 err_S(h)，那么你也保证了非常好的 err_D(h)，这就是你该如何运用 Generalization Theorem。 就如同图里的分类器也就是绿色线圈住的部分，是正确的”1“，其余是”-1“，可以看出这个分类器很复杂，你能做的就是通过大量的数据集，去尽可能贴近这个分类器，描述它的轮廓，你不可能完美的复现这个分类器，你能做的就是尽可能减少你的划分的红方框与绿色线的距离，也就是误差。 Proof of the Generalization TheoremHoeffding Bounds p 是我们想要估计得值，Xi 为独立的随机值，可能是1或0，我们设 t 为 Xi 的和，t 可以表达有多少 X 是1，那么 t/n 就是 p 的估计值。我们令 t/n &gt; p + ε，这里的 ε 就需要是很小的值，因为如果很大的话 t/n 会更大。但是如果 ε 很小的话，n 就会非常大。这意味着如果你想要保证只有 1% 的误差，那么你可能需要 10000 的样本。 Hoeffding Bounds 的用途就是，你需要多少的样本来保证你的准确率，它会给你的训练集一个上限值。 Union Bound 比如 A 的概率是 1%，B 的概率是 2%，那么 A 并 B 的概率会小于等于三，因为可能它们会有交集。 Proof令 H 作为可以被 b 个 bits 描述的分类器的集合，H ≤ 2的b次方。给定任意一个分类器 h ∈ H，设 S 为 O1, O2, …, On 训练集，并且 n = S。对于每一个 i ∈ [i, n] 的 Oi，定义 Xi = 1为误分类，那么 err_S(h) = 1/n ∑ Xi。 同时，因为每个 S 里的 object 都是独立地取自 D，那么对于每一个 i ∈ [1, n]， Pr[Xi = 1] = err_D(h)。 通过 Hoeffding bounds，我们可以得到 Pr[err_S(h) &lt; err_D(h) - ε] ≤ e^(-2na^2) 此时，我们要让右边尽可能小， 这里的 0.001 就是之前 Generalization Theorem 提到的 δ，至于为什么是这个形式先往下看。因为我们主要控制将这个不等式中的 ε，因此我们将 ε 抽出来，如下图。 总的来说，就是要通过 err_S(h) 来限定 err_D(h)，但是我们是有可能失败的，失败来自于 反过来说，h 发生失败的时候为当 这里是 ＞，因此我们说它错了。同时我们可以说它发生的概率 ≤ 0.001/2^b。我们可以通过 Union bounds 证明。举个例子，假设老师对每一个同学说你挂科的几率最多为 0.1%/人数总和，那么任意一个学生挂科的概率就为 0.1%，也就是 99.9% 的可能性没有人挂科。回到刚刚的不等式 当不等式左边发生的时候，就是错误发生的时候，我们要让他尽可能小，因此我们就要选择适合的 ε 来达到目的。 OtherRelationship between the error of a classifier h on S and the error of h on D. 将会在后面课程讨论。 另一种问法 Q: Can we somehow prove a connection between err_S(h) and err_D(h)? A: 由于 err_S(h) 是我们可以控制的，如果能够证明这两者之间的关系，那么就可以知道最大的误差，也就可以知道最优的决策树。那么如何证明呢，见下一讲的 Generalization Theorem。","link":"/2020/09/10/CMSC-5724-Data-Mining-and-Knowledge-Discovery-Classification-and-Decision-Trees/"},{"title":"CMSC 5724 Data Mining and Knowledge Discovery - Lecture 03","text":"An alternative way to describe the Bayes method Bayesian network An alternative way to describe the Bayes method接下来会提供一种代替方法来描述贝叶斯方法（朴素贝叶斯为例）。这个描述方法会阐明分类器集合 H 的实际含义，这可以使用 generalization theorem 来限定获得的分类器的 generalization error (err_D(h))。 这里的 placeholders 就是决策树中的一个参数，每一个 placeholder 可以用 64 位来表述，因为有 1/-1，因此是 2^64，那么就有 2^(64 * #placeholder)。 Bayesian network我们定义 Bayesian network 为一个 acyclic directed graph (DAG) 非循环有向图 G，G 满足以下条件： G 有 d + 1 个节点，包括一个表示 label 的的节点跟表示每个 attribute 的节点 G 有一个单独的根节点，该节点就是 label 如果 attribute u 没有到 attribute v1, …, vx（x ≥ 1可以为任意整数） 的路径，那么 u 和（v1, …, vx）是条件独立于 parents(u) e.g. A3 跟 (y, A5)，(y, A5) 是一个联合，因此只能以 A3 为 u，那么就是条件独立于 A3 的父节点 A1。 A4 跟 A5 条件独立于 A1，A2，但是不能说 A4 跟 A5 条件独立于 A1，因为 A4 跟 A5 都可以作为 u A3 跟 A2，当A3 为 u 时，它们条件独立于 A1；当 A2 为 u 时，它们条件独立于 y 定理给定 Bayesian network G 描述的条件独立性假设，我们有 e.g. 引理如果 A 跟 B 是条件独立于 C，那么 第一行是条件独立的定义，第二行可以证明 证明拓扑排序：对一个有向无环图(Directed Acyclic Graph简称DAG)G进行拓扑排序，是将G中所有顶点排成一个线性序列，使得图中任意一对顶点u和v，若边&lt;u,v&gt;∈E(G)，则u在线性序列中出现在v之前。通常，这样的线性序列称为满足拓扑次序(Topological Order)的序列，简称拓扑序列。 e.g. 第一行中 A2 出现在了 A4 跟 A5 后面，所以不是拓扑排序。 在不失一般性的情况下，假设 y，A1，… ，Ad 是 G 的拓扑顺序（即，从顶点 u 到 u 之前任何顶点的路径都不存在） 最后的等式使用了 G 隐含的条件独立属性和下面的事实 e.g.e.g. 1如果我们给定 Bayesian network 如图， 那么 Pr[30+, undergrad, programmer | y = -1] 会计算为 e.g. 2如果我们给定 Bayesian network 如图， 那么 Pr[30+, undergrad, programmer | y = -1] 会计算为","link":"/2020/09/24/CMSC-5724-Data-Mining-and-Knowledge-Discovery-Lecture-03/"},{"title":"CMSC 5724 Data Mining and Knowledge Discovery - Lecture 04","text":"Linear classification Perceptron Theorem Proof Linear classification 训练集 S 是线性可分离的，那么就存在一个 d 维的向量 w，x · w &gt; 0 表示 label 1，x · w &lt; 1 表示 label 2，x · w = 0 是 S 的分割平面。 God ： 选择了一个 linear classifier h* 选择了一个 X 上的数据集 D 我们： 从 D 中取一些样本集 从 D 取 x y = h*(x) 那么 God 会返回 (x, y) errD(h) = Pr[h(x) ≠ h*(x)] Note: errD(h*) = 0 linear classification 的目标是训练集中学习一个 classifier h，它在 D 上的错误尽可能小 Perceptron问题：给定一个线性分割的数据集 S，找一个分割平面，该分割平面由权重向量 w 定义，给定一个 linear classifier h，并且 errS(h) = 0。 引理：S （有限的）一定包含一个分割平面。 该算法从 w = (0, 0, … , 0) 开始，以迭代的方式进行。在每次迭代中，寻找一个 violation point p ∈ S 如果 p 为 label 1，那么如果 w · p ≤ 0，p 是一个 violation point 如果 p 为 label -1，那么如果 w · p ≥ 0，p 是一个 violation point 如果 p 存在，算法会按如下方式调整 w 如果 p 是 label 1，那么 w = w + p 如果 p 是 label -1，那么 w = w - p 当没有 violation point 的时候，算法会停止 现在我们分析 Perceptron 的迭代次数，给定一个向量 v = (v1, … , vd)，我们定义它的长度为 对于任意 v1，v2，有 定义一个单位向量 u，|u| = 1。 可见 p · u 为 p 距离分割平面的距离。 定义： S ：样本集 π*：有最大边界的分割平面 γ：π* 的边界 u*：π* 的法向 R：S 的半径，也就是距离分割平面最远的点的距离 w0 = 0 wi：第 i 次调整以后的 w 分割平面的边界由 w 决定 TheoremPerceptron 会在 w 最多 (R / γ)² 次调整后停止。 Proof证明最终结论我们先假定下面两个引理是正确的，后面再证明 我们令 k 为最大调整次数，由引理1我们可以得到， 由引理2我们可以得到， 于是可以推得， 证明引理1 证明引理2","link":"/2020/10/09/CMSC-5724-Data-Mining-and-Knowledge-Discovery-Lecture-04/"},{"title":"CMSC 5724 Data Mining and Knowledge Discovery - Lecture 05","text":"Review VC-dimension VC-dimension of Extended Linear Classifies VC-Based Generalization Theorem Review 这里的 H 为 classifier 的总数，那么如果 H 趋于无限呢？那么就需要 Linear classification 我们可以看出来第一节课讲的 Generalization Theorem 受限于需要知道 H 的大小，那么要如何摆脱，就需要 VC-dimension VC-dimension我们定义 Example 在二维空间内，对3个点的分割方式如下 Definition这里就可以引出 VC-dimension 的定义 对于一个假设空间 H，如果存在 m 个数据样本能够被假设空间H中的函数按所有可能的 2^h 种形式分开 ，则称假设空间 H 能够把 m 个数据样本打散（shatter）。 假设空间 H 的 VC-dimension 就是能打散的最大数据样本数目 m。 若对任意数目的数据样本都有函数能将它们 shatter，则假设空间 H 的 VC-dimension 为无穷大。 在二维空间的4个点中，因为4个点的情况下不能做到对分，所以二维实平面上所有线性划分构成的假设空间 H 的 VC-dimension 为3。 课上对 VC-dimension 的定义为 令 P 为 X 的子集。H 在 P 上的 VC-dimension 是可以被 H shatter 的最大子集 P’⊆P 的大小。 如果 VC-dimension 是 λ，我们写作 VC-dim(P, H) = λ。 VC-dimension of Extended Linear ClassifiesTheorem令 H 为 extended linear classifies 的集合，则 VC-dim(R^d, H) = d + 1 Example VC-Based Generalization TheoremSupport set of DD 的 support set 是在 d 维实数集上，从 D 中取出的有正概率（概率密度函数pdf &gt; 0）的集合。 Theorem 该理论对 H 的大小没有限制 如果一组 classifiers 是更强大的，即具有更大的 VC dimension，则学习起来会更加困难，因为需要更大的训练集。 对于 (extended) linear classifiers 的集合 H，训练集大小需要为 Ω(d) 以确保较小的 generalization error。当 d 大时，这就会成为问题。实际上，在某些情况下，我们甚至可能希望使用 d = 无限。 Recall Linear classifier 令 h 为由 d 维向量 w 定义的 linear classifier，我们说对于每个 p ∈ S，h 是 canonical，canonical 解释如下 w‘ 就是 h 的 canonical 形式，canonical h 是一个由 w 描述的 classifier，它需要满足以下几点 Margin-Based Generalization Theorem δ 满足 0 ＜ δ ≤ 1，对于每个用 w 描述的 canonical h （这里没有 err_S(h)，因为它总是为0） 这个 theorem 不依赖于维度 d 这个 theorem 为什么是 margin-based的？ 由 w 定义的分割平面的 margin 等于 1/|w|（后面课程会得出）。","link":"/2020/10/17/CMSC-5724-Data-Mining-and-Knowledge-Discovery-Lecture-05/"},{"title":"CMSC 5724 Data Mining and Knowledge Discovery - Lecture 06","text":"Review SVM problem Optimal Plane Review用基于VC-dimension bounds generalization error，所以任何分割平面都有很小的 generalization errordefinition plane 不够好，所以需要第二个theorem。第二个 theorem 表示，如果能够增加 margin，可以更好地限制 generalization error。 SVM problem找到一个有着最大 margin 的分割平面。一个用来解决这个问题的算法叫做 “support vector machine”(SVM) Optimal Plane 将过原点的分割平面复制两份，两个有相同的移动速度，一个向上平移，一个向下平移。一旦碰到一个 S 中的点就会停止。 distance between π- and π+ 因此，分割平面的 margin w · x = 0 是 1 / |w| Approximate Plane 一个分割平面是由 w · x = 0 定义的，目标是去找到一个好的 w。我们仍然是使用 Perceptron，但是我们不仅是当点落在分割平面错误的一侧时会调整 w，也会在点过于靠近平面时进行调整。 现在，我们假设给定一个任意的值 点 p 导致 violation 的几种情况如下所示： 它与平面 w · x = 0 的距离小于 ，无论它的 label 如何 p 的 label 是 1，但是 w · p &lt; 0 p 的 label 是 -1，但是 w · p &gt; 0 Margin Perceptron以 w = 0 开始迭代，在每次迭代中，找 violation porin p ∈ S。如果找到，那么算法会如下方式调整 w 如果 p 的 label 为1，则 w = w + p 否则，w = w - p What if γguess &gt; γ*？ 有可能会停止，甚至2倍于 γ*，也有可能，不过概率会再低50%，如果3倍，那么几乎不可能。 Theorem如果 γguess ≤ γ*，margin Perceptron 会终止在最多次迭代，并且返回一个 margin 至少为 的平面。 Corollary如果算法进行了 1 + R^2/γguess^2 次调整，那么 γguess &gt; γ* Proof 假设 γguess ≤ γ* 根据 theorem，算法应该调整次数 ≤ R^2 / γ*^2。 根据推论，就算你做了一些错误的做法，算法也不会永远迭代下去 halving-technique从一个很大的 γguess 开始，然后逐渐降低 Proof Incremental Algorithm 然后重复这个过程。 Theorem incremental algorithm 返回一个 margin 至少为 γ*/4 的分割平面。 它总共会进行 O(R^2/γ*^2) 次迭代 Proof","link":"/2020/10/23/CMSC-5724-Data-Mining-and-Knowledge-Discovery-Lecture-06/"},{"title":"CMSC 5713 IT Project Management - Lecture 02","text":"Lecture 01 Project Management Framework Project Management Framework Understand the organization. Involve the users. Manage the processes – PMBOK vs. PRINCE2 Understand the organization 项目不能孤立运行。 项目必须在广泛的组织环境中运作。 项目经理需要运用系统思维（system thinking）： 全面了解项目并了解其与大型组织的关系。 高级经理必须确保项目继续支持当前的业务需求（business needs） Strategy frame Strategy 是旨在实现特定目标的长期行动计划 Strategy 的组成 Strategy vs Tactics 战略与战术 “没有战术的战略是获得胜利的最慢途径。 没有策略的战术就是失败之前的喧嚣。” （中国军事家孙子） 战略定义了你的长期目标以及实现目标的计划。 战略为你提供了实现组织使命所需的途径。 战术要具体得多，而且通常会朝着较小的步骤和较短的时间范围发展。 它们涉及最佳做法，特定计划，资源等。它们也称为“倡议（initiatives）” Organization structure 组织结构是正式的决策框架，工作任务通过该框架进行划分，分组和协调。 组织结构图显示组织结构并显示职务，职权范围以及部门之间的关系 三个基本的组织结构 Functional 职能 将相似或相关的职业专业或过程归为财务，制造，营销等领域的熟悉标题。 Project 项目 or Divisional 部门 由独立生产的战略业务部门组成，每个部门均生产单个产品 。 Matrix 矩阵 职能结构与项目结构之间的中间立场 Origanization structure influences on projects 项目经理需要了解组织结构。 原因 组织结构会极大地影响他们的作用。 确定他们需要从哪些职能中获得哪些技能 Human Resources Frame 人力资源处理组织中管理人员的各个方面，其中包括： 选择 薪酬体系 培训和发展 绩效评估 公司轮换 项目经理需要知道他们在跟谁工作 有助于选择团队成员。 有助于了解如何激励他们的团队。 Q &amp; AQ: What does “walls : can lack customer orientation” means in P.13? A: Means that due to lack of interaction between depts, engineering may simply develop the products without understanding the needs from marketing (who may have better understanding of the customers) Q: What does “silos : create longer decisions cycles” means in P.13? A: Means that decisions need to go through senior executive levels rather than simply at managerial levels (e.g. project manager level) Q: Why project manager need to understand the organizational culture? A: So they can make up what lackings in organization culture within the team, e.g. if the organization has no strong member identify, may be project manager can encourage team to be proud to be part of the team Q: What does “means-ends orientation” means in P.22? A: Means-end orientation describes how much an organization focuses on processes versus end results Q: What does “open-systems focus” means in P.22? A: Open-systems focus describes how aware and reactive your business is to changes outsideyour corporate environment Q: What is the difference between “development” and “implementation” in project lifecycle in P.31? A: Development may include analysis, design and getting details of the concept, implementation including buidling and testing Q: What does “exception” means in “manage by exception” in P.56? A: manage by exception is making decision to fix things when it is not according to plan Q: In “tailor to suit the project environment”, what does “project environment” means? any examples? A: Project environment including the size, how the project fits into company strategy, the important of the project, the organization culture, etc. Q: Does “largely descriptive” in P.61 means “not necessary strictly follow”? And “highly prescriptive” means “must strictly follow”? A: Largely descriptive means at high level, e.g. you just need to take medicine to get better, highly prescriptive means at specific levels, e.g. you need to take this medicine 1 tablet 3 times a day. They both ask you to follow the best practice/advice. Q: What does “directing a project” means in P.58? A: Directing project means making overall decision about the project and exercising overall control. So, it is the duty of project board. While the day-to-day running of the project will be managed by PM.","link":"/2020/09/16/CMSC-5713-IT-Project-Management-Lecture-02/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 02","text":"Minimax Theorem Nash’s Theorem Pareto Optimal Cournot Model of Duopoly Games With no Saddle Points当没有 saddle points，如下图所示 此时就P1和P2就需要做出一些策略，称之为 Mixed Strategies。每个玩家都会分配一定的概率选择某个策略，如下图所示 此时， 当 P1 选择 A 的时候：A = 1/3(4) + 2/3(0) = 4/3 当 P1 选择 B 的时候：B = 1/3(-5) + 2/3(3) = 1/3 显然此时选择 A 的收益会更大一些。但是这只是只考虑 P1 的情况，实际上 P2 的选择也会对 P1 有影响，我们继续看。 P2 设使用 C 策略的概率为 x，使用 D 的概率为 ( 1 - x )。此时， 当 P1 选择 A 的时候：A = x(4) + (1-x)(0) = 4x 当 P1 选择 B 的时候：B = x(-5) + (1-x)(3) = 3 - 8x 4x = 3 - 8x，x = 1/4，P1 的 payoff 为 1。 反过来，P1 也有理性，P1 也会做出一些策略的选择 当 P2 选择 C 的时候：C = x(-4) + (1-x)(5) = 5 - 9x 当 P2 选择 D 的时候：D = x(0) + (1-x)(-3) = -3 + 3x 5 - 9x = -3 + 3x，x = 2/3，P2 的 payoff 为 -1。 Two-person Non-zero Sum Games玩家之间并非一定要对立对抗，也可以互相合作，如下图所示，矩阵中会有两个量，分别代表 P1 跟 P2 的损益。 但是这样会造成一个问题，Zero-sum game 的解决方法就是找 saddle points，但是 Non-zero sum game可能会有如下两种情况 左边这种情况，P1 跟 P2 都找到了对各自利益最大化的点，即（5，4）。但是右边这种情况会形成一个循环，P1 认为对自己利益最大的点 P2 不认可，反之 P1 也不认可。此时就需要一个新的解决方法，Nash’s Theorem Nash’s Theorem 在博弈论中，如果每个参与者都选择了自己的策略，并且没有玩家可以透过改变策略而其他参与者保持不变而获益，那么当前的策略选择的集合及其相应的结果构成了纳什均衡。——维基百科 根据 Nash’s Theorem 得出的游戏结果是一个 NEP（Nash Equilibrium Point纳什均衡点），任何 saddle point 也是 NEP。 The Prisoner’s Dilemma 囚徒困境Nash’s Theorem 著名的例子就是囚徒困境 S 为 Silence 保持沉默，C 为 confess 招供。由图可知，当两人都保持沉默时是对两人都是最好的情况，都只判刑2年。但是双方是不能交流的，如果自己保持沉默，对方招供了，那么对方将仅判1年，而自己将判10年。因此，两个犯人都会为了争取尽可能最小的判刑时间，而都选择招供，这个点就是 NEP。 Paroto Optimal 在没有使任何人境况变坏的前提下，使得至少一个人变得更好，这就是帕雷托改善。帕雷托最优的状态就是不可能再有更多的帕雷托改善的状态；换句话说，不可能在不使任何其他人受损的情况下再改善某些人的境况。 需要指出的是，帕雷托最优只是各种理想态标准中的“最低标准”。也就是说，一种状态如果尚未达到帕雷托最优，那么它一定是不理想的，因为还存在改进的余地，可以在不损害任何人的前提下使某一些人的福利得到提高。但是一种达到了帕雷托最优的状态并不一定真的很“理想”。——维基百科 如上图所示，（2，2）、（10，1）、（1，10）都是 Paroto Optimal。 Cournot Model of Duopoly古诺寡头竞争的假设 两个寡头厂商生产的产品是同质或无差别的； 每个厂商都根据对手的策略采取行动，并假定对手会继续这样做，据此来做出自己的决策； 为方便起见，假定每个厂商的边际成本为常数，并假设每个厂商的需求函数是线形的； 两个厂商都通过调整产量来实现各自利润的最大化； 两个厂商不存在任何正式的或非正式的串谋行为。 边际成本亦作增量成本，指的是每增产一单位的产品所造成的总成本的增量。 这个概念表明每一单位的产品的成本与总产品量有关。比如，仅生产一辆汽车的成本是极其巨大的，而生产第101辆汽车的成本就低得多，而生产第10000汽车的成本就更低了。 但是，考虑到机会成本，随着生产量的增加，边际成本可能会增加。——维基百科 博弈分析设 q1、q2 分别代表 企业1 和 企业2 生产的同质产品的产量，市场中该产品的总供给 Q = q1 + q2，令 P(Q) = a - Q 表示市场出清时的价格（假设这只是个简单的市场，固定需要 a 数量的产品） 设 企业i 生产 qi 的总成本 Ci(qi) = cqi，即企业不存在固定成本，且生产每单位产品的边际成本为常数 c （假设 c &lt; a）。 两个企业同时进行产量决策。假定产品是连续可分割的，由于产出不可能为负，因此，每一企业的战略空间可表示为 Si = [0, 无穷]，其中一个代表性战略 si 就是企业选择的产量 qi （qi ≥ 0）。假定企业的收益是其利润额 U。 为了找到 NEP，企业1 会采取解决方案 企业2 会采取解决方案 则企业i 最优化问题的一阶条件是 也即是，若产量组合 (q1, q2) 为纳什均衡，则企业的产量选择必须满足 联立，得 总结","link":"/2020/09/15/CMSC-5728-Decision-Analysis-Game-Theory-Lecture-02/"},{"title":"CMSC 5713 IT Project Management - Lecture 01","text":"Lecture 01 Introduction 项目生命周期 传统项目主要包括如图所示的四个阶段，其中值得一提的是 ”Lessons learned“，即从每次项目中所学到的东西，比如对类似项目的优化、总结改进等，其体现形式一般是文档。 三个目标 在项目中主要考虑三个目标，scope（规模）、time（时间）、cost（金钱）。这三者是相互关联的，比如当你固定了规模，想降低时间成本，那么你的金钱支出可能会上升，因为你可能需要花钱雇佣别人来干活，那么你就通过花费金钱减少了时间。同样的，当你想降低你的规模，那么你的时间与金钱也会相应降低，因为你不需要那么多的时间与金钱就能完成你的目标。 一个好的 project management，就是能够满足以下两点 满足这三个目标，scope、time与cost 让项目的赞助商满意 Project、Program、Portfolio Program是一组以协作方式进行管理的一组相关 projects，以从单独管理它们中获得收益和控制（？） Protofolio 是 program 或 project 和其他工作的集合，它们组合在一起以促进对该工作的有效管理以实现战略业务目标。 因此 program managers 跟 project managers 的职能也会有所不同，program manager 会同时管理多个近似的 projects，而project manager则相反。 Project Management Project management is “the application of knowledge, skills, tools and techniques to project activities to meet project requirements.” 项目管理是“将知识，技能，工具和技术应用于项目活动中以满足项目要求”。 这里的 skills 分为 soft skill 跟 hard skill，是一个 project manager 需要的技能。 Hard skills 包括产品知识，并知道如何使用各种项目管理工具和技术。 Soft skills 包括能够与各种类型的人一起工作。 Project Stakeholders Stakeholders are the people involved in or affected by project activities. 利益相关者是参与项目活动或受其影响的人员。 Stakeholders 包括： Project sponsor 项目发起人 Project manager 项目经理 Project team 项目团队 Support staff 支持人员 Users 用户 Suppliers 供应商 Opponents to the project 项目的反对人员 Project management framework PM Knowledge Areas 四个核心知识领域导致特定的项目目标（scope，time，cost，quality）。 五个促进 knowledge 的领域是实现项目目标的手段（human resources，communication，stakeholder management，risk and procurement management）。 一个 knowledge 领域（project integration management）会影响所有其他知识领域并受其影响。 所有知识领域都很重要！ PM Tools and Techniques 项目管理工具和技术可在项目管理的各个方面协助项目经理及其团队 特别的工具与技术包括： project charters（项目章程）、scope statement（范围声明）、WBS （scope） Gantt charts （甘特图）、network diagrams（网络图）、critical path analyses（关键路径分析）、criticial chain scheduling（关键链调度）（time） cost estimates（成本估算）、earned value management（挣值管理）（cost） Sample Gantt Chart 甘特图 Work Breakdown Structure (WBS)，工作分解结构，是将一个项目细部分解为交付标的导向的较小组成。工作分解结构是关键的项目交付标的，可将项目团队工作组编成为可管理的部分。项目管理知识体系〈PMBOK第五版〉所定义的工作分解结构：“由项目团队实施整个项目工作范围的阶层化分解，以达成项目目标，并产出必要的交付标的”。 工作分解结构的基本元素，可为资料、服务、产品或其组合，工作分解结构也为细部成本估算与管制提供了必要的框架，以及时程展开与管制的指引。——维基百科 Sample Network Diagram","link":"/2020/09/09/CMSC-5713-IT-Project-Management-Introduction/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 04","text":"The Cournot Duopoly Model Bertrand Model of Duopoly The Stackelberg Duopoly Model Sequential Bargaining Bank Runs War of Attrition The Cournot Duopoly Model 考虑两个公司通过生产商品来进行市场竞争 公司会选择生产多少件商品，比如公司 i 生产 qi 件商品 每件商品的生产成本是 c 我们让两公司总产品量为 Q = q1 + q2 市场价格取决于 Q 这里的 Q0 是市场饱和点，当两公司生产的产品越多，Q 越接近 Q0，那么整体的价格就会下降，P0 大概就是原始市场价格。 公司 i 的 payoff 是 公司1的解决方案公司1需要针对公司2的所有可能性进行考虑，最好的方式就是最大化 πi (q1, q2) 解得 我们需要通过以下式子检验这是最好的而不是最坏的 还需要检验 总体解决方案同样地，公司2的解决方案为 一个 pure strategy NE 是 (q1*, q2*)，里面是对对方最好的回应策略，所我们需要解决 解得 每个公司的 payoff 与垄断对比在垄断的情况下，payoff 是 解得 因为 qm &lt; 2qc*，每一个产品的价格在垄断里面会高于有竞争的市场，说明竞争对顾客有利 与 cartel 对比假设两家公司组成 cartel 并且同意以 q1 = q2 = qm* / 2 的价格生产，那么 payoff 就是 高于 Cournot 的 payoff，并且顾客的价格与垄断市场相同 这个结论是不稳定的，因为 cartel 最好的对策是 我们不是说 cartel 是不可能的，只是说 cartel 在 cournot 模型下不会发生 Bertrand Model of Duopoly 考虑一个不同的案例，公司1跟公司2生产的产品选择不同的价格 顾客对公司1的需求量为 这里 a 就是随便一个数。b 保证这两个 price 的变化和 demand 的需求变化不是1比1。 假设生产产品没有固定的成本，边际成本固定为 c，c &lt; a 两个公司同时行动 公司 i 的收益函数为 对于每个公司 i，(p1*, p2*) 是一个 NE，pi* 解得 对公司 i 的优化解决方案是 求解两个等式，解得 案例背景两家公司生产同一件产品，每家生产的边际成本都为 c，每个公司 i 需要选择销售价格 pi ∈ [0, 1]。假设需求曲线是线性的，即 Q(p) = 1 - p，其中 p = min(p1, p2)。换句话说，如果公司设定不一样的价格，那么所有的消费者都是从价格最低的公司那里购买商品。如果两家公司的价格相等，那么它们就会平分市场。我们令 qi 为对公司 i 的需求，那么可以表示为 假设两家公司都是理性的并且它们都想要最大化利益。公司 i 的获利表示为 最优反应分析如果公司 j 选择价格 pj，那么公司 i 的最优价格？ 如果公司 i 选择价格 pi = pj，那么它们平分市场 如果公司 i 的价格略低于 pj，那么它将得到整个市场 如果 pj &gt; c，使得公司 i 能够以略低于 pj 的价格得到正利润，并以此价格独享全部市场 如果 Pj = c，那么公司 i 就会选择 pi = pj，因为如果 pi 再小的话就会是负利润，再大的话也会失去市场 如果 pj &lt; c，那么公司 i 就会选择比 pj 高的价格，以保证没有顾客 纳什均衡点为 (c, c) The Stackelberg Duopoly Model 与 Cournot model 类似，有两家公司，每家公司确定产量和相同的市场价格 然而，有一个决定顺序：公司1先决定然后到公司2。我们假设每个公司想要最大化它的收益 解决方案 我们首先用反向归纳法，通过针对 q1 每个可能值，找到公司2的最佳响应 q2(q1)，来找到子博弈完美 NE。 假设公司1知道公司2的最佳响应，我们找到公司1的最佳响应 q1(q2)，从而找到这个游戏的 NE 公司2 的收益 通过求解下面的式子可以找到对 q1 选择的最佳响应 解得 公司1基于 q2(q1) 的最佳响应选择 q1，公司1的 payoff 通过 可以找到公司1 最大化它的收益在 NE 是 Note 先选择的优势：因为 q1* &gt; q2*，这意味着 在 Stackbelberg duopoly 下，商品的价格比 Cournot duopoly 低。 Sequential Bargaining 在第一阶段，P1 建议占用资源的 s1，将 1 - s1 留给 P2。 P2 接受（游戏结束）或拒绝（游戏继续）。 在第二阶段，P2 建议 P1 占用资源的 s2，将 1- s2 留给 P2。 P1 接受（游戏结束）或拒绝（游戏继续） 在第三阶段，P1 接受 s 的资源，P2 接受 1 - s 的资源，0 &lt; s &lt; 1 折扣因子 0 &lt; δ &lt; 1 解决方案如果到了第二阶段，P1 会选择 s2 或者接受 δs。P1 接受 s2 的条件是 s2 ≥ δs。P2 在第二阶段的考虑就是： 接受 1 - δs （通过提供 s2 = δs 给P1）或 接受 δ (1 - s) 在第三阶段 因为 1 - δ &gt; δ ( 1 - s)，P2 在第二阶段的最优选择是 s2* = δ，此时 P1 会接受。 接着再往上到第一阶段，此时 P1 会面临一个选择，P2 在第一阶段只有下面两种情况才会接受： 1 - s1 ≥ δ (1 - s2*) ，或 s1 ≤ 1 - δ (1 - s2*) P1 在第一阶段的考虑就是： 接受 1 - δ (1 - s2*) = 1 - δ (1 - δs)，或 接受 δs2* = (s^2)s 因为 1 - δ (1 - δs) &gt; (s^2)s，所以 P1 在第一阶段的最优选择就是 s1* = 1 - δ (1 - δs) 该游戏在第一阶段的解决方案就是 (s1*, 1 - s1*)，s1* = 1 - δ (1 - δs) 扩展为无限轮次 截断无限轮次为有限轮次，并应用有限轮次中的逻辑 如果达到第三阶段的游戏，则与第一阶段开始的游戏相同 让 SH 成为收益最高玩家 P1 可以在整个游戏的任何 backwards-induction 结果中实现 让 SH 作为 P1 在第三阶段的 payoff P1 在第一阶段的 payoff 可以作为一个方程 f(SH) 但是 SH 一直是第一阶段最高可能的 payoff，所以 f(SH) = SH。唯一 s 满足 f(s) = s 的是 解决方案就是，在第一阶段，P1 提供 给 P2，并且 P2 会接受。 Bank Runs 两个投资者各自将存款 D 存入银行 银行投资了一个项目。如果在项目到期前清算，则返回 2r，其中 D &gt; r &gt; D / 2。如果项目成熟，则返回 2R，其中 R &gt; D。 投资者可以在日期 1（项目成熟之前），或日期 2（项目成熟之后）退出。 该游戏的流程： 如果两个投资者都在日期 1 提款，每个都收到 r，游戏结束 如果只有一个人在日期 1 提款，则该投资者获得 D，另一个人获得 2r - D，游戏结束 如果两个人都在日期 2退出，则每个人都收到 R，游戏结束 如果只有一个人在日期 2 退出，该投资者获得 2R - D，另一个人获得 D，游戏结束 如果两个人均未在日期 2 退出，则银行向每个人返回 R，游戏结束 分析考虑日期 2，因为 R &gt; D（并且 2R - D &gt; R），”withdraw” strictly dominates “don’t”，我们有一个唯一的 NE，(R, R)。对于日期 1， 因为 r &lt; D（并且 2r - D &lt; r），我们有两个纯策略 NE，且第二个 NE 更优 both withdraw (r, r) both don’t withdraw (R, R) Tariffs and Imperfect Competition 定义两个国家 i = 1, 2，每个国家设定每个产品的关税比例 ti 一个公司生产的产品用于国内消费与国外出口 顾客可以从国内公司或国外公司购买产品 国家 i 的市场清算价格为 P(Q1) = a - Qi，Qi 是国家 i 的市场总量 在国家 i 的公司提供 hi(ei) 单位的产品给本地（国外）的市场。i.e. Qi = hi + ej 公司 i 的生产成本是 Ci(hi, ei) = c(hi + ei) 并且它支付 tjei 的税给国家 j 分析首先，政府同时选择税率 t1 跟 t2。公司观察到税率，同时决定 (h1, e1) 与 (h2, e2)。给公司跟政府 payoff： 公司 i 的盈利 政府 i 的盈利 这里的 1/2Qi^2 代表在国家 i 的居民的利益，首先 Qi 代表产品总量，我们需要更多的产品以让价格下降 第二阶段假设政府选择了 t1 跟 t2，如果 ( h1*, e1*, h2*, e2* ) 是公司 1 和公司 2 的NE，公司 i 需要解决 重新安排分为两个分离的优化 假设 ej* ≤ a - c 并且 hj* ≤ a -c - tj，我们有 解得 第一阶段在第一阶段，政府 i 的 payoff 是 因为 hi* (ei*) 是 ti (tj) 的一个函数 如果 (t1*, t2*) 是一个 NE，每个政府解 解决该最优问题，我们可以得到 这是一个对每个政府的 dominant strategy 代入 ti*，我们得到 小结在子博弈完美结果中，每个市场的总数量为 5( a - c ) / 9。但是如果两个政府合作，它们会寻求社会最优点，并解决以下优化问题 解得 t1* = t2* = 0（无税率），并且总数量为 2 (a - c ) / 3。 因此，对于以上游戏，我们有一个唯一 NE，并且是社会效率低下的。 War of Attrition 有两个参与者争夺价值 v 的资源 每个玩家的策略是选择持续时间 ti，其中 ti ∈ [0, 无限]。 三个假设 竞赛成本仅与持续时间有关 持续时间最长的玩家将获得所有资源 每个玩家支付的费用与选择最短持续时间成正比，即，持续时间最短的玩家会在它的持续时间结束后退出，那么另一位玩家赢得了所有的资源，但由于在该持续时间内有竞争力，所以会损失该部分的钱 两位玩家的 payoff： 一个纯策略 NE 是 t1* = v / c 并且 t2* = 0，给定 这是一个 NE 因为对于 P1： 给定 对于 P2： 那么 其他 NE 第二个纯策略 NE 是：t1* = 0 并且 t2* = v / c。给定 与前面的分析类似 混合策略 NE 参考教科书","link":"/2020/09/30/CMSC-5728-Decision-Analysis-Game-Theory-Lecture-04/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 01","text":"Lecture 01 Introduction Two-person Zero-sum Games零和博弈 所有博弈方的利益之和为零或一个常数，即一方有所得，其他方必有所失。也可以说：自己的幸福是建立在他人的痛苦之上的，二者的大小完全相等，因而双方都想尽一切办法以实现“损人利己”。零和博弈的例子有赌博、期货和选举等。——维基百科 重点是如果P1赢，就意味着P2输，双方是一种对立关系。比如你和朋友吃一张披萨，你多吃一口，他就少吃一口。 我们拿围棋举例说明。为了简单起见，我们就假定X、Y两人下围棋，该X走下一步棋了，他有3种可选的下法，分别是x1、x2、x3，Y也有三种，分别是y1、y2和y3。 在围棋中，一方的所得必然是另一方所失，因此这是一个零和游戏，比如说X走了x1这步棋后，在盘面上的胜率所得是7点，那么Y的胜率损失也是7点。在这样的情形下，我们只要考虑X的胜率变化即可，因为X赢了多少就是Y输的。 我们知道当X采用了x1、x2、 x3之中的一种策略后，Y也有相应的三种策略y1、y2和y3,因此它们的组合就有9种结果，就构成了一个3x3的矩阵。在每一个组合中， X有一个胜率的变化，这些变化就构成了矩阵的值：(我们假设这9个结果对应了X能获得的9个分数。) 在这个矩阵中，你可以看到，当X采用x1策略时，他最好的情况是碰上Y采用y1，这时X的胜率就增加7点，但是如果Y是一个高手，他采用了y3策略应对，你可以看到X的胜率就小了10点。因此X如果考虑到Y可能的应对策略，他就应该知道，x1其实不能算是一步好棋。 相比之下，采用x2策略就稳健得多，因为无论Y如何应对，他至少可以让自己的胜率增加一点。至于x3，因为有胜率减少一点的可能性，也没有x2好。因此，在制定策略时，如果我们不考虑对方的应对，显然x1是最好的，x2是最差的，但是考虑到对方应对的情况，可能最好和最差的策略就反过来了。 具体到博弈这件事，特别是计算机博弈，最通用的策略是，“在对方给我们造成最糟糕的局面里，选择相对最好的”。也就是说，我们要把x1、x2和x3所有策略算出来后，在可能得到的最糟糕结果中进行比较，具体到这个问题，就是-10、 1和-1这三个结果，然后排序找到最大的，那就是1。在计算机算法中，这种策略被称为最小值中的最大值策略（min-max algorithm）。 接下来我们站在Y的角度来看看他的选择。我们假设他先行棋后，胜率变化的矩阵还是上面那个，当然负值表示他的胜率上升。如果他选择y3，虽然可能让胜率增加10点（对应-10那个值），但是，也冒着损失4点的风险。相比之下，y2的选择就比较好，因为最不济也不过让胜率损失1个点。类似的，可以分析出来y1也不如y2。 回到课件。对P1来说，数值越大得分越高，B的每一列都比C小，则称C ”dominates“ B；对P2来说，数值越大失分越多，B的每一列都比C小，则称B ”dominates“ C。 Saddle Points鞍点然而这种分析方法并不是每次都有用，因为会出现如下的情况。 此时P2没有一列能够”dominates“另一列，因此需要另一种解决方法，鞍点。 Salle Point鞍点：非局部极值点的驻点。——维基百科 此时，对P1来说，鞍点是所有最小值（-1、2、-16）中的最大值；对P2来说，是所有最大值（12、2、3）中的最小值。 在两方的博弈中，大家其实就是在寻找马鞍点这样一个平衡点， 因为大家都知道，如果自己走出了这个平衡点，试图扩大自己的利益，对方就会有反制手段，让自己的利益受损。 当然并非所有的问题里这样的平衡点都在。比如前面那个对弈的胜率矩阵，如果里面的数字都是些很大的正值，也就是说X的实力可以秒杀Y，采用什么策略可能Y都无法应对，这种情况其实不用担心。但是当参与方的水平势均力敌，不相上下时，很多时候寻找最小值中的最大值才是最好的出路，或者说其实双方必然会被锁死在那个平衡点上。 这里需要补充一点的是，我们其实作了一个隐含的假定，就是双方的策略都是透明公开的，即双方都知道对方所有可能的选择，也就是说一切是阳谋，不是阴谋。双方所不知道的，无非是对方最终采取的策略。 其次，双方都足够理性（rantional），能够判断出该采用什么策略。","link":"/2020/09/08/CMSC5728-Decision-Analysis-Game-Theory-Introduction/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 07","text":"Coalition Multi-armed Bandit (MAB) CoalitionSuperadditive Games 如果两个不相交的 coalitions 的联合至少值其成员之和，那么这个 coalitional game G = (N, v) 是 superadditive。N 是玩家的集合，v 是 payoff。 voting-game 是 superadditive 如果 G 是 superadditive，那么 grand coalition 总是会取得最高可能的 payoff 如果下面成立，那么 G = (V, v) 是 additive (或 inessential) Constant-Sum Games 如果 grand coalition 的价值等于对 N 进行划分的任何两个 coalitions 的价值之和，则 G 为常数和 每个 additive game 都是 constant-sum game 但是，并非每个 constant-sum game 都是 additive Convex Games 对于所有 S, T ⊆ N，G 是 convex 以上定义等同于 N 中所有 i 和所有 S ⊆ T ⊆ N - {i} 回想一下 superadditive game 的定义，我们可以得出每个 super-additive game 都是 convex game Simple Coalitional Games G = (N, v) 对于每个 coaliton S 是简单的 v(S) = 1(win) 或 v(S) = 0 (lose) 通常添加一个要求，如果 S 获胜，则 S 的所有超集也获胜 if v(S) = 1，所有 S ⊆ T，v(T) = 1 这并不意味着 super-additivity 一个 voting game G 需要 50% 投票来通过 两个 coalitions S 和 T，每个都有 50% 的 N v(S) = 1，v(T) = 1 但是 v(S ∪ T) ≠ 2 Proper-Simple Games 如果 G 同时是 simple 跟 constant-sum game，那么它是 proper simple game 如果 S 是 winning coalition，那么 N - S 是 losing coalition v(S) + v(N - S) = 1，所以如果 v(S) = 1，那么 v(N - S) = 0 games 分类的关系 Terminology Feasible payoff set = {所有 payoff 分布不超过 grand coalition} = {(x1, …, xn) | x1 + … + xn} ≤ v(N) Pre-imputation set P = {feasible pay 是有效的，即分配 grand coalition 的全部价值} = {(x1, …, xn) | x1 + … + xn} = v(N) Imputation set C = {P中的 payoff，其中每个 agent 至少获得一个通过单独行动（即，形成一个单人 coalition）将获得的 payoff} = {(x1, …, xn) ∈ P: i ∈ N, xi ≥ v({i})} Fairness &amp; Symmetry 如果 agent i 和 j，总是对其他 agent 的每次coalition 贡献相同的金额，则他们是可互换的 v(S ∪ {i}) = v(S ∪ {j}) 在 payoff 的公平分配中，可互换的 agent 应该获得相同的 payment xi = xj Dummy Players 如果 agent i 对任何 coalition 的贡献恰好是 i 可以单独实现的量，则 i 是 dummy player 对于 i ∉ S，v(S ∪ {i}) = v(S) + v({i}) 在公平分配的 payoff 中，dummy player 应该获得与自己获得的金额相等的 payment 如果 i 是 dummy player 并且 (x1, …, xn) 是 payoff，那么 xi = v({i}) Additivity 令 G1 = (N, v1) 并且 G2 = (N, V2) 作为有相同 agents 的两个 coalitonal games 考虑 G = (N, v1 + v2) (v1 + v2)(S) = v1(S) + v2(S) 在公平分配 G 的 payoff 时，agent 应该得到他们在两个单独 games 中得到的总和 对于每个 player i， Shapley ValuesTheorem给定一个 coalitional game (N, v)，有一个唯一的 pre-imputation φ(N, v) 满足 symmetry、dummy play 和 additivity。对于每个 player i，i 的 φ(N, v) 份额是 Example voting game 政党 A、B、C、D 分别有 45、25、15、15 个代表 51 个投票以通过一百万账单 每个 coalition ≥ 51 个人都可以获得价值 1，其他 coalition 获得价值 0 两个 agents i 和 j 可以互换的含义 v(S ∪ {i}) = v(S ∪ {j}) B 与 C 可互换 对于 ∅ 来说是 0，因为 B 或 C 加上 ∅ 都不够票数 对 {A} 来说是1，因为加上 A 后的票数大于51 对 {A, D} 来说是 0，因为 {A, D} 的票数已经大于51 同样地，B、D 是可互换的，C 与 D 也是 所以根据 fairness，B、C、D 应该有同样的 amount 类似地，φB = φC = φD = 1/6 如果 A 获得 1/2，那么剩下 1/2 会分给 B、C 、D 它们是可互换的，所以会平分：1/6 所以金钱的分配如下 A 获得 1/2 * 100M = 50M B、C、D 每个获得 1/6 * 100m Multi-armed Bandit (MAB) MAB 是进行在线决策的框架属性 在线：学习者没有完整的数据集，但是在学习者做出决策时（通常是学习者要优化一些目标功能）数据会正在进来 顺序决策：学习者做出决策，然后观察系统，然后做出下一个决策 通常来说，学习者需要在随机环境下做出顺序决策 总体框架 时隙系统：t = 1, 2, …, T 每个时间点学习者有 N ∈ N+ 个选择 在每个时间点 t ∈ [T] 学习者在 N 中选择一个选择（在其他情况下，学习者可以在一个时间点做出 K ≤ N 个选择） 在做决策的时候，学习者可以依赖过往经验或观察结果 在学习者做出决策后，学习者可以观察到 reward （可能是随机的） 目标：做出正确决定以最大化总 reward 以及其他可能的目标，如满足某些约束 Sequential decisioncommon features 一个人只能使用过去的观察（或反馈）和过去的动作在当前时间步长做出决定 过去的观察/反馈必须与将来的奖励有一定关系（存在相关性）。 如果没有相关性，就很难学习任何东西。 关键：我们应该从过去中学习，以便改善未来（例如，优化我们的累积奖励） 我们可以利用我们的统计知识来估计决策/选择的良好程度。 我们必须考虑 “exploration” 和 “exploitation” Different variation 我们可以使用随机变量来表示它，例如 IID。 这是 MAB 中的主要方法 我们可以用随机过程来表示它，这叫 Markov chain。 这是 RL 中的主要方法 我们可以使用 adversary 来建模（一种任意反馈序列，但在某些特定方式上受限） Distinctions 完整信息模型 奖励你的决定，但对所有可能选择（例如，购买股票）的即时表现提供反馈 在完整信息模型中，我们可以回答 what-if 反馈在决定之前或者之后 受限反馈模型 反馈只在你的决定的表现情况 Multi-armed Bandit model N 个 arms 或选择 拉下 arm 的时候产生一个 reward 在每次时间点 t = 1, 2,…, T，拉一个 arm It ∈ {1,2,…,N} 我们观察到 reward rt ∈ R 我们的目标是最大化 Σrt 这是一个受限反馈模型，因为因为你只能观察到您拉出的手臂的奖励，而无法观察其他手臂的奖励 Exploitation &amp; Exploration 自然的选择是迄今为止最好的选择，例如，只选择那些已经选择的 arm，然后选择给予学习者最高奖励的arm 这称为 “exploitation”。 尝试使用一些未开发的 arm 或很少拉的 arm，因为它们可能会提供更高的奖励 这称为 “exploration”","link":"/2020/10/20/CMSC-5728-Decision-Analysis-Game-Theory-Lecture-07/"},{"title":"ELCT 5830 Network and Web Programming 05","text":"DOM Event Propagation Event Delegation class, this, Function.bind() DOM 文档对象模型（Document Object Model，缩写DOM），是W3C组织推荐的处理可扩展置标语言的标准编程接口。 The “document” object 浏览器从 HTML 文档在内存中构建树形数据结构 该树通过 window.document暴露出来 window.document充当浏览器客户区中显示的视图的内存表现形式 通过window.document我们可以 访问和操作文档树 获取与文档相关的有用数据 cookies 文档 URL 最后修改日期 DOM 操作检索元素document.querySelector()返回与指定 CSS 选择器匹配的第一个元素，如果未找到匹配项，则返回 null。 12&lt;span class=foo&gt;ABC&lt;/span&gt;&lt;span class=foo&gt;XYZ&lt;/span&gt; 123let node = document.querySelector('.foo')if (node != null) console.log(node.textContent) // Output ABC document.querySelectorAll()返回一个静态（非实时） NodeList，其中包括与指定 CSS 选择器匹配的元素。 NodeList 对象是节点的集合，通常是由属性，如Node.childNodes 和 方法，如document.querySelectorAll 返回的。 NodeList 不是一个数组，是一个类似数组的对象(Like Array Object)。虽然 NodeList 不是一个数组，但是可以使用 forEach() 来迭代。你还可以使用 Array.from() 将其转换为数组。 12&lt;span class=foo&gt;ABC&lt;/span&gt;&lt;span class=foo&gt;XYZ&lt;/span&gt; 123456let nodes = document.querySelectorAll('.foo')for (let i = 0; i &lt; nodes.length; i++) console.log(nodes[i].text)// ORfor (let node of nodes) console.log(node.text) 从子树中检索元素假设我们要检索所有 “div” 元素和 #container 中包含的第一个 “a” 元素 12345678910// 方法1：搜索整个树两次let allDivs = document.querySelectorAll('#container div')let firstAnchor = document.querySelector('#container a')// 方法2：避免搜索整个树两次// 首先定位 #containerlet node = document.querySelector('#container')// 在 #container 下搜索子树let allDivs = node.querySelectorAll('div')let firstAnchor = node.querySelector('a') Other本文未包括的内容 遍历文档树 检索注释 在文档对象中其他检索元素的方法 getElementById() getElementsByTagName() 操作元素例如 HTML content, text content, attributes, value, CSS style HTML content123&lt;div id=\"foo\"&gt; ...&lt;/div&gt; 1234// Getlet htmlStr = document.querySelector('#foo').innerHTML// Setdocument.querySelector('#foo').innerHTML = htmlStr 分配给.innerHTML的值会被解析，如果该值包含 HTML 标记，则创建相应的树节点 Text content1234&lt;div id=\"foo\"&gt; &lt;b&gt;ABC&lt;/b&gt; &lt;i&gt;XYZ&lt;/i&gt;&lt;/div&gt; 12345678// Getlet textStr = document.querySelector('#foo').textContent // textStr is \"ABC XYZ\"// 仅返回 HTML 内容的文本部分，即没有标签内容// Setdocument.querySelector('#foo').textContent = textStr// textStr 中的所有特殊字符（例如 &lt;, &gt;, &amp;）将被编码，以便 textStr中每个字符都会出现 Attributes1&lt;a id=\"foo\" href=\"...\"&gt;...&lt;/a&gt; 1234567891011// Getlet ele = document.querySelector('#foo')let attrValue = ele.getAttribute('href')// Setlet ele = document.querySelector('#foo')ele.setAttribute('href', attrValue)// Removelet ele = document.querySelector('#foo')ele.removeAttribute('href') Value of Form Elements1&lt;input id=\"foo\" value=\"...\"&gt; 123456// Getlet value = document.querySelector('#foo').value// Property value 是一个 string// Setdocument.querySelector('#foo').value = newValue Checkbox States1&lt;input id=\"foo\" type=checkbox value=\"...\"&gt; 1234567// Getlet isChecked = document.querySelector('#foo').checked// checked 是一个 boolean 类型的值// Setdocument.querySelector('#foo').checked = true // checkeddocument.querySelector('#foo').checked = false // unchecked Style window.getComputedStyle() 返回一个只读对象，该对象表示应用于元素的最终样式 element.style 该对象表示内联样式 应该用于设置元素的样式，或检查通过 JavaScript 操作直接添加的样式 获取与设置 Styles123&lt;div id=foo style=\"width:100%\"&gt; ABC&lt;/div&gt; 123456789101112131415let foo = document.querySelector('#foo')let computed = window.getComputedStyle(foo)let inline = foo.style// computed.width 是 \"800px\" （即，实际的宽度值）// inline.width 是 \"100%\"// inline.height 是 \"\"（因为这个 inline 的 style 没有设置）// 为一个元素设置 stylesfoo.style.color = \"red\"foo.style.border = \"blue solid 1px\"// 如果一个 style 名字有连字符号（例如，font-size）// 使用数组语法（foo.style['font-size']// 或用驼峰表示法（foo.style.fontSize) 用 class 来表示样式12345678910111213// 我们可以通过增加或移除 classes 来改变样式let foo = document.querySelector('#foo')foo.classList.add(\"big\")foo.classList.remove(\"big\")// 如果 class \"big\" 已经被设置，那么就移除它，否则就增加它foo.classList.toggle(\"big\")// 如果 class \"big\" 已经被设置if (foo.classList.contains(\"big\")) { ...} 附加或插入内容12345&lt;div id=\"foo\"&gt; &lt;span&gt;Blah&lt;/span&gt;&lt;/div&gt;// 插入到上面&lt;a href=\"http://www.example.com\"&gt;Example&lt;/a&gt; 12345678// 创建 “a\" 元素let newEle = document.createElement('a')newEle.textContent = 'Example'newEle.setAttribute('href', 'http://www.example.com')// 将元素附加到 #foo 作为最后一个子节点let foo = document.querySelector('#foo')foo.appendChild(newEle) 使用element.insertBefore()将元素插入到最后一个位置以外的任何位置 另一种附加/插入/替换内容方式1234// 更新整个内容let foo = document.querySelector('#foo')foo.innerHTML = foo.innerHTML + '&lt;a href=\"http://www.example.com\"&gt;Example&lt;/a&gt;' 这个方法涉及更新.innerHTML属性 需要将内容构造为 HTML 字符串 更少的代码 依靠内置的解析器将 HTML 字符串转换为 DOM 对象（性能降低） 事件处理支持机制 当 事件X 发生时，调用 函数Y 事件X： 鼠标点击按钮 键入输入文本框 一个异步操作（如文件下载）完成 函数Y： 事件处理 事件监听 回调函数 概述1function myHandler (event) { ... } 创建函数为事件处理提供服务，有关事件的信息将作为第一个参数传递给事件处理程序。 12let ele = document.querySelector('#foo')ele.onclick = myHandler 注册事件处理程序以监听目标上所需事件 常见事件 Focus: focus, blur form: reset, submit, change view: resize, scroll keyboard: keydown, keypress.keyup mouse: click, dblclick, mouseenter, mouseout, … progress: load value change: input 注册事件处理123// 方法1let ele = document.querySelector('#foo')ele.onxxx = myHandler 将事件处理程序分配给目标节点 onxxx，onxxx 是节点名称，例如 click, focus 属性 onxxx 最多可以容纳一个事件处理程序 1234// 方法2&lt;div id=foo onxxx=\"myHander(event)\"&gt; ...&lt;/div&gt; 在目标元素的 HTML 标签内，通过属性 onxxx 指定事件处理程序 分配给属性 onxxx 的值以以下方式附加到相应树节点的属性 onxxx 上 1234// Let \"ele\" represents the corresponding tree nodeele.onxxx = function (event) { myHandler(event) // Value of the attribute}","link":"/2020/10/12/ELCT-5830-Network-and-Web-Programming-05/"},{"title":"CMSC 5724 Data Mining and Knowledge Discovery - Lecture 02","text":"Bayesian Classification Naive Bayes Classification Bayesian Classification 我们给定一个在实例空间的点 p ，即 p ∈ X。如果 Pr[y = -1 | x = p] ≥ Pr[y = 1 | x = p]，那么上帝的分类器（最准确的分类器）hgod(p) = -1，反之则 = 1。（这里的Pr[y = -1 | x = p]，右上角举例，先限定 x 是年龄为30，学历为 undergraduate，再看标签为 -1 的比例）。但是即便是最准确的分类器，也是有误差的，我们称为 err_D(hgod)，也叫贝叶斯误差。 当每个属性都有一个较小的域时，即该属性只有少量可能的值时，贝叶斯分类最有效。 当属性具有较大的域时，我们可以通过离散化来减小其域大小。比如之前的例子，我们可以让年龄划分为更小的域，{20 +，30 +，40 +，50 +}，其中“ 20+”对应于区间 [20、29]，“ 30+”至[30、39]，并且以此类推。 Bayes’ Theorem 给定一个实例 x，我们预测它的标签是 -1 当且仅当 应用 Bayes’ Theorem，我们得到 类似的 由于分母都一样，我们只需要比较分子哪个比较大，即 Bayesian Classification 通过训练集来估计上面两式的大小 Naive Bayes Classification 以标签 y = -1 的前提下，30+、udergrad、programmer的概率举例，假设 age 跟 education 是独立于 occupation 跟 class label 的，那么 Q &amp; AQ: Is number of attribute relevant to err_D(h)? A: Yes and no. No 的原因是在 Generalization Theorem 里看不到 attribute，Yes 是因为它会间接影响 b Q：We choose the number 0.001/(2^b) because if Pr(h fails) &lt;= 0.001/(2^b) &lt;= 0.001/|H| then Pr(none from H fails) &gt;= 0.999 A：exactly Q：Could you show how union bound conclude number δ/2b（这个问题本身是错的，但是也有可取之处） A：","link":"/2020/09/17/CMSC-5724-Data-Mining-and-Knowledge-Discovery-Lecture-02/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 03","text":"Game Trees (Extensive form) The Cournot Duopoly Model The Stackelberg Duopoly Model Game Trees (Extensive form) 顺序游玩 玩家轮流做出选择 玩家可以选择之前的选择 游戏被表示成一棵树 每个非叶子节点代表一些玩家的决策 边缘代表可用选择 可以被转换为矩阵 matrix game （Normal form） 每个飞叶子节点代表一些玩家的决策点 边缘代表可用的选择（可能是无限的） 完美信息 Perfect information 没有任何两个参与人同时行动，并且所有后行动者能确切知道前行动者选择了什么行动 例子：房地产开发博弈假设 由于规模经济的限定，在一个特定地区只能容纳一个开发商去进行房地产开发 A 和 B 两个开发商决定是否对该地区的房地产进行开发 A 具有优先行动的权力，B在观察到A的 行动后决定自己的下一步行动 支付向量 如果 A 和 B 同时选择开发，激烈的竞争将导致两败俱伤，每家仅获得 -3 的支付 如果 A 和 B 两家中的一家选择开发，另外一家选择不开发，则开发的一家将获得 1 的支付，而不开发的一家获得 0 的支付 如果两家都不开发，则其支付分别为 0 房地产开发博弈的扩展式表述（博弈树） 含义 A 选择开发，B 在观察到 A 选择开发后仍然选择开发，则 A 和 B 的支付分别为 -3，-3 A 选择开发，B 在观察到 A 选择开发后选择不开发，则 A 获得 1 的支付，而 B 获得 0的支付 A 选择不开发，B 在观察到 A 选择不开发后，选择开发，则 A 的支付为 0，B 的支付为 1 A 选择不开发，B 在观察到 A 选择不开发后，仍然选择不开发，则 A 和 B 的支付分别为 0 和 0 问题：如何求解上述博弈（完美信息动态）的 NE ? 把扩展式转化为标准式，然后利用 NE 的定义寻找 NE B 的纯战略 不论 A 开发还是不开发，B 开发，记为（开发，开发）。含义：不论博弈到达 B 的哪一个信息集，B 都选择开发 A 开发，B 开发；A 不开发，B 不开发，记为（开发，不开发）。含义：博弈到达左信息集（A开发），B 选择开发；博弈到达右信息集（A 不开发)，B 选择不开发 A 开发，B 不开发；A 不开发，B 开发，记为（不开发，开发）。含义：博弈到达左信息集（A 开发），B 选择不开发；博弈到达右信息集（A 不开发），B 选择开发 不论 A 开发还是不开发，B 都不开发，记为（不开发，不开发）。含义：不论博弈到达 B 的哪一个信息集，B 都选择不开发 B （开，开） （开，不开） （不开，开） （不开，不开） A 开发 -3, -3 -3, -3 1, 0 1, 0 不开发 0, 1 0, 0 0, 1 0, 0 不可置信战略 Incredible threat B 在决策前可以清楚地看到 A 的选择 如果 A 选择开发，B 的最优选择是不开发 如果 A 选择不开发，B 的最优选择是开发，而不是不开发 因此（不开发，不开发）是 B 的不可置信战略，该 NE 不合理 剔除完美信息博弈不合理 NE 的逻辑：只有当一个站略规定的行动规则在所有可能的情况下都是最优时，它才是一个合理的战略 在完美信息动态博弈中，会有部分 NE 包含不可置信的战略，精练 NE 的思路就是利用扩展式表述剔除包含不可置信战略 NE 子博弈精练 Subgame perfect（SPNE） （完美信息博弈中），每一个决策结及其后续结构成一个子博弈 博弈本身构成自身的一个子博弈 房地产开发博弈中的三个子博弈 自身 两个只有开发商 B 决策的单人博弈 SPNE：一个战略组合 它是整个博弈的 NE 其相关行动规则在每一子博弈上都是 NE 在所有可能的路径上到达均衡，不仅包括均衡路径（equilibrium path，一个特 定的NE决定的原博弈树上唯一的一条路径），还包括所有其他在不同子博弈中 的分支路径（非均衡路径） 求解 SPNE 思路 寻找整个博弈的可能NE(可以通过把扩展式转化为标准式表述实现) 在每一个子博弈(博弈的分支路径)验证可能的NE是否达到均衡 剔除只在某些分支路径达到均衡，而在另一些分支路径无法达到均衡的NE 案例分析(不开发，(开发，开发))： 在子博弈B1中，B的最优战略是不开发 在子博弈B2中，B的最优战略是开发 NE (不开发，(开发，开发))中B的 均衡战略(开发，开发)在子博弈B2上构成NE，但在子博弈B1上不构成NE 上述NE不是一个SPNE (开发，(不开发，开发))： B的均衡战略(不开发，开发)(如果A 开发，B不开发；如果A不开发，B开发) 无论在子博弈B1还是子博弈B2上都构成 NE 上述NE构成开发博弈唯一的SPNE “A开发，B不开发”成为开发博弈唯一合理的均衡结果 小结：SPNE 与 NE 的关系 SPNE是NE在完美信息博弈中的精练(refinement)解概念(引入参与人行动的先后顺序) SPNE一定是NE 但NE不一定都是SPNE 逆向归纳法 backward induction一种求解 SPNE 的简便方法 以开发博弈为例 首先在从最后一个决策结开始的子博弈 中寻找该博弈中的NE 只有B决策的单人博弈B1和B2 B1:已知A选择开发，B的最优选择是不 开发(开发的支付-3低于不开发的支付 0) 剔除不可置信的战略开发 B2:已知A选择不开发，B的最优选择是 开发(开发的支付1高于不开发的支付0) 剔除不可置信的战略不开发 在博弈进入B的决策阶段，B的最优行动 规则: (不开发，开发) 含义:如果A在第一阶段选择开发，B在 第二阶段选择不开发;如果A在第一阶段 选择不开发，B在第二阶段选择开发 一个新的博弈:第二阶段单人博弈的均 衡战略和初始结A的行动共同构成 只有开发商A在决策 A在第一阶段的最优战略:开发(A选择开发的支付1高于选择不开发的支付0) A在第一阶段理性预期到B在第二阶段的 最优战略，A第一阶段的最优选择是开发 开发博弈的扩展式表述及其SPNE结果","link":"/2020/09/22/CMSC-5728-Decision-Analysis-Game-Theory-Lecture-03/"},{"title":"ELCT 5830 Network and Web Programming - Exercises 01","text":"Lecture 01 JavaScript Basics Q1.Let X be a variable that stores a positive integer in which its last digitis not zero. Write a segment of JS code to reverse its digits. e.g. if X = 13579; X should become 97531. 12345678function reverseDigit(x) { let res = 0; while (x != 0) { res = res * 10 + x % 10; x = (x / 10) | 0; } return res;} Q2.Write a segment of JS code to make a DEEP copy of array “fruits” and sort the elements in the cloned arrayby “id” in ascending order. 12345const fruits = [ {id:43, title:'Apple', price:12}, {id:21, title:'Banana', price:5}, {id:13, title:'Orange', price:8}, {id:55, title:'Mango', price:15}, {id:44, title:'Grape', price:10}, {id:6, title:'Peach', price: 14}] 12345678910function deepCopy(obj) { return JSON.parse(JSON.stringify(obj));}function compare(a, b) { return a.id - b.id;}newArr = deepCopy(fruits).sort(compare)console.log(newArr) Q3.Implement a function named “length” that takes one parameter and perform the followings: If the parameter is an array, return its length (# of elements) If the parameter is a string, return its length (# of characters) If the parameter is a number that is neither NaN nor Infinity, return thenumber of digits in its integer portion. (e.g., for -456.99, the integerportion is -456, so return 3) Otherwise, return 0 123456789function length(x) { if (Array.isArray(x) || typeof(x) == \"string\") { return x.length; } else if (!isNaN(x) &amp;&amp; x != Infinity) { return Math.abs(x | 0).toString().length; } else { return 0; }} Q4.Implement a function that takes one parameter and returns true if the parameter is an object containing the following three properties: ‘foo’, ‘bar’, and ‘foo-bar’. Otherwise, the function returns false. Please note that the value of the properties can be undefined. 123456function checkProperty(obj) { if (!obj) { return false; } return obj.hasOwnProperty(\"foo\") &amp;&amp; obj.hasOwnProperty(\"bar\") &amp;&amp; obj.hasOwnProperty(\"foo-bar\");}","link":"/2020/09/12/ELCT-5830-Network-and-Web-Programming-Exercises-01/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 05","text":"The Iterated prisoners’ Dilemma Subgame Perfection The Iterated prisoners’ Dilemma考虑以下囚徒困境，合作（C）、背叛（D） 我们令这个博弈只重复一次，那么就有两个阶段，同时我们从后往前分析。在第二阶段也就是最后一个阶段，由于没有下一步交互了，所以双方都会在这一阶段收到 payoff。我们选择最优反应 D，那么 （D， D）就是这个 subgame 的 NE。 注意对于每个玩家在整个博弈来看，纯策略集合是 S = {CC, CD, DC, DD}。但是我们只关注 subgame perfect NE，所以我们只考虑 {CD, DD}。 分析上面的博弈，整个博弈的 NE 是 (D, D) 。所以整个博弈的 subgame perfect NE 是选在两个阶段都选 D。 stationary strategy固定策略是在每个阶段选择动作的规则都相同的策略。但这并不意味着在每个阶段选择的动作都相同。 e.g. 在每个阶段选 C 在每个阶段选 D 如果其他玩家不选 D ，那么选 C；反之亦然 我们令 SC 为 “每个阶段选 C”，SD 为 “每个阶段选D”，那么它们的总 payoff 分别为 这就会带来一些困扰，因此引入 discount factor δ (0 &lt; δ &lt; 1)，于是总 payoff 就是 于是上面的两个 payoff 可以表示为 trigger strategy当一次背叛触发行为改变时，该策略称为触发策略 e.g. 我们令 SG 为”一开始合作，并一直合作到其他玩家背叛，然后在之后就一直背叛“ 如果两个玩家都采用 SG，那么 (SG, SG) 是 NE 吗（非正式分析）假设两个玩家被限制选择纯策略集 S = {SG, SC, SD} 假设 P1 决定使用 SC，那么 payoff 就是 同样，P2 选择 SC 的话结果也不会比 (SG, SG) 更好。 假设 P2 选择 SD，那么结果就是 计算 结果为 δ ≥ 1/2。所以当 δ ≥ 1/2 的时候，(SG, SG) 是一个 NE。 练习同样是囚徒困境，我们令它的纯策略集为 S1 = S2 = {SD, SC, ST, SA}，其中 ST 表示 “一开始合作，然后做其他玩家前一个阶段的同样的选择” SA 表示 “一开始背叛，然后做其他玩家前一个阶段的同样的选择” 那么 δ 需要什么条件才能满足 (ST, ST) 是 NE 呢？ Subgame Perfection如果两个玩家都采用了 trigger strategy SG，那么它是一个 subgame perfect NE 策略吗。 正式分析 因为这是一个无限迭代的博弈，因此在博弈的任何时候，博弈的未来（即 subgame）都等同于整个博弈。 可能的 subgame 可以分为以下四类 两个玩家都不选 D 双方都不选 D，因此会一直是合作，直到其他玩家背叛。(SG, SG) 是 subgame 的 NE，因为它是整个博弈的 NE。 两个玩家都选 D 两个玩家都选 D，因此会是一直背叛。(SD, SD) 是 subgame 的NE，因为它是整个博弈的 NE。 P1 在最后阶段选 D，P2 不选 在这个情况下， 因为 P2 之前选了 C，SG 会让 P1 选 C 并且 P2 选 D。总而言之就是 P1 会是 C, D, D…，而 P2 会是 D, D, D,…. 所以此 subgame 采用 (SG, SD) 但是 (SG, SD) 并不是 subgame 的 NE，因为 P1 通过选择 SD 获得了更好的 payoff P2 在最后阶段选 D，P1 不选 同上 因此，整个博弈的 NE，(SG, SG)，没有让每个玩家都在各个可能的 subgame 中都是 NE，所以并不是subgame perfect。 虽然 (SG, SG) 不是 subgame perfect NE，我们可以考虑下面类似的策略，即 subgame perfect NE 策略。 我们让 Sg 表示为“一开始合作，并一直合作到任一玩家背叛，然后在之后一直背叛”。这个原因是： P1 或 P2 在情况1跟情况2选择 (Sg, Sg)（对于情况2，实际上是 (SD, SD)） 情况3和情况4的 P1 和 P2 选择(SD, SD)。 进一步分析我们证明了 (SG, SG) 是整个博弈的 NE，这是假设策略集是有限的。是否可以允许更多的策略集呢？如果允许更多策略，(SG, SG) 仍然是 NE吗？ 如果我们限制自己达到 subgame perfect NE，那么我们首先需要 one-stage deviation principle。 定义如果两个玩家都无法通过在任何单个阶段单方面偏离其策略并随后返回制定策略来增加收益，则一对策略满足 one-stage deviation principle。 情况一 如果两个玩家都已经合作，那么 Sg 在此阶段会指定合作 如果在此阶段任何一个玩家改为 D，那么 Sg 就会指定一直使用 D。进行此更改的玩家的预期未来收益为，如果 δ ＞ 1/2，那么这个收益会小于持续合作的收益。因此玩家不会进行改变。 情况二 如果任何一个玩家在之前背叛了，那么 Sg 会在此阶段为两个玩家都指定背叛 如果在此阶段任一玩家改为 C，那么 Sg 会仍然在之后指定为 D。进行此更改的玩家预期未来收益为 ， 如果 δ &lt; 1，那么这个收益会小于一直为 D 所获得的收益。 因此，(Sg, Sg) 在 1/2 &lt; δ &lt; 1 的情况下会满足 one-stage deviation condition。 定理一对策略是 discounted 重复博弈的 subgame perfect NE，前提是当且仅当满足 one-stage deviation condition","link":"/2020/10/07/CMSC-5728-Decision-Analysis-Game-Theory-Lecture-05/"},{"title":"CMSC 5728 Decision Analysis &amp; Game Theory - Lecture 06","text":"English Auction First-Price Sealed Auctions Dutch Auction Second Price Auction, Sealed-Bid Auctions 拍卖是出售没有固定市场的固定商品的一种方式 竞拍者进行投标 提议为商品支付各种金额的费用 商品出售给出价最大的投标人 一些拍卖都是信息不完整（可以建模为所谓的贝叶斯游戏） 私人价值拍卖 每个投标人可能具有不同的投标人价值（BV），即该投标人对商品的价值 投标人的BV是他/她的私人信息，而其他人不知道 根据规则对拍卖进行分类 English Dutch First price sealed bid 拍卖中可能出现的问题是共谋（欺诈目的的秘密协议） 一群竞拍者不会互相竞价来保持低价格 伪造（虚假）出价的竞拍人提高价格（因此拍卖师获利） 如果存在共谋，则均衡分析不再有效 English Auction规则 拍卖师向小组征集开标价 任何想要竞标的人都应至少喊出高于之前 c 的新价（例如 c = 1 美元） 出价一直持续到只剩最后一个人 最高出价者以等于其最终出价的价格出售了该物品 对于每个竞拍者 i，我们令 vi 为 i 对该物品的估价（私人信息） Bi 为 i 的最终出价 如果 i 胜出，那么 i 的收益为 πi = vi - Bi，其他竞拍者的收益为0 NE 每个竞拍者 i 参与竞拍直到出价到达 vi，然后退出 最高的出价者 i 以价格 Bi &lt; vi 获得物品，并且 πi = vi - Bi &gt; 0 Bi 是接近于第二高出价者的估价 对于每个竞拍者 j ≠ i，πj = 0，即收益都为0 假设竞拍者 j 偏离了预定计划 如果 j 通过提前退出偏离计划，那么 j 的收益为0，不会更好 如果 j 通过出价 Bj &gt; vj 偏离计划，那么 j 的收益为 vj - Bj &lt; 0，更坏 缺点 如果竞拍者们的估值范围很大，即不同竞拍者之间的估值差别很大，那么最高和第二高的估值之间的差异可能会很大 因此竞拍成功者可能能够以比其估价低得多的价格获得它 我们令 n 为竞拍者的数量 n 越高，最高和第二高出价之间就越可能接近 因此竞拍成功者越有可能以接近它的估价值拍得该物品 First-Price Sealed Auctions 竞拍者在纸条上写下对物品的出价，然后交给拍卖人 拍卖人打开出价，找到最高出价者 最高出价者以自己的出价拍下物品 竞拍成功者的收益为 BV - 支付的价格 其他拍卖者收益为0 假设 有 n 个拍卖者 每个拍卖者有一个私人的估价 vi 关于 vi 的概率分布是常识 我们假设 vi 是均匀分布在 [0, 100] 上的 令 Bi 为 i 的出价 令 πi 为 i 的收益 NE 首先我们寻找最优的竞价策略 如果 Bi ≥ vi，那么 πi ≤ 0 所以我们假设理性：Bi &lt; vi 因此，我们有 如果 Bi ≠ max{ Bj }，πi = 0 如果 Bi = max{ Bj }，πi = vi - Bi 竞拍者 i 需要出低于 vi 多少的价？ Bi 越小 i 越不可能拍得该物品 i 拍得该物品，收益就越大 n = 2 情况 假设你的 BV 是 v，出价是 B 令 x 为其他竞拍者的 BV，ax 为其出价，0 &lt; a &lt; 1 你不知道 x 跟 a 你的预期收益是： E ( π ) = P ( your bid is higher ) * ( v - B ) + P ( your bid is lower ) * 0 如果 x 是均匀分布在 [0, 100] 上，那么概率密度函数是 f ( x ) = 1 / 100，0 ≤ x ≤ 100 所以 E ( π ) = B ( v - B ) / ( 100a ) 如果你想要最大程度提高预期收益（因此你对金钱的估价是风险中立的），那么最高出价就是 当 v - 2B = 0 =&gt; B = v / 2 时，最大化 所以出该物品 1 / 2 的价格是值得的 n ≥ 2 情况 当 n 增加的时候，B -&gt; v。因为竞争加剧导致出价接近估价。 Dutch Auction 规则 拍卖人以高价开始 拍卖人逐渐降低价格，直到一些买家喊“mine” 第一位喊”mine”的买家以拍卖人的叫价拍得商品 成功拍卖的竞拍者获利 BV - prices 其他竞拍者获利0 理论上来讲 Dutch Auction 等同于 first-price sealed auctions 对象以最高价出售给出价者 竞拍者必须在不知道其他任何出价者的出价的情况下选择出价 最佳出价策略是相同的 Second Price Sealed Auction 竞拍者在纸条上写下对物品的出价，然后交给拍卖人 拍卖人打开出价，找到最高出价者 最高出价者以第二高出价者的出价拍下物品 竞拍成功者的收益为 BV - 支付的价格 其他拍卖者收益为0 均衡竞价策略 以真实价值来出价是一种弱势策略，此属性也称为拍卖的真实或策略验证性。为了说明这一点，需要证明出价过高或过低不会增加收益，还可能降低收益。 令 V 为对物品的估价，X 为其他竞拍者出的最高价 令 SV 作为出价 V 的策略，πv 为使用 SV 的收益 令 SB 为出价 B ≠ V 的策略，πB 为使用 SB 的收益 B、V、 X 有 3! = 6 种排列顺序 本 auction 是近乎等于 English auctions的 物品给最高出价者 价格接近于第二高的 BV Coalitional Games with Transferable Utility 给定一组 agents，一个联盟博弈定义了每组（或多个联盟）agents 能为自己做得收益 不关注 agents 个人在联盟中的选择 他们如何合作 Transferable utility 假设：联盟的收益可以在其成员之间自由重新分配 只要系统中存在用于交换的通用货币，就满足 这意味着可以为每个联盟分配一个值作为其收益 一个 coalitional games with transferable utility 通常由 G = (N, v) 来表示 N = {1, 2, …, n} 是一个 players 的有限集 v: 2^N -&gt; R 将与每个联盟 S ⊆ N 关联的实值收益 v(S)，联盟成员可以在相互之间分配 coalitional game theory 通常需要回答两个问题 coalition 的形式是什么？ “the grand coalition”（所有的 agents） coalition 如何将 payoff 分给它的成员？ 上一个问题就取决于这个问题中的正确选择 Example: A Voting Game 一个包含4个政党的100名代表的议会 A：45名、B：25名、C：15名、D：15名 他们投票决定是否通过1亿元的支出单（以及每方控制的金额） 需要大于等于51的票才能通过 如果没有通过，每个政党都获得0 一个 voting game 会包括 一个 agents 集合 N 一个胜者 coalition W ⊆ 2^N 在这个例子中，所有 coalition 有足够的票通过这个支出单 对每个 coalition S ∈ W，v(S) = 1 对每个 coalition S ∉ W，v(S) = 0 Superadditive Games 如果 coalition 的价值至少等于其两个不相交成员的价值之和，那么一个 coalitional game G = (N, v) 是 superadditive voting game 例子是 superadditive 如果 G 是 superadditive，那么 grand coalition 会是最可能的 payoff 如果以下成立，那么 G = (N, v) 是 superadditive","link":"/2020/10/14/CMSC-5728-Decision-Analysis-Game-Theory-Lecture-06/"},{"title":"Homebrew安装Docker Desktop for Mac及MacOS10.15版本的镜像配置方法","text":"搜索Docker的包brew search docker 12345678910111213141516==&gt; Formulaedocker docker-machine-driver-vmwaredocker-clean docker-machine-driver-vultrdocker-completion docker-machine-driver-xhyvedocker-compose docker-machine-nfsdocker-compose-completion docker-machine-parallelsdocker-credential-helper docker-slimdocker-credential-helper-ecr docker-squashdocker-gen docker-swarmdocker-ls docker2acidocker-machine dockerizedocker-machine-completion lazydockerdocker-machine-driver-hyperkit==&gt; Casksdocker-edge homebrew/cask/docker-toolboxhomebrew/cask/docker Casks下的才是Docker Desktop可以使用命令查看信息 brew cask info docker 12345678910111213141516171819==&gt; Tapping homebrew/caskCloning into '/usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask'...remote: Enumerating objects: 2, done.remote: Counting objects: 100% (2/2), done.remote: Compressing objects: 100% (2/2), done.remote: Total 452026 (delta 0), reused 0 (delta 0), pack-reused 452024Receiving objects: 100% (452026/452026), 205.09 MiB | 5.08 MiB/s, done.Resolving deltas: 100% (320412/320412), done.Tapped 1 command and 3619 casks (3,736 files, 219.9MB).docker: 2.3.0.3,45519 (auto_updates)https://www.docker.com/community-editionNot installedFrom: https://github.com/Homebrew/homebrew-cask/blob/HEAD/Casks/docker.rb==&gt; NamesDocker DesktopDocker Community EditionDocker CE==&gt; ArtifactsDocker.app (App) 安装Dockerbrew cask install docker 123456==&gt; Downloading https://desktop.docker.com/mac/stable/45519/Docker.dmg######################################################################## 100.0%==&gt; Verifying SHA-256 checksum for Cask 'docker'.==&gt; Installing Cask docker==&gt; Moving App 'Docker.app' to '/Applications/Docker.app'.🍺 docker was successfully installed! 安装完成，现在可以从启动台里找到Docker.app了，打开后会在顶部菜单栏里出现 docker logo，并且会弹出登陆界面，登陆后就能正常使用Docker Desktop for Mac了。 # 获取阿里云镜像地址 前往 [阿里云官网](https://www.aliyun.com/?spm=5176.10695662.amxosvpfn.2.26766339P4o7tB) 使用淘宝/支付宝/注册登录，进入控制台 搜索容器镜像服务 获取镜像地址 配置Docker点击小齿轮图形进入设置页面，在Docker Engine配置镜像地址 修改括号内的内容即可：&quot;registry-mirrors&quot;: [ &quot;https://********.mirror.aliyuncs.com&quot; ] 运行Docker在终端输入 docker run hello-world 12Hello from Docker!This message shows that your installation appears to be working correctly.","link":"/2020/07/18/Homebrew%E5%AE%89%E8%A3%85Docker-Desktop-for-Mac%E5%8F%8AMacOS10-15%E7%89%88%E6%9C%AC%E7%9A%84%E9%95%9C%E5%83%8F%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/"},{"title":"Eight rounding modes八种舍入模式解析","text":"读《Effective Java》Item 60 : Avoid float and double if exact answers are required的时候，结尾写BigDecimal的优点时写道 Using BigDecimal has the added advantage that it gives you full control over rounding, letting you select from eight rounding modes whenever an operation that entails rounding is performed. 这里的eight rounding modes（八种舍入模式）第一次看到，查找资料记录一下 ROUND_UP舍入远离零的舍入模式。在丢弃非零部分之前始终增加数字（始终对非零舍弃部分前面的数字加1）。即，有小数位的情况下，去掉小数位，正数+1，负数-1 ROUND_DOWN接近零的舍入模式。在丢弃某部分之前始终不增加数字（从不对舍弃部分前面的数字加1，即截短）。即，有小数位的情况下，去掉小数位，整数位不变 ROUND_CEILING接近正无穷大的舍入模式。如果 BigDecimal 为正，则舍入行为与 ROUND_UP 相同；如果为负，则舍入行为与 ROUND_DOWN 相同。即，有小数位的情况下，去掉小数位，正数+1，负数不变! ROUND_FLOOR接近负无穷大的舍入模式。如果 BigDecimal 为正，则舍入行为与 ROUND_DOWN 相同；如果为负，则舍入行为与 ROUND_UP 相同。即，有小数位的情况下，去掉小数位，正数不变，负数-1 ROUND_HALF_UP向“最接近的”数字舍入，如果与两个相邻数字的距离相等，则为向上舍入的舍入模式。如果舍弃部分 &gt;= 0.5，则舍入行为与 ROUND_UP 相同；否则舍入行为与 ROUND_DOWN 相同。即，四舍五入 ROUND_HALF_DOWN向“最接近的”数字舍入，如果与两个相邻数字的距离相等，则为上舍入的舍入模式。如果舍弃部分 &gt; 0.5，则舍入行为与 ROUND_UP 相同；否则舍入行为与 ROUND_DOWN 相同。即，五舍六入 ROUND_HALF_EVEN向“最接近的”数字舍入，如果与两个相邻数字的距离相等，则向相邻的偶数舍入。如果舍弃部分左边的数字为奇数，则舍入行为与 ROUND_HALF_UP 相同；如果为偶数，则舍入行为与 ROUND_HALF_DOWN 相同。注意，在重复进行一系列计算时，此舍入模式可以将累加错误减到最小。此舍入模式也称为“银行家舍入法”，主要在美国使用。四舍六入，五分两种情况。如果前一位为奇数，则入位，否则舍去。以下例子为保留小数点1位，那么这种舍入方式下的结果。1.15&gt;1.2 1.25&gt;1.2 ROUND_UNNECESSARY断言请求的操作具有精确的结果，因此不需要舍入。如果对获得精确结果的操作指定此舍入模式，则抛出ArithmeticException。 参考http://www.blogjava.net/wangzc2001/archive/2010/12/17/340988.html","link":"/2020/07/23/Eight-rounding-modes%E5%85%AB%E7%A7%8D%E8%88%8D%E5%85%A5%E6%A8%A1%E5%BC%8F%E8%A7%A3%E6%9E%90/"},{"title":"Homebrew常用命令","text":"本文为Homebrew的常用命令，便于日常操作 常用命令12345678910brew help 查看帮助brew install &lt;package name&gt; 安装软件包brew uninstall &lt;package name&gt; 卸载软件包brew list [--versions] 列出已安装的软件包(包括版本)brew search &lt;package name&gt; 查找软件包brew info &lt;package name&gt; 查看软件包信息brew update 更新brewbrew outdated 列出过时的软件包（已安装但不是最新版本）brew upgrade [&lt;package name&gt;] 更新过时的软件包（不指定软件包表示更新全部）brew doctor 检查brew运行状态 常用软件1234567brew install wgetbrew install curlbrew install opensslbrew install fish #安装fish shellbrew install git-flow #安装git-flowbrew install python #安装python Homebrew-CaskHomebrew-Cask 是 Homebrew 的一个扩展。它能够优雅、简单、快速的安装和管理 macOS 图形界面程序，比如Google Chrome 和Dropbox等等。官网 https://caskroom.github.io/。 Cask 必装的理由有图形界面的软件可以直接在 App Stroe 中下载更新，为啥还需要 Cask 呢？因为有的很好用的免费 Mac 软件并没有选择在 App Store 上架，对于没有上架的软件我们只能是通过搜索找到官网然后在下载安装包，这样不够优雅也不方便管理，而使用 Cask 可以通过一行命令就搞定安装了，还可以统一更新升级所有的软件，实现从非 App Store 途径安装的软件的统一管理。Cask 从软件官方网站下载软件包，然后在后台安装并将 .app 移动到 Applications。通过 Cask 安装的软件也会在 Lanuchpad 显示，跟从 App Store 安装的软件没啥区别。对于那些收费的软件，用 Cask 安装只是比普通安装方法节省了时间和步骤，没啥其他的区别。 Cask 常用命令123456brew cask -help 查看帮助brew cask install &lt;software name&gt; 安装软件brew cask uninstall &lt;software name&gt; 卸载软件brew cask search &lt;software name&gt; 搜索软件brew cask info &lt;software name&gt; 查看软件相关信息brew cask list 列出通过 Homebrew-Cask 安装的包 经过测试，虽然 -help 是未知命令，但是仍然可查看 Cask 的命令，其他帮助命令（如 brew cask -h 和 brew cask –help）好像都不行。还有其他的命令就不一一介绍了，其他命令可以通过brew cask -help查看。 Cask 常用软件12345678910111213141516brew cask install iterm2 #安装iTerm 2brew cask install launchrocket #管理软件后台服务brew cask install google-chrome #安装Chromebrew cask install the-unarchiver #解压软件brew cask install alfred #效率软件brew cask install qq #腾讯QQbrew cask install evernote #云笔记软件brew cask install sublime-text #文本编辑器brew cask install skitch #ervernote配套的截图软件brew cask install dropbox #文件同步软件brew cask install zotero #网页收藏与文献管理软件brew cask install anki #记忆软件brew cask install virtualbox #虚拟机，可以装个Windowsbrew cask install self-control #避免分心的软件brew cask install vlc #视频软件brew cask install appcleaner #应用清理 Quick Look 系列12345brew cask install qlcolorcode #预览脚本时自动代码配色brew cask install qlstephen #预览未知拓展名的纯文本文件brew cask install qlmarkdown #预览Markdown文件brew cask install quicklook-json #预览JSON文件brew cask install quicklook-csv #预览CSV文件 Homebrew-Cask 是一个开源项目，其详细信息可以看其开源项目介绍，所支持的软件列表在这里：https://github.com/caskroom/homebrew-cask/tree/master/Casks 。如果觉得管理软件在后台运行的服务很麻烦，可以装个LaunchRocket，这也是个开源项目。关于 Quick Look 的介绍可以看这篇文章加强你的「一指禅」：Mac QuickLook「快速预览」兼容性扩展教程，同时Quick Look plugins这个开源项目列出了所有支持 Homebrew-Cask 的 Quick Look 扩展，据说支持的都是程序员必备。 轻松实现一键装机在使用 Mac 的过程中，总想着有没有方便、简单的办法实现在不同Mac 上同步开发环境的办法，今天在整理 Homebrew 使用笔记的时候突然冒出一个想法，如果我把所有的 Homebrew 安装命令列成一个清单形式，当在另一台新的 Mac 上工作时，那么就可以先装一个 Homebrew 然后将命令清单中的所有命令复制粘贴到终端中，等待命令执行完毕后，新的 Mac 的大部分开发环境就跟常用的 Mac 开发环境一致了。下面列出笔者的常用命令清单： 12345678910#安装 Homebrew/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"#安装基础套件brew install fish #安装fish shellbrew install git-flow #安装git-flowbrew install python #安装python#Homebrew-Caskbrew tap caskroom/cask 安装Cask基础软件12345678brew cask install iterm2 #安装iTerm 2brew cask install visual-studio-code#微软出品的文本编辑器，可替代 Sublime Textbrew cask install google-chrome #安装Chromebrew cask install the-unarchiver #解压软件brew cask install alfred #效率软件brew cask install qq #腾讯QQbrew cask install sourcetree #Git GUI 客户端brew cask install cheatsheet # 显示当前程序的快捷键列表，默认的快捷键是长按⌘ 参考https://www.cnblogs.com/javalouvre/p/10618340.html","link":"/2020/07/19/Homebrew%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"title":"【深入理解Kafka】Kafka基础","text":"目前Kafka已经定位为一个分布式流式处理平台,它以高吞吐、可持久化、可水平扩展、支持流数据处理等多种特性而被广泛使用。目前越来越多的开源分布式处理系统如Clo?dera、Storm、Spark、Flink 等都支持与Kafka集成。 初识KafkaKafka之所以受到越来越多的青睐，与它所“扮演”的三大角色是分不开的： 消息系统：Kafka 和传统的消息系统（也称作消息中间件）都具备系统解耦、冗余存储、流量削峰、缓冲、异步通信、扩展性、可恢复性等功能。与此同时，Kafka 还提供了大多数消息系统难以实现的消息顺序性保障及回溯消费的功能。 存储系统：Kafka 把消息持久化到磁盘，相比于其他基于内存存储的系统而言，有效地降低了数据丢失的风险。也正是得益于Kafka 的消息持久化功能和多副本机制，我们可以把Kafka作为长期的数据存储系统来使用，只需要把对应的数据保留策略设置为“永久”或启用主题的日志压缩功能即可。 流式处理平台：Kafka 不仅为每个流行的流式处理框架提供了可靠的数据来源，还提供了一个完整的流式处理类库，比如窗口、连接、变换和聚合等各类操作。 基本概念一个典型的 Kafka 体系架构包括若干 Producer、若干 Broker、若干Consumer，以及一个ZooKeeper集群。 Producer：生产者，也就是发送消息的一方。生产者负责创建消息,然后将其投递到Kafka中。 Consumer：消费者，也就是接收消息的一方。消费者连接到Kafka上并接收消息，进而进行相应的业务逻辑处理。 Broker：服务代理节点。对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以将Broker看作一台Kafka服务器,前提是这台服务器上只部署了一个Kafka实例。一个或多个Broker组成了一个Kafka集群。一般而言,我们更习惯使用首字母小写的broker来表示服务代理节点。 主题与分区在 Kafka 中 还 有 两 个 特 别 重 要 的 概 念 — 主 题 ( Topic ) 与 分 区(Partition)。Kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题(发送到Kafka集群中的每一条消息都要指定一个主题)，而消费者负责订阅主题并进行消费。 主题是一个逻辑上的概念，它还可以细分为多个分区，一个分区只属于单个主题 , 很多时候也会把分区称为主题分区 ( Topic- Partition)。同一主题下的不同分区包含的消息是不同的，分区在存储层面可以看作一个可追加的日志(Log)文件，消息在被追加到分区日志文件的时候都会分配一个特定的偏移量(offset)**。offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。** 多副本机制Kafka 为分区引入了多副本(Replica)机制，通过增加副本数量可以提升容灾能力。同一分区的不同副本中保存的是相同的消息(在同一时刻，副本之间并非完全一样)，副本之间是“一主多从”的关系，其中leader副本负责处理读写请求，follower副本只负责与leader副本的消息同步。副本处于不同的broker中，当leader副本出现故障时，从follower副本中重新选举新的leader副本对外提供服务。Kafka通过多副本机制实现了故障的自动转移,当Kafka集群中某个broker失效时仍然能保证服务可用。 如下图所示，Kafka集群中有4个broker，某个主题中有3个分区，且副本因子(即副本个数)也为3，如此每个分区便有1个leader副本和2个follower副本。生产者和消费者只与leader副本进行交互，而follower 副本只负责消息的同步，很多时候follower副本中的消息相对leader副本而言会有一定的滞后。 Kafka 消费端也具备一定的容灾能力。Consumer 使用拉(Pull)模式从服务端拉取消息,并且保存消费的具体位置，当消费者宕机后恢复上线时可以根据之前保存的消费位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。 服务端参数配置Kafka服务端还有很多参数配置，涉及使用、调优的各个方面,虽然这些参数在大多数情况下不需要更改，但了解这些参数，以及在特殊应用需求的情况下进行有针对性的调优，可以更好地利用 Kafka为我们工作。下面挑选一些重要的服务端参数来做细致的说明，这些参数都配置在$KAFKA_HOME/config/server.properties文件中。 zookeeper.connect 该参数指明**broker要连接的ZooKeeper集群的服务地址(包含端口号)**，没有默认值,且此参数为必填项。可以配置为localhost:2181，如果ZooKeeper集群中有多个节点，则可以用逗号将每个节点隔开，类似于localhost1:2181，localhost2:2181，localhost3:2181这种格式。 listeners 该参数指明broker监听客户端连接的地址列表，即为客户端要连接broker的入又地址列表，配置格式为protocol1://hostname1:port1，protocol2://hostname2:port2，其中protocol代表协议类型，Kafka当前支持的协议类型有PLAINTEXT、SSL、SASL_SSL等，如果未开启安全认证，则使用简单的 PLAINTEXT 即可。如果不指定主机名，则表示绑定默认网卡，注意有可能会绑定到127.0.0.1，这样无法对外提供服务，所以主机名最好不要为空；如果主机 名 是 0.0.0.0，则表示绑定所有的网卡。 broker.id 该参数用来指定Kafka集群中broker的唯一标识，默认值为-1。 log.dir 和 log.dirs Kafka 把所有的消息都保存在磁盘上,而这两个参数用来配置 Kafka 日志文件存放的根目录。一般情况下，log.dir 用来配置单个根目录，而 log.dirs 用来配置多个根目录(以逗号分隔)。log.dirs 的优先级比 log.dir 高，但是如果没有配置log.dirs，则会以 log.dir 配置为准。默认情况下只配置了 log.dir 参数，其默认值为/tmp/kafka-logs。 message.max.bytes 该参数用来指定 broker 所能接收消息的最大值，如果 Producer 发送的消息大于这个参数所设置的值，那么(Producer)就会报出RecordTooLargeException 的异常。为了避免修改此参数而引起级联的影响，建议在修改此参数之前考虑分拆消息的可行性。 参考《深入理解Kafka：核心设计与实践原理》-朱忠华","link":"/2020/10/05/Kafka%E5%9F%BA%E7%A1%80/"},{"title":"Homebrew安装指定版本Mysql（以5.7为例）","text":"安装HomebrewHomebrew官网：https://brew.sh/安装途中可能遇到connection refused 问题，可看本人之前发的教程解决 搜索可安装版本$ brew search boost 修改环境变量brew安装的东西都是在 /usr/local/Cellar/ 本人的路径是： /usr/local/Cellar/mysql@5.7/5.7.23/bin mysql版本不同，路径会有一些不同 终端输入命令sudo vim .bash_profile 按i进入insert模式，输入：export PATH=$PATH:/usr/local/Cellar/mysql@5.7/5.7.23/bin 然后esc退出insert模式，并在最下方输入:wq保存退出。（注意“wq”前有个“:”） 生效配置文件source .bash_profile 回车执行，运行环境变量。 自动生效配置文件如果没有这一步，那么每次关掉终端在打开都需要重新source .bash_profile。首先， vi ~/.zshrc 输入： export PATH=${PATH}:/usr/local/Cellar/mysql@5.7/5.7.23/bin 保存后 source ~/.zshrc","link":"/2020/07/18/Homebrew%E5%AE%89%E8%A3%85%E6%8C%87%E5%AE%9A%E7%89%88%E6%9C%ACMysql%EF%BC%88%E4%BB%A55-7%E4%B8%BA%E4%BE%8B%EF%BC%89/"},{"title":"MacOS下安装JDK并配置路径以解决Homebrew安装Maven后的Java路径不匹配问题","text":"本文记录首次安装JDK及解决Maven路径不匹配问题 安装JDK前往 Oracle官网选择对应版本下载，安装时一路Next即可 验证JDK在终端输入java -version 123java version \"1.8.0_261\"Java(TM) SE Runtime Environment (build 1.8.0_261-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.261-b12, mixed mode) 表示已经安装成功，也能正常使用，但此时是没有配置路径的 Homebrew安装的maven中路径与系统Java路径不匹配 Homebrew安装maven方法可参考本人之前的安装mysql的文章 使用Homebrew安装好maven后，在终端输入mvn -v 12345Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-18T02:33:14+08:00)Maven home: /usr/local/Cellar/maven@3.5/3.5.4_1/libexecJava version: 14.0.1, vendor: N/A, runtime: /usr/local/Cellar/openjdk/14.0.1/libexec/openjdk.jdk/Contents/HomeDefault locale: zh_CN_#Hans, platform encoding: UTF-8OS name: \"mac os x\", version: \"10.15.4\", arch: \"x86_64\", family: \"mac\" 可以看到，这里的Java版本是14.0.1，这是因为使用Homebrew安装maven的时候会一并安装最新版本的OpenJDK。因此需要我们手动配置一下。 配置Java路径 参考来源：https://www.jianshu.com/p/6831bfb8e012 在终端输入sudo vim /etc/profile按i进入insert模式，在下方加入四行配置 1234JAVA_HOME=\"/Library/Java/JavaVirtualMachines/jdk1.8.0_261.jdk/Contents/Home\"export JAVA_HOMECLASS_PATH=\"$JAVA_HOME/lib\"PATH=\".$PATH:$JAVA_HOME/bin\" 输入:wq!保存并退出 1JAVA_HOME=\"/Library/Java/JavaVirtualMachines/jdk1.8.0_261.jdk/Contents/Home\" 此处的路径可以去资源库查找自己的路径，也可以打开一个新的终端输入 /usr/libexec/java_home 然后把出现的路径复制过来就行完成上面内容后继续在终端输入 source /etc/profile 回车后即完成配置。 验证配置是否成功终端输入 echo $JAVA_HOME 出现Java路径即成功！ 配置完成后再查看maven，可发现Java已显示为自己安装的1.8版本 12345Apache Maven 3.5.4 (1edded0938998edf8bf061f1ceb3cfdeccf443fe; 2018-06-18T02:33:14+08:00)Maven home: /usr/local/Cellar/maven@3.5/3.5.4_1/libexecJava version: 1.8.0_261, vendor: Oracle Corporation, runtime: /Library/Java/JavaVirtualMachines/jdk1.8.0_261.jdk/Contents/Home/jreDefault locale: zh_CN, platform encoding: UTF-8OS name: \"mac os x\", version: \"10.15.4\", arch: \"x86_64\", family: \"mac\"","link":"/2020/07/21/MacOS%E4%B8%8B%E5%AE%89%E8%A3%85JDK%E5%B9%B6%E9%85%8D%E7%BD%AE%E8%B7%AF%E5%BE%84%E4%BB%A5%E8%A7%A3%E5%86%B3Homebrew%E5%AE%89%E8%A3%85Maven%E5%90%8E%E7%9A%84Java%E8%B7%AF%E5%BE%84%E4%B8%8D%E5%8C%B9%E9%85%8D%E9%97%AE%E9%A2%98/"},{"title":"【深入理解Kafka】生产者","text":"从编程的角度而言，生产者就是负责向Kafka发送消息的应用程序。 客户端开发一个正常的生产逻辑需要具备以下几个步骤: 配置生产者客户端参数及创建相应的生产者实例。 构建待发送的消息。 发送消息。 关闭生产者实例。 12345678910111213141516171819202122232425// 生产者客户端示例代码public class KafkaProducerAnalysis { public static final String brokerList = \"localhost:9092\"; public static final String topic = \"topic-demo\"; public static Properties initConfig() { Properties props = new Properties(); props.put(\"bootstrap.servers\", brokerList); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerilizer\"); props.pus(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); properties.put(\"client.id\", \"producer.client.id.demo\"); return props; } public static void main(String[] args) { Properties props = initConfig(); KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, \"Hello, Kafka!\"); try { producer.send(record); } catch (Exception e) { e.printStackTrace(); } }} 必要的参数配置在创建真正的生产者实例前需要配置相应的参数，比如需要连接的Kafka集群地址。在Kafka 生产者客户端KafkaProducer中有3个参数是必填的。 bootstrap.servers：该参数用来指定生产者客户端连接Kafka集群所需的broker地址清单，具体的内容格式为host1:port1,host2:port2，可以设置一个或多个地址，中间以逗号隔开,此参数的默认值为“”。注意这里并非需要所有的 broker 地址，因为生产者会从给定的broker里查找到其他broker的信息。不过建议至少要设置两个以上的 broker 地址信息，当其中任意一个宕机时，生产者仍然可以连接到 Kafka集群上。 key.serializer 和 value.serializer：broker 端接收的消息必须以字节数组(byte[])的形式存在。生产者使用的KafkaProducer &lt;String , String&gt; 和 ProducerRecord&lt;String , String&gt; 中的泛型 &lt;String,String&gt; 对应的就是消息中 key 和 value 的类型，生产者客户端使用这种方式可以让代码具有良好的可读性，不过在发往 broker 之前需要将消息中对应的 key 和 value 做相应的序列化操作来转换成字节数组。key.serializer 和 value.serializer 这两个参数分别用来指定 key 和 value 序列化操作的序列化器，这两个参数无默认值。注意这里必须填写序列化器的全限定名，如 org.apache.kafka.common.serialization.StringSerializer , 单单指定StringSerializer是错误的 在配置完参数后，我们就可以使用它来创建一个生产者实例 1KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); KafkaProducer 是线程安全的，可以在多个线程中共享单个 KafkaProducer 实例，也可以将 KafkaProducer 实例进行池化来供其他线程调用。 KafkaProducer 中有多个构造方法，比如在创建 KafkaProducer 实例时并没有设定 key.serializer 和 value.serializer 这两个配置参数，那么就需要在构造方法中添加对应的序列化器 1KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props, new StringSerializer(), new StringSerializer()); 其内部原理和无序列化器的构造方法一样，不过就实际应用而言，一般都选用public KafkaProducer(Properties properties)这个构造方法来创建 KafkaProducer 实例。 消息的发送在创建完生产者实例之后，接下来的工作就是构建消息，即创建 ProducerRecord 对象。通过上面的生产者客户端示例代码中我们已经了解了 ProducerRecord 的属性结构，其中 topic 属性和 value 属性是必填项，其余属性是选填项，对应的 ProducerRecord 的构造方法也有多种。 创建生产者实例和构建消息之后,就可以开始发送消息了。发送消息主要有三种模式：发后即忘(fire-and-forget)、同步(sync)及异步(async)。 发后即忘示例代码的发送方式就是发后即忘，它只管往Kafka中发送消息而并不关心消息是否正确到达。在大多数情况下，这种发送方式没有什么问题，不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高，可靠性也最差。 KafkaProducer 的 send() 方法并非是 void 类型，而是 Future&lt;RecordMetadata&gt;类型，实际上 send() 方法本身就是异步的，send() 方法返回的 Future 对象可以使调用方稍后获得发送的结果。示例中在执行 send() 方法之后直接链式调用了 get() 方法来阻塞等待 Kafka 的响应，直到消息发送成功，或者发生异常。 也可以在执行完 send() 方法之后不直接调用 get() 方法，比如下面的一种同步发送方式的实现： 1234567try { Future&lt;RecordMetadata&gt; future = producer.send(record); RecordMetadata metadata = future.get(); System.out.println(metadata.topic() + \"-\" + metadata.partition() + \":\" + metadata.offset());} catch (ExecutionException | InterruptedException e) { e.printStackTrace();} 这样可以获取一个 RecordMetadata 对象，在 RecordMetadata 对象里包含了消息的一些元数据信息，比如当前消息的主题、分区号、分区中的偏移量(offset)、时间戳等。如果在应用代码中需要这些信息，则可以使用这个方式。如果不需要，则直接采用producer.send(record).get()的方式更省事。 Future 表示一个任务的生命周期，并提供了相应的方法来判断任务是否已经完成或取消，以及获取任务的结果和取消任务等。既然KafkaProducer.send()方法的返回值是一个 Future 类型的对象，那么完全可以用Java语言层面的技巧来丰富应用的实现，比如使用Future 中的get(long timeout, TimeUnit unit)方法实现可超时的阻塞。 同步同步发送的方式可靠性高，要么消息被发送成功,要么发生异常。如果发生异常，则可以捕获并进行相应的处理，而不会像“发后即忘”的方式直接造成消息的丢失。不过同步发送的方式的性能会差很多，需要阻塞等待一条消息发送完之后才能发送下一条。 异步一般是在 send() 方法里指定一个 Callback 的回调函数，Kafka在返回响应时调用该函数来实现异步的发送确认。有读者或许会有疑问，send() 方法的返回值类型就是 Future，而 Future 本身就可以用作异步的逻辑处理。这样做不是不行，只不过 Future 里的 get() 方法在何时调用，以及怎么调用都是需要面对的问题，消息不停地发送，那么诸多消息对应的 Future 对象的处理难免会引起代码处理逻辑的混乱。使用 Callback 的方式非常简洁明了，Kafka 有响应时就会回调，要么发送成功，要么抛出异常。 1234567891011// 异步发送方式示例producer.send(record, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (exception != null) { exception.printStackTrace(); } else { System.out.println(metadata.topic() + \"-\" + metadata.partition() + \":\" + metadata.offset()); } }}); 通常，一个 KafkaProducer 不会只负责发送单条消息，更多的是发送多条消息，在发送完这些消息之后，需要调用 KafkaProducer 的 close() 方法来回收资源。 序列化生产者需要用序列化器(Serializer)把对象转换成字节数组才能通过网络发送给 Kafka 。而在对侧，消费者需要用反序列化器(Deserializer)把从 Kafka 中收到的字节数组转换成相应的对象。在上面的示例代码中，为了方便，消息的 key 和 value 都使用了字符串，对应程序中的序列化器也使用了客户端自带的org.apache.kafka.common.serialization.StringSerializer，除了用于String类型的序列化器，还有 ByteArray、ByteBuffer、Bytes、Double、Integer、Long 这几种类型, 它们都实现了org.apache.kafka.common.serialization.Serializer 接口。 分区器消息在通过 send() 方法发往 broker 的过程中，有可能需要经过拦截器(Interceptor)、序列化器(Serializer)和分区器(Partitioner)的一系列作用之后才能被真正地发往 broker。拦截器一般不是必需的，而序列化器是必需的。消息经过序列化之后就需要确定它发往的分区，如果消息 ProducerRecord 中指定了 partition 字段，那么就不需要分区器的作用，因为 partition 代表的就是所要发往的分区号。 如果消息 ProducerRecord 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。分区器的作用就是为消息分配分区。 生产者拦截器拦截器(Interceptor)是早在Kafka 0.10.0.0中就已经引入的一个功能，Kafka 一共有两种拦截器：生产者拦截器和消费者拦截器。本节主要讲述生产者拦截器的相关内容。 生产者拦截器既可以用来在消息发送前做一些准备工作,比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。 KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和 partition 等信息，如果要修改，则需确保对其有准确的判断，否则会与预想的效果出现偏差。比如修改key不仅会影响分区的计算，同样会影响broker端日志压缩(Log Compaction)的功能。 KafkaProducer 会在消息被应答(Acknowledgement)之前或消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。这个方法运行在 Producer 的 I/O 线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。 close() 方法主要用于在关闭拦截器时执行一些资源的清理工作。 原理分析在前面的章节中，我们已经了解了 KafkaProducer 的具体使用方法，而本节的内容主要是对 Kafka 生产者客户端的内部原理进行分析，通过了解生产者客户端的整体脉络可以让我们更好地使用它，避免因为一些理解上的偏差而造成使用上的错误。 整体架构 整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。 在主线程中由 KafkaProducer 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 Sender 线程负责从 RecordAccumulator 中获取消息并将其发送到 Kafka 中。 主线程中发送过来的消息都会被追加到 RecordAccumulator 的某个双端队列（Deque）中，在 RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是 ProducerBatch，即 Deque&lt;ProducerBatch&gt;。消息写入缓存时，追加到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。注意 ProducerBatch 不是 ProducerRecord，ProducerBatch 中可以包含一至多个 ProducerRecord。 通俗地说，ProducerRecord 是生产者中创建的消息，而 ProducerBatch 是指一个消息批次，ProducerRecord 会被包含在 ProducerBatch 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 ProducerRecord 拼凑成一个较大的 ProducerBatch，也可以减少网络请求的次数以提升整体的吞吐量。 元数据的更新我们使用如下的方式创建了一条消息 ProducerRecord 1ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, \"Hello, kafka!\"); 我们只知道主题的名称，对于其他一些必要的信息却一无所知。KafkaProducer 要将此消息追加到指定主题的某个分区所对应的 leader 副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后 KafkaProducer 需要知道目标分区的 leader 副本所在的 broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka ，在这一过程中所需要的信息都属于元数据信息。 元数据是指 Kafka 集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的 leader 副本分配在哪个节点上，follower 副本分配在哪些节点上，哪些副本在AR、ISR等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 重要的生产者参数在 KafkaProducer 中，除了之前提及的3个默认的客户端参数，大部分的参数都有合理的默认值，一般不需要修改它们。不过了解这些参数可以让我们更合理地使用生产者客户端，其中还有一些重要的参数涉及程序的可用性和性能，如果能够熟练掌握它们，也可以让我们在编写相关的程序时能够更好地进行性能调优与故障排查。 acks 这个参数用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的。acks 是生产者客户端中一个非常重要的参数，它涉及消息的可靠性和吞吐量之间的权衡。 max.request.size 这个参数用来限制生产者客户端能发送的消息的最大值，默认值为 1048576B，即 1MB。一般情况下，这个默认值就可以满足大多数的应用场景了。 retries 和 retry.backoff.ms retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、leader副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置 retries 大于0的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数，那么生产者就会放弃重试并返回异常。 重试还和另一个参数 retry.backoff.ms 有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔,避免无效的频繁重试。 compression.type 这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。 connections.max.idle.ms 这个参数用来指定在多久之后关闭限制的连接，默认值是540000(ms)，即9分钟。 linger.ms 这个参数用来指定生产者发送 ProducerBatch 之前等待更多消息 (ProducerRecord) 加入 ProducerBatch 的时间，默认值为 0。生产者客户端会在 ProducerBatch 被填满或等待时间超过 linger.ms 值时发送出去。增大这个参数的值会增加消息的延迟，但是同时能提升一定的吞吐量。这个 linger.ms 参数与 TCP 协议中的 Nagle 算法有异曲同工之妙。 receive.buffer.bytes 这个参数用来设置 Socket 接收消息缓冲区 (SO_RECBUF) 的大小，默认值为32768(B)，即32KB。如果设置为-1，则使用操作系统的默认值。如果 Producer 与 Kafka 处于不同的机房，则可以适地调大这个参数值。 send.buffer.bytes 这个参数用来设置 Socket 发送消息缓冲区 (SO_SNDBUF) 的大小，默认值为 131072(B)，即 128KB。与 receive.buffer.bytes 参数一样，如果设置为-1，则使用操作系统的默认值。 request.timeout.ms 这个参数用来配置 Producer 等待请求响应的最长时间，默认值为 30000(ms)。请求超时之后可以选择进行重试。注意这个参数需要比 broker 端参数 replica.lag.time.max.ms 的值要大，这样可以减少因客户端重试而引起的消息重复的概率。 总结本文主要讲述了生产者客户端的具体用法及其整体架构，主要内容包括配置参数的详解、消息的发送方式、序列化器、分区器、拦截器等。在实际应用中，一套封装良好的且灵活易用的客户端可以避免开发人员重复劳动，也提高了开发效率，还可以提高程序的健壮性和可靠性，而Kafka的客户端正好包含了这些特质。对于 KafkaProducer 而言，它是线程安全的，我们可以在多线程的环境中复用它，而对于消费者客户端 KafkaConsumer 而言，它是非线程安全的，因为它具备了状态，具体怎么使用请看下一篇的内容。 参考《深入理解Kafka：核心设计与实践原理》-朱忠华","link":"/2020/10/06/Kafka%E7%94%9F%E4%BA%A7%E8%80%85/"},{"title":"MacOS下编译RedisDesktopManagement报no file at...&#x2F;Python.framework&#x2F;...错解决方案","text":"问题描述执行命令：/Users/zzq/Qt5.9.8/5.9.8/clang_64/bin/macdeployqt rdm.app -qmldir=../../../src/qml 出错：ERROR: no file at &quot;/usr/local/opt/python/lib/Python.framework/Versions/3.7/Python&quot; 解决方案该问题是找不到Python.framework，如果你跟笔者一样是用brew安装的，那么就将/usr/local/Cellar/python/3.7.5/Frameworks下的Python.framework文件复制一份到/usr/local/Cellar/python/3.7.5/lib下就行了，我的是3.7.5版本，安装的自己到相应路径下操作即可","link":"/2020/06/29/MacOS%E4%B8%8B%E7%BC%96%E8%AF%91RedisDesktopManagement%E6%8A%A5no-file-at-Python-framework-%E9%94%99%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"MySQL基础语法","text":"读《MySQL必知必会》学习MySQL基础语法，记录一下 USE 使用crashcourse数据库 1USE crashcourse; USE语句不返回任何结果。 SHOW 显示数据库名 1SHOW DATABASES; 显示当前选择的数据库内可用表的列表 1SHOW TABLES; 给出表名，对每个字段返回一行，行中包含字段名、数据类型、是否允许NULL、键信息、默认值以及其他信息（如字段cust_id的auto_increment） 1SHOW COLUMNS FROM customers; NOTE：自动增量auto_increment 某些表列需要唯一值。 在每个行添加到表中时， MySQL可以自动地为每个行分配下一个可用编号，不用在添加一行时手动分配唯一值（这样做必须记住最后一次使用的值） 。如果需要它，则必须在用CREATE语句创建表时把它作为表定义的组成部分。 其他SHOW语句： SHOW STATUS，用于显示广泛的服务器状态信息； SHOW CREATE DATABASE和SHOW CREATE TABLE，分别用来显示创建特定数据库或表的MySQL语句； SHOW GRANTS，用来显示授予用户（所有用户或特定用户）的安全权限； SHOW ERRORS和SHOW WARNINGS， 用来显示服务器错误或警告消息。 SELECT 用SELECT语句从products表中检索一个名为prod_name的列 （如果没有明确排序查询结果 ，则返回的数据的顺序没有特殊意义 ） 12SELECT prod_nameFROM products; 从products表中选择3列 12SELECT prod_id, prod_name, prod_priceFROM products; 检索所有列 12SELECT *FROM products; 检索不同的行，只返回不同的vend_id行 12SELECT DISTINCT vend_idFROM products; LIMIT 限制结果。使用LIMIT 5返回不多于5行 123SELECT prod_nameFROM productsLIMIT 5; NOTE：LIMIT 为得出下一个5行，可以指定开始行和行数，如LIMIT 5,5，表示从行5开始的5行。第一个数为开始位置，第二个数为要检索的行数。 行0：检索出来的第一行为行0而不是行1。因此， LIMIT 1, 1将检索出第二行而不是第一行。 在行数不够时 ，MySQL将只返回它能返回的那么多行 使用完全限定的表名 12SELECT products.prod_nameFROM crashcourse.products; ORDER BY 使用ORDER BY子句取一个或多个列的名字，据此对输出进行排序 123SELECT prod_nameFROM productsORDER BY prod_name; 按多个列排序。检索3个列，并按其中两个列对结果进行排序，首先按价格，然后再按名称排序 123SELECT prod_id, prod_price, prod_nameFROM productsORDER BY prod_price, prod_name; 按降序排序。 123SELECT prod_id, prod_price, prod_nameFROM productsORDER BY prod_price DESC; 使用ORDER BY和LIMIT组合，prod_price DESC保证行由最贵到最便宜检索，LIMIT 1告诉MySQL仅返回一行 1234SELECT prod_priceFROM productsORDER BY prod_price DESCLIMIT 1; NOTE：ORDER BY子句的位置 在给出ORDER BY子句时，应该保证它位于FROM子句之后。如果使用LIMIT，它必须位于ORDER BY之后。 WHERE 在SELECT语句中，数据根据WHERE子句指定的搜索条件进行过滤。 123SELECT prod_name, prod_priceFROM productsWHERE prod_price = 2.50; NOTE： SQL过滤与应用过滤。数据也可以在应用层过滤。为此目的， SQL的SELECT语句为客户机应用检索出超过实际所需的数据，然后客户机代码对返回数据进行循环，以提取出需要的行。通常，这种实现并不令人满意。因此，对数据库进行了优化，以便快速有效地对数据进行过滤。让客户机应用（或开发语言）处理数据库的工作将会极大地影响应用的性能，并且使所创建的应用完全不具备可伸缩性。此外，如果在客户机上过滤数据，服务器不得不通过网络发送多余的数据，这将导致网络带宽的浪费。 WHERE子句的位置。在同时使用ORDER BY和WHERE子句时，应该让ORDER BY位于WHERE之后 WHERE子句操作符 123456789101112131415161718192021222324#检查单个值SELECT prod_name, prod_priceFROM productsWHERE prod_name = 'fuses';#列出小于10元的产品SELECT prod_name, prod_priceFROM productsWHERE prod_price &lt; 10;#不匹配检查SELECT vend_id, prod_nameFROM productsWHERE vend_id != 1003;#范围检查SELECT prod_name, prod_priceFROM productsWHERE prod_price BETWEEN 5 AND 10;#空值检查SELECT prod_nameFROM productsWHERE prod_price IS NULL; AND OR AND`操作符，必须满足所有条件 123SELECT prod_id, prod_price, prod_nameFROM productsWHERE vend_id = 103 AND prod_price &lt;= 10; OR操作符，匹配任一条件即可 123SELECT prod_name, prod_priceFROM productsWHERE vend_id = 1002 OR vend_id = 1003; AND和OR组合使用。列出价格为10元（含）以上且由1002或1003制造的所有产品 123SELECT prod_name, prod_priceFROM productsWHERE (vend_id = 1002 OR vend_id = 1003) AND prod_price &gt;= 10; NOTE： SQL的计算次序中AND的优先级比OR更高，因此在不加括号时会优先计算AND两侧条件 任何时候使用具有AND和OR操作符的WHERE子句，都应该使用圆括号明确地分组操作符。不要过分依赖默认计算次序。 IN IN操作符用来指定条件范围，范围中的每个条件都可以进行匹配 1234SELECT vend_id, prod_name, prod_priceFROM productsWHERE vend_id IN (1002,1003)ORDER BY prod_name; NOTE：IN操作符优点 在使用长的合法选项清单时， IN操作符的语法更清楚且更直观。 在使用IN时，计算的次序更容易管理（因为使用的操作符更少）。 IN操作符一般比OR操作符清单执行更快。 IN的最大优点是可以包含其他SELECT语句，使得能够更动态地建立WHERE子句。 NOT WHERE子句中的NOT操作符有且只有一个功能，那就是否定它之后所跟的任何条件。 1234SELECT vend_id, prod_name, prod_priceFROM productsWHERE vend_id NOT IN (1002,1003)ORDER BY prod_name; NOTE：MySQL中的NOT MySQL 支 持 使 用 NOT 对 IN 、 BETWEEN 和EXISTS子句取反，这与多数其他DBMS允许使用NOT对各种条件取反有很大的差别。 LIKENOTE： 通配符（wildcard）：用来匹配值的一部分的特殊字符 搜索模式（search pattern）：由字面值、通配符或两者组合构成的搜索条件 谓词（predicate）： 操作符何时不是操作符？答案是在它作为谓词（ predicate）时。从技术上说， LIKE是谓词而不是操作符。虽然最终的结果是相同的，但应该对此术语有所了解 百分号（%）通配符。 例：找出所有以词jet起头的产品 123SELECT prod_id, prod_nameFROM productsWHERE prod_name LIKE 'jet%'; 匹配任意位置 123SELECT prod_id, prod_nameFROM productsWHERE prod_name LIKE '%anvil%'; 找出s起头以e结尾所有产品 123SELECT prod_id, prod_nameFROM productsWHERE prod_name LIKE 's%e'; NOTE： 注意尾空格：尾空格可能会干扰通配符匹配。例如，在保存词anvil 时 ， 如 果 它 后 面 有 一 个 或 多 个 空 格 ， 则 子 句 WHEREprod_name LIKE ‘%anvil’将不会匹配它们，因为在最后的l后有多余的字符。解决这个问题的一个简单的办法是在搜索模式最后附加一个%。一个更好的办法是使用函数 注意NULL：虽然似乎%通配符可以匹配任何东西，但有一个例外，即NULL。即使是WHERE prod_name LIKE ‘%’也不能匹配用值NULL作为产品名的行 下划线（_）通配符。用途与%一样，但下划线只匹配单个字符而不是多个字符。 123SELECT prod_id, prod_nameFROM productsWHERE prod_name LIKE '_ ton anvil'; 123SELECT prod_id, prod_nameFROM productsWHERE prod_name LIKE '% ton anvil'; NOTE： 不要过度使用通配符。如果其他操作符能达到相同的目的，应该使用其他操作符。 在确实需要使用通配符时，除非绝对有必要，否则不要把它们用在搜索模式的开始处。把通配符置于搜索模式的开始处，搜索起来是最慢的。 仔细注意通配符的位置。如果放错地方，可能不会返回想要的数据 正则表达式 基本字符匹配。 1234SELECT prod_idFROM productsWHERE prod_name REGEXP '.000';ORDER BY prod_name; NOTE：匹配不区分大小写。 MySQL中的正则表达式匹配（自版本3.23.4后）不区分大小写（即，大写和小写都匹配）。为区分大小写，可使用BINARY关键字，如WHERE prod_name REGEXPBINARY 'JetPack .000' 进行OR匹配。也可以给出两个以上的OR条件，如1000|2000|3000 1234SELECT prod_idFROM productsWHERE prod_name REGEXP '1000|2000';ORDER BY prod_name; 匹配几个字符之一。使用正则表达式[123]定义一组数字，匹配1或2或3 1234SELECT prod_nameFROM productsWHERE prod_name REGEXP '[123] Ton';ORDER BY prod_name; 范围匹配 1234SELECT prod_nameFROM productsWHERE prod_name REGEXP '[1-5] Ton';ORDER BY prod_name; 匹配特殊字符。如果使用.，则会匹配任意字符 1234SELECT prod_nameFROM productsWHERE prod_name REGEXP '\\\\.';ORDER BY prod_name; NOTE： 匹配\\。为了匹配\\本身，需要使用\\\\\\ **\\或\\\\?**。多数正则表达式实现使用单个反斜杠转义特殊字符，以便能使用这些字符本身。但MySQL要求两个反斜杠（ MySQL自己解释一个，正则表达式库解释另一个）。 匹配字符类 匹配多个实例 1234SELECT prod_nameFROM productsWHERE prod_name REGEXP '\\\\([0-9] sticks?\\\\)';ORDER BY prod_name; 分析：正则表达式\\\\([0-9] sticks?\\\\)需要解说一下。 \\\\(匹配(，[0-9]匹配任意数字（这个例子中为1和5），sticks?匹配stick和sticks（ s后的?使s可选，因为?匹配它前面的任何字符的0次或1次出现），\\\\)匹配)。没有?，匹配stick和sticks会非常困难 1234SELECT prod_nameFROM productsWHERE prod_name REGEXP '[[:digit:]]{4}';ORDER BY prod_name; 分析：[:digit:]匹配任意数字，因而它为数字的一个集合。 {4}确切地要求它前面的字符（任意数字）出现4次，所以[[:digit:]]{4}匹配连在一起的任意4位数字 定位符 1234SELECT prod_nameFROM productsWHERE prod_name REGEXP '^[0-9\\\\.]';ORDER BY prod_name; 分析：之前的例子都是匹配一个串中任意位置的文本。^[0-9\\\\.]只在.或任意数字为串中第一个字符时才匹配它们 NOTE：使REGEXP起类似LIKE的作用。LIKE和REGEXP的不同在于， LIKE匹配整个串而REGEXP匹配子串。利用定位符，通过用^开始每个表达式，用$结束每个表达式，可以使REGEXP的作用与LIKE一样。 计算字段 例：vendors表包含供应商名和位置信息。假如要生成一个供应商报表，需要在供应商的名字中按照name(location)这样的格式列出供应商的位置。此报表需要单个值，而表中数据存储在两个列vend_name和vend_country中。此外，需要用括号将vend_country括起来。 123SELECT Concat(vend_name, '(', vend_country, ')')FROM vendorsORDER BY vend_name; 使用RTrim()函数删除数据右侧多余的空格来整理数据。也可以使用LTrim()删除左边的空格，Trim()删除左右两边的空格 123SELECT Concat(RTrim(vend_name), '(', RTrim(vend_country), ')')FROM vendorsORDER BY vend_name; 使用别名。使用AS关键字将vend_title作为新计算列的名字 1234SELECT Concat(RTrim(vend_name), '(', RTrim(vend_country), ')') ASvend_titleFROM vendorsORDER BY vend_name; NOTE： 别名的其他用途。别名还有其他用途。常见的用途包括在实际的表列名包含不符合规定的字符（如空格）时重新命名它，在原来的名字含混或容易误解时扩充它。 导出列。别名有时也称为导出列（ derived column），不管称为什么，它们所代表的都是相同的东西。 执行算术运算 1234567SELECT prod_id, quantity, item_price, order_num, quantity * item_price AS expanded_priceFROM orderitemsWHERE order_num = 20005; 函数NOTE：函数没有SQL的可移植性强。 能运行在多个系统上的代码称为可移植的（ portable）。相对来说，多数SQL语句是可移植的，在SQL实现之间有差异时，这些差异通常不那么难处理。而函数的可移植性却不强。几乎每种主要的DBMS的实现都支持其他实现不支持的函数，而且有时差异还很大。 为了代码的可移植，许多SQL程序员不赞成使用特殊实现的功能。虽然这样做很有好处，但不总是利于应用程序的性能。如果不使用这些函数，编写某些应用程序代码会很艰难。必须利用其他方法来实现DBMS非常有效地完成的工作。 如果你决定使用函数，应该保证做好代码注释，以便以后你（或其他人）能确切地知道所编写SQL代码的含义 本文处理函数 123SELECT vend_name, Upper(vend_name) AS vend_name_upcaseFROM vendorsORDER BY vend_name; 常用文本处理函数 Soundex()通过发音字符和音节检索 123SELECT cust_name, cust_contactFROM customersWHERE Soundex(cust_contact) = Soundex('Y Lie'); 常用日期和时间处理函数 检索2005年9月1日订单记录 123SELECT order_date, cust_id, order_numFROM ordersWHERE Date(order_date) = '2005-09-01'; 检索2005年9月所有订单 123SELECT cust_id, order_numFROM ordersWHERE Year(order_date) = 2005 AND Month(order_date) = 9; 数值处理函数 汇总数据 AVG()函数 12SELECT AVG(prod_price) AS avg_priceFROM products; 123SELECT AVG(prod_price) AS avg_priceFROM productsWHERE vend_id = 1003;#返回特定供应商产品平均价格 NOTE： **只用于单个列 AVG()**。只能用来确定特定数值列的平均值，而且列名必须作为函数参数给出。为了获得多个列的平均值，必须使用多个AVG()函数。 NULL值。AVG()函数忽略列值为NULL的行 COUNT()函数 123#返回customers表中客户的总数SELECT COUNT(*) AS num_custFROM customers; 123#只对具有电子邮件地址的客户计数SELECT COUNT(cust_email) AS num_custFROM customers; MAX()函数 123#返回products表中最贵物品的价格SELECT MAX(prod_price) AS max_priceFROM products; NOTE： 对非数值数据使用MAX()。虽然MAX()一般用来找出最大的数值或日期值，但MySQL允许将它用来返回任意列中的最大值，包括返回文本列中的最大值。在用于文本数据时，如果数据按相应的列排序，则MAX()返回最后一行 NULL值。MAX()函数忽略列值为NULL的行。 MIN()函数 12SELECT MIN(prod_price) AS min_priceFROM products; SUM()函数 12345678#检索订购物品的总数SELECT SUM(quantity) AS items_orderedFROM orderitemsWHERE order_num = 20005;#合计计算值SELECT SUM(item_price * quantity) AS total_priceFROM orderitemsWHERE order_num = 20005; 聚集不同值，DISTINCT 1234#平均值只考虑各个不同的价格SELECT AVG(DISTINCT prod_price) AS avg_priceFROM productsWHERE vend_id = 1003; 组合聚集函数 12345SELECT COUNT(*) AS num_items, MIN(prod_price) AS price_min, MAX(prod_price) AS price_max, AVG(prod_price) AS price_avgFROM products; 分析：这里用单条SELECT语句执行了4个聚集计算，返回4个值（products表中物品的数目，产品价格的最高、最低以及平均值） 分组数据 GROUP BY创建分组 1234#对每个vend_id计算num_prods一次SELECT vend_id, COUNT(*) AS num_prodsFROM productsGROUP BY vend_id; NOTE： GROUP BY子句可以包含任意数目的列。这使得能对分组进行嵌套，为数据分组提供更细致的控制。 如果在GROUP BY子句中嵌套了分组，数据将在最后规定的分组上进行汇总。换句话说，在建立分组时，指定的所有列都一起计算（所以不能从个别的列取回数据）。 GROUP BY子句中列出的每个列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在SELECT中使用表达式，则必须在GROUP BY子句中指定相同的表达式。不能使用别名。 除聚集计算语句外， SELECT语句中的每个列都必须在GROUP BY子句中给出。 如果分组列中具有NULL值，则NULL将作为一个分组返回。如果列中有多行NULL值，它们将分为一组。 GROUP BY子句必须出现在WHERE子句之后， ORDER BY子句之前。 1234#使用ROLLUP对每个组及每个分组汇总SELECT vend_id, COUNT(*) AS num_prodsFROM productsGROUP BY vend_id WITH ROLLUP; HAVING过滤分组 1234SELECT vend_id, COUNT(*) AS num_prodsFROM productsGROUP BY vend_idHAVING COUNT(*) &gt;= 2; WHERE与HAVING同时使用的情况 123456#通过WHERE进一步过滤上面的例子SELECT vend_id, COUNT(*) AS num_prodsFROM productsWHERE prod_price &gt;= 10GROUP BY vend_idHAVING COUNT(*) &gt;= 2; ORDER BY与GROUP BY 1234SELECT order_num, SUM(quantity*item_price) AS ordertotalFROM orderitemsGROUP BY order_numHAVING SUM(quantity*item_price) &gt;= 50; 123456#将总计订单价格排序输出SELECT order_num, SUM(quantity*item_price) AS ordertotalFROM orderitemsGROUP BY order_numHAVING SUM(quantity*item_price) &gt;= 50ORDER BY ordertotal; 子查询1234567891011#检索订购物品TNT2的所有客户#通过客户ID查询客户信息SELECT cust_name, cust_contactFROM customers#外部查询，通过返回的订单号查询客户IDWHERE cust_id IN(SELECT cust_id FROM orders #内部查询，检索包含TNT2的所有订单编号 WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = 'TNT2')); NOTE： 列必须匹配。在WHERE子句中使用子查询（如这里所示），应该保证SELECT语句具有与WHERE子句中相同数目的列。通常，子查询将返回单个列并且与单个列匹配，但如果需要也可以使用多个列。 子查询和性能。这里给出的代码有效并获得所需的结果。但是，使用子查询并不总是执行这种类型的数据检索的最有效的方法。 联结表12345#创建联结SELECT vend_name, prod_name, prod_priceFROM vendors, productsWHERE vendors.vend_id = products.vend_idORDER BY vend_name, prod_name; NOTE： 完全限定列名。在引用的列可能出现二义性时，必须使用完全限定列名（用一个点分隔的表名和列名）。如果引用一个没有用表名限制的具有二义性的列名， MySQL将返回错误。 不要忘了WHERE子句。应该保证所有联结都有WHERE子句，否则MySQL将返回比想要的数据多得多的数据。同理，应该保证WHERE子句的正确性。不正确的过滤条件将导致MySQL返回不正确的数据。 叉联结。有时我们会听到返回称为叉联结（ cross join）的笛卡儿积的联结类型。 内部联结 123SELECT vend_name, prod_name, prod_priceFROM vendors INNER JOIN productsON vendors.vend_id = products.vend_id; 分析：这里，两个表之间的关系是FROM子句的组成部分，以INNER JOIN指定。在使用这种语法时，联结条件用特定的ON子句而不是WHERE子句给出。传递给ON的实际条件与传递给WHERE的相同 联结多个表 12345SELECT prod_name, vend_name, prod_price, quantityFROM orderitems, products, vendorsWHERE products.vend_id = vendors.vend_id AND orderitems.prod_id = products.prod_id AND order_num = 20005; 分析：显示编号为20005的订单中的物品。订单物品存储在orderitems表中。每个产品按其产品ID存储，它引用products表中的产品。这些产品通过供应商ID联结到vendors表中相应的供应商，供应商ID存储在每个产品的记录中。这里的FROM子句列出了3个表，而WHERE子句定义了这两个联结条件，而第三个联结条件用来过滤出订单20005中的物品 NOTE：性能考虑 MySQL在运行时关联指定的每个表以处理联结。这种处理可能是非常耗费资源的，因此应该仔细，不要联结不必要的表。联结的表越多，性能下降越厉害 对比子查询 1234567891011121314151617#返回订购产品TNT2的客户列表#子查询SELECT cust_name, cust_contactFROM customers#外部查询，通过返回的订单号查询客户IDWHERE cust_id IN(SELECT cust_id FROM orders #内部查询，检索包含TNT2的所有订单编号 WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = 'TNT2'));#联结查询SELECT cust_name, cust_contactFROM customers, orders, orderitemsWHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num AND prod_id = 'TNT2';","link":"/2020/06/10/MySQL%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/"},{"title":"【Spring源码】IoC之Spring统一资源加载策略","text":"在学 Java SE 的时候，我们学习了一个标准类java.net.URL，该类在 Java SE中定位为统一资源定位器（Uniform Resource Locator），但是我们知道它的实现基本只限于网络形式发布的资源的查找和定位。然而，实际上资源的定义比较广泛，除了网络形式的资源，还有以二进制形式存在的、以文件形式存在的、以字节流形式存在的等等。而且它可以存在于任何场所，比如网络、文件系统、应用程序中。所以java.net.URL的局限性迫使 Spring 必须实现自己的资源加载策略，该资源加载策略需要满足如下要求： 职能划分清楚。资源的定义和资源的加载应该要有一个清晰地界限； 统一的抽象。统一的资源定义和资源加载策略。资源加载后要返回统一的抽象给客户端，客户端要对资源进行怎样的处理，应该由抽象资源接口来界定。 前言在学 Java SE 的时候，我们学习了一个标准类java.net.URL，该类在 Java SE中定位为统一资源定位器（Uniform Resource Locator），但是我们知道它的实现基本只限于网络形式发布的资源的查找和定位。然而，实际上资源的定义比较广泛，除了网络形式的资源，还有以二进制形式存在的、以文件形式存在的、以字节流形式存在的等等。而且它可以存在于任何场所，比如网络、文件系统、应用程序中。所以java.net.URL的局限性迫使 Spring 必须实现自己的资源加载策略，该资源加载策略需要满足如下要求： 职能划分清楚。资源的定义和资源的加载应该要有一个清晰地界限； 统一的抽象。统一的资源定义和资源加载策略。资源加载后要返回统一的抽象给客户端，客户端要对资源进行怎样的处理，应该由抽象资源接口来界定。 统一资源：Resourceorg.springframework.core.io.Resource 为 Spring 框架所有资源的抽象和访问接口，它继承 org.springframework.core.io.InputStreamSource 接口。作为所有资源的统一抽象，Resource 定义了一些通用的方法，由子类 AbstractResource 提供统一的默认实现。定义如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public interface Resource extends InputStreamSource { /** * 资源是否存在 */ boolean exists(); /** * 资源是否可读 */ default boolean isReadable() { return true; } /** * 资源所代表的句柄是否被一个 stream 打开了 */ default boolean isOpen() { return false; } /** * 是否为 File */ default boolean isFile() { return false; } /** * 返回资源的 URL 的句柄 */ URL getURL() throws IOException; /** * 返回资源的 URI 的句柄 */ URI getURI() throws IOException; /** * 返回资源的 File 的句柄 */ File getFile() throws IOException; /** * 返回 ReadableByteChannel */ default ReadableByteChannel readableChannel() throws IOException { return java.nio.channels.Channels.newChannel(getInputStream()); } /** * 资源内容的长度 */ long contentLength() throws IOException; /** * 资源最后的修改时间 */ long lastModified() throws IOException; /** * 根据资源的相对路径创建新资源 */ Resource createRelative(String relativePath) throws IOException; /** * 资源的文件名 */ @Nullable String getFilename(); /** * 资源的描述 */ String getDescription();} 子类结构 从类结构图可以看到，Resource 根据资源的不同类型提供不同的具体实现 FileSystemResource：对 java.io.File 类型资源的封装。只要是跟 File 打交道的，基本上与 FileSystemResource 也可以打交道。支持文件和 URL 的形式，实现 WritableResource 接口，且从 Spring Framework 5.0 开始，FileSystemResource 使用 NIO2 API 进行读、写交互。 ByteArrayResource：对字节数组提供的数据的封装。如果通过 InputStream 形式访问该类型的资源，该实现会根据字节数组的数据构造一个相应的 ByteArrayInputStream。 UrlResource：对java.net.URL 类型资源的封装。内部委派 URL 进行具体的资源操作。 ClassPathResource：class path 类型资源的实现。使用给定的 ClassLoader 或者给定的 Class 来加载资源。 InputStreamResource：将给定的 InputStream 作为一种资源的 Resource 的实现类。 AbstractResourceorg.springframework.core.io.AbstractResource，为 Resource 接口的默认抽象实现。它实现了 Resource 接口的大部分的公共实现，作为 Resource 接口中德重中之重，其定义如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163public abstract class AbstractResource implements Resource { /** * 判断文件是否存在，若判断过程产生异常（因为会调用SecurityManager来判断），就关闭对应的流 */ @Override public boolean exists() { try { // 基于 File 进行判断 return getFile().exists(); } catch (IOException ex) { // Fall back to stream existence: can we open the stream? // 基于 InputStream 进行判断 try { InputStream is = getInputStream(); is.close(); return true; } catch (Throwable isEx) { return false; } } } /** * 直接返回true，表示可读 */ @Override public boolean isReadable() { return true; } /** * 直接返回 false，表示未被打开 */ @Override public boolean isOpen() { return false; } /** * 直接返回false，表示不为 File */ @Override public boolean isFile() { return false; } /** * 抛出 FileNotFoundException 异常，交给子类实现 */ @Override public URL getURL() throws IOException { throw new FileNotFoundException(getDescription() + \" cannot be resolved to URL\"); } /** * 基于 getURL() 返回的 URL 构建 URI */ @Override public URI getURI() throws IOException { URL url = getURL(); try { return ResourceUtils.toURI(url); } catch (URISyntaxException ex) { throw new NestedIOException(\"Invalid URI [\" + url + \"]\", ex); } } /** * 抛出 FileNotFoundException 异常，交给子类实现 */ @Override public File getFile() throws IOException { throw new FileNotFoundException(getDescription() + \" cannot be resolved to absolute file path\"); } /** * 根据 getInputStream() 的返回结果构建 ReadableByteChannel */ @Override public ReadableByteChannel readableChannel() throws IOException { return Channels.newChannel(getInputStream()); } /** * 获取资源的长度 * * 这个资源内容长度实际就是资源的字节长度，通过全部读取一遍来判断 */ @Override public long contentLength() throws IOException { InputStream is = getInputStream(); try { long size = 0; byte[] buf = new byte[255]; // 每次最多读取 255 字节 int read; while ((read = is.read(buf)) != -1) { size += read; } return size; } finally { try { is.close(); } catch (IOException ex) { } } } /** * 返回资源最后的修改时间 */ @Override public long lastModified() throws IOException { long lastModified = getFileForLastModifiedCheck().lastModified(); if (lastModified == 0L) { throw new FileNotFoundException(getDescription() + \" cannot be resolved in the file system for resolving its last-modified timestamp\"); } return lastModified; } protected File getFileForLastModifiedCheck() throws IOException { return getFile(); } /** * 抛出 FileNotFoundException 异常，交给子类实现 */ @Override public Resource createRelative(String relativePath) throws IOException { throw new FileNotFoundException(\"Cannot create a relative resource for \" + getDescription()); } /** * 获取资源名称，默认返回 null ，交给子类实现 */ @Override @Nullable public String getFilename() { return null; } /** * 返回资源的描述 */ @Override public String toString() { return getDescription(); } @Override public boolean equals(Object obj) { return (obj == this || (obj instanceof Resource &amp;&amp; ((Resource)obj).getDescription().equals(getDescription()))); } @Override public int hashCode() { return getDescription().hashCode(); }} 如果我们想要实现自定义的Resource，记住不要实现 Resource 接口，而应该继承 AbstractResource 抽象类，然后根据当前的具体资源特性覆盖相应的方法即可。 统一资源定位：ResourceLoader一开始就说了 Spring 将资源的定义和资源的加载区分开了，Resource 定义了统一的资源，那资源的加载则由 ResourceLoader 来统一定义。 org.springframework.core.io.ResourceLoader 为 Spring 资源加载的统一抽象，具体的资源加载则由相应的实现类来完成，所以我们可以将 ResourceLoader 称作为统一资源定位器。其定义如下： ResourceLoader，定义资源加载器，主要应用于根据给定的资源文件地址，返回对应的 Resource。 ——《Spring 源码深度解析》P16 123456789public interface ResourceLoader { String CLASSPATH_URL_PREFIX = ResourceUtils.CLASSPATH_URL_PREFIX; // CLASSPATH URL 前缀。默认为：\"classpath:\" Resource getResource(String location); ClassLoader getClassLoader();} getResource(String location) 方法，根据所提供资源的路径 loaction 返回 Resource 实例，但是它不确保该 Resource 一定存在，需要调用 Resource.exist() 方法来判断。 该方法支持以下模式的资源加载： URL 位置资源，如 &quot;file:C:/test.dat&quot;。 ClassPath 位置资源，如 classpath:test.dat。 相对路径资源，如 WEB-INF/test.dat ，此时返回的 Resource 实例，根据实现不同而不同。 该方法的主要实现是在其子类 DefaultResourceLoader 中实现，具体过程我们在分析 DefaultResourceLoader 时做详细说明。 getClassLoader() 方法，返回 ClassLoader 实例，对于想要获取 ResourceLoader 使用的 ClassLoader 用户来说，可以直接调用该方法来获取。在分析 Resource 时，提到了一个类 ClassPathResource，这个类是可以根据指定的 ClassLoader 来加载资源的。 子类结构作为 Spring 统一的资源加载器，它提供了统一的抽象，具体的实现则由相应的子类来负责实现，其类的类结构图如下： DefaultResourceLoader与 AbstractResource 相似，org.springframework.core.io.DefaultResourceLoader 是 ResourceLoader 的默认实现。 构造函数它接收 ClassLoader 作为构造函数的参数，或者使用不带参数的构造函数。 在使用不带参数的构造函数时，使用的 ClassLoader 为默认的 ClassLoader （一般 Thread.currentThread()#getContextClassLoader()）。 在使用带参数的构造函数时，可以通过 ClassUtils#getDefaultClassLoader() 获取 代码如下： 1234567891011121314151617181920@Nullableprivate ClassLoader classLoader;public DefaultResourceLoader() { // 无参构造函数 this.classLoader = ClassUtils.getDefaultClassLoader();}public DefaultResourceLoader(@Nullable ClassLoader classLoader) { // 带 ClassLoader 参数的构造函数 this.classLoader = classLoader;}public void setClassLoader(@Nullable ClassLoader classLoader) { this.classLoader = classLoader;}@Override@Nullablepublic ClassLoader getClassLoader() { return (this.classLoader != null ? this.classLoader : ClassUtils.getDefaultClassLoader());} 另外，也可以调用 #setClassLoader() 方法进行后续设置。 getResource 方法ResourceLoader 中最核心的方法为 #getResource(String location)，它根据提供的 location 返回相应的 Resource。而 DefaultResourceLoader 对该方法提供了核心实现（因为，它的两个子类都没有提供覆盖该方法，所以可以断定 ResourceLoader 的资源加载策略就封装在 DefaultResourceLoader 中），代码如下： 1234567891011121314151617181920212223242526272829303132// DefaultResourceLoader.java@Overridepublic Resource getResource(String location) { Assert.notNull(location, \"Location must not be null\"); // 首先，通过 ProtocolResolver 来加载资源 for (ProtocolResolver protocolResolver : this.protocolResolvers) { Resource resource = protocolResolver.resolve(location, this); if (resource != null) { return resource; } } // 其次，以 / 开头，返回 ClassPathContextResource 类型的资源 if (location.startsWith(\"/\")) { return getResourceByPath(location); // 再次，以 classpath: 开头，返回 ClassPathResource 类型的资源 } else if (location.startsWith(CLASSPATH_URL_PREFIX)) { return new ClassPathResource(location.substring(CLASSPATH_URL_PREFIX.length()), getClassLoader()); // 然后，根据是否为文件 URL ，是则返回 FileUrlResource 类型的资源，否则返回 UrlResource 类型的资源 } else { try { // Try to parse the location as a URL... URL url = new URL(location); return (ResourceUtils.isFileURL(url) ? new FileUrlResource(url) : new UrlResource(url)); } catch (MalformedURLException ex) { // 最后，返回 ClassPathContextResource 类型的资源 // No URL -&gt; resolve as resource path. return getResourceByPath(location); } }} 首先，通过 ProtocolResolver 来加载资源，成功返回 Resource。 其次，若 locaation 以 / 开头，则调用 #getResourceByPath() 方法，构造 ClassPathContextResource 类型资源并返回。代码如下： 123protected Resource getResourceByPath(String path) { return new ClassPathContextResource(path, getClassLoader());} 再次，若 location 以 classpath:开头，则构造 ClassPathResource 类型资源并返回。在构造该资源时，通过 #getClassLoader() 获取当前的 ClassLoader。 然后，构造 URL，尝试通过它进行资源定位，若没有抛出 MalformedURLException 异常，则判断是否为 FileURL，如果是则构造 FileUrlResource 类型，否则构造 UrlResource 类型的资源。 最后，若在加载过程中抛出 MalformedURLException 异常，则委派 #getResourceByPath() 方法，实现资源定位加载。实际上，和「其次」相同。 ProtocolResolverorg.springframework.core.io.ProtocolResolver，用户自定义协议资源解决策略，作为 DefaultResourceLoader 的 SPI：它允许用户自定义资源加载协议，而不需要继承 ResourceLoader 的子类。在介绍 Resource 时，提到如果要实现自定义 Resource，我们只需要继承 AbstractResource 即可，但是有了 ProtocolResolver 后，我们不需要直接继承 DefaultResourceLoader，改为实现 ProtocolResolver 接口也可以实现自定义的 ResourceLoader。 ProtocolResolver 接口，仅有一个方法 Resource resolve(String location, ResourceLoader resourceLoader)。代码如下： 12345678910111213/** * 使用指定的 ResourceLoader ，解析指定的 location 。 * 若成功，则返回对应的 Resource 。 * * Resolve the given location against the given resource loader * if this implementation's protocol matches. * @param location the user-specified resource location 资源路径 * @param resourceLoader the associated resource loader 指定的加载器 ResourceLoader * @return a corresponding {@code Resource} handle if the given location * matches this resolver's protocol, or {@code null} otherwise 返回为相应的 Resource */@NullableResource resolve(String location, ResourceLoader resourceLoader); 在 Spring 中你会发现该接口并没有实现类，它需要用户自定义，自定义的 Resolver 如何加入 Spring 体系呢？调用 DefaultResourceLoader#addProtocolResolver(ProtocolResolver) 方法即可。代码如下： 123456789/** * ProtocolResolver 集合 */private final Set&lt;ProtocolResolver&gt; protocolResolvers = new LinkedHashSet&lt;&gt;(4);public void addProtocolResolver(ProtocolResolver resolver) { Assert.notNull(resolver, \"ProtocolResolver must not be null\"); this.protocolResolvers.add(resolver);} 示例下面示例是演示 DefaultResourceLoader 加载资源的具体策略，代码如下（该示例参考《Spring 揭秘》 P89）： 12345678910111213ResourceLoader resourceLoader = new DefaultResourceLoader();Resource fileResource1 = resourceLoader.getResource(\"D:/Users/von/Documents/spark.txt\");System.out.println(\"fileResource1 is FileSystemResource:\" + (fileResource1 instanceof FileSystemResource));Resource fileResource2 = resourceLoader.getResource(\"/Users/von/Documents/spark.txt\");System.out.println(\"fileResource2 is ClassPathResource:\" + (fileResource2 instanceof ClassPathResource));Resource urlResource1 = resourceLoader.getResource(\"file:/Users/von/Documents/spark.txt\");System.out.println(\"urlResource1 is UrlResource:\" + (urlResource1 instanceof UrlResource));Resource urlResource2 = resourceLoader.getResource(\"http://www.baidu.com\");System.out.println(\"urlResource1 is urlResource:\" + (urlResource2 instanceof UrlResource)); 运行结果： 1234fileResource1 is FileSystemResource:falsefileResource2 is ClassPathResource:trueurlResource1 is UrlResource:trueurlResource1 is urlResource:true 其实对于 fileResource1，我们更加希望是 FileSystemResource 资源类型。但是，事与愿违，它是 ClassPathResource 类型。为什么呢？在 DefaultResourceLoader#getResource() 方法的资源加载策略中，我们知道 D:/Users/von/Documents/spark.txt 地址，其实在该方法中没有相应的资源类型，那么它就会在抛出 MalformedURLException 异常时，通过 DefaultResourceLoader#getResourceByPath(...) 方法，构造一个 ClassPathResource 类型的资源。 而 urlResource1 和 urlResource2，指定有协议前缀的资源路径，则通过 URL 就可以定义，所以返回的都是 UrlResource 类型。 FileSystemResourceLoader从上面的示例，我们看到，其实 DefaultResourceLoader 对 #getResourceByPath(String) 方法处理其实并不是很恰当，这个时候我们可以使用 org.springframework.core.io.FileSystemResourceLoader。它继承 DefaultResourceLoader，且覆写了 #getResourceByPath(String) 方法，使之从文件系统加载资源并以 FileSystemResource 类型返回，这样我们就可以得到想要的资源类型。代码如下： 123456789@Overrideprotected Resource getResourceByPath(String path) { // 截取首 / if (path.startsWith(\"/\")) { path = path.substring(1); } // 创建 FileSystemContextResource 类型的资源 return new FileSystemContextResource(path);} FileSystemContextResourceFileSystemContextResource， 为FileSystemResourceLoader 的内部类，它继承 FileSystemResource 类，实现 ContextResource 接口。代码如下： 123456789101112131415/** * FileSystemResource that explicitly expresses a context-relative path * through implementing the ContextResource interface. */private static class FileSystemContextResource extends FileSystemResource implements ContextResource { public FileSystemContextResource(String path) { super(path); } @Override public String getPathWithinContext() { return getPath(); }} 在构造器中，也是调用 FileSystemResource 的构造函数来构造 FileSystemResource 的。 为什么要有 FileSystemContextResource 类的原因是，实现 ContextResource 接口，并实现对应的 #getPathWithinContext() 接口方法。 示例再回过头看上一节的示例，如果将 DefaultResourceLoader 改为 FileSystemResourceLoader，则 fileResource1 则为 FileSystemResource 类型的资源。 ClassRelativeResourceLoaderorg.springframework.core.io.ClassRelativeResrouceLoader，是 DefaultResourceLoader 的另一个子类的实现。和 FileSystemResourceLoader 类似，在实现代码的结构上类似，也是覆写 #getResourceByPath(String path) 方法，并返回其对应的 ClassRelativeContextResource 的资源类型。 感兴趣的话可以看Spring5：就这一次，搞定资源加载器之ClassRelativeResourceLoader ClassRelativeResourceLoader扩展功能是，可以根据给定的class所在包或者所在包的子包下加载资源。 ResourcePatternResolverResourceLoader 的 Resource getResource(String location) 方法，每次只能根据 location 返回一个 Resource 。当需要加载多个资源时，我们除了多次调用 #getResource(String location) 方法外，别无他法。org.springframework.core.io.support.ResourcePatternResolver 是 ResourceLoader 的扩展，它支持根据指定的资源路径匹配模式每次返回多个 Resource 实例，其定义如下： 1234567public interface ResourcePatternResolver extends ResourceLoader { String CLASSPATH_ALL_URL_PREFIX = \"classpath*:\"; Resource[] getResources(String locationPattern) throws IOException;} ResourcePatternResolver 在 ResourceLoader 的基础上增加了 #getResources(String locationPattern) 方法，以支持根据路径匹配模式返回多个 Resource 实例。 同时，也新增了一种新的协议前缀 &quot;classpath*:&quot;，该协议前缀由其子类负责实现。 PathMatchingResourcePatternResolverorg.springframework.core.io.support.PathMatchingResourcePatternResolver ，为 ResourcePatternResolver 最常用的子类，它除了支持 ResourceLoader 和 ResourcePatternResolver 新增的 &quot;classpath*:&quot; 前缀外，还支持 Ant 风格的路径匹配模式（类似于 &quot;**/*.xml&quot;）。 构造函数PathMatchingResourcePatternResolver 提供了三个构造函数，如下： 123456789101112131415161718192021/** * 内置的 ResourceLoader 资源定位器 */private final ResourceLoader resourceLoader;/** * Ant 路径匹配器 */private PathMatcher pathMatcher = new AntPathMatcher();public PathMatchingResourcePatternResolver() { this.resourceLoader = new DefaultResourceLoader();}public PathMatchingResourcePatternResolver(ResourceLoader resourceLoader) { Assert.notNull(resourceLoader, \"ResourceLoader must not be null\"); this.resourceLoader = resourceLoader;}public PathMatchingResourcePatternResolver(@Nullable ClassLoader classLoader) { this.resourceLoader = new DefaultResourceLoader(classLoader);} PathMatchingResourcePatternResolver 在实例化的时候，可以指定一个 ResourceLoader，如果不指定的话，它会在内部构造一个 DefaultResourceLoader 。 pathMatcher 属性，默认为 AntPathMatcher 对象，用于支持 Ant 类型的路径匹配。 getResource12345678@Overridepublic Resource getResource(String location) { return getResourceLoader().getResource(location);}public ResourceLoader getResourceLoader() { return this.resourceLoader;} 该方法，直接委托给相应的 ResourceLoader 来实现。所以，如果我们在实例化的 PathMatchingResourcePatternResolver 的时候，如果未指定 ResourceLoader 参数的情况下，那么在加载资源时，其实就是 DefaultResourceLoader 的过程。 其实在下面介绍的 Resource[] getResources(String locationPattern) 方法也相同，只不过返回的资源是多个而已。 getResources1234567891011121314151617181920212223242526272829303132@Overridepublic Resource[] getResources(String locationPattern) throws IOException { Assert.notNull(locationPattern, \"Location pattern must not be null\"); // 以 \"classpath*:\" 开头 if (locationPattern.startsWith(CLASSPATH_ALL_URL_PREFIX)) { // 路径包含通配符 // a class path resource (multiple resources for same name possible) if (getPathMatcher().isPattern(locationPattern.substring(CLASSPATH_ALL_URL_PREFIX.length()))) { // a class path resource pattern return findPathMatchingResources(locationPattern); // 路径不包含通配符 } else { // all class path resources with the given name return findAllClassPathResources(locationPattern.substring(CLASSPATH_ALL_URL_PREFIX.length())); } // 不以 \"classpath*:\" 开头 } else { // Generally only look for a pattern after a prefix here, // 通常只在这里的前缀后面查找模式 // and on Tomcat only after the \"*/\" separator for its \"war:\" protocol. 而在 Tomcat 上只有在 “*/ ”分隔符之后才为其 “war:” 协议 int prefixEnd = (locationPattern.startsWith(\"war:\") ? locationPattern.indexOf(\"*/\") + 1 : locationPattern.indexOf(':') + 1); // 路径包含通配符 if (getPathMatcher().isPattern(locationPattern.substring(prefixEnd))) { // a file pattern return findPathMatchingResources(locationPattern); // 路径不包含通配符 } else { // a single resource with the given name return new Resource[] {getResourceLoader().getResource(locationPattern)}; } }} 非 &quot;classpath*:&quot; 开头，且路径不包含通配符，直接委托给相应的 ResourceLoader 来实现。 其他情况，调用 #findAllClassPathResources(...)、或 #findPathMatchingResources(...) 方法，返回多个 Resource 。下面，我们来详细分析。 findAllClassPathResources当 locationPattern 以 &quot;classpath*:&quot; 开头但是不包含通配符，则调用 #findAllClassPathResources(...) 方法加载资源。该方法返回 classes 路径下和所有 jar 包中的所有相匹配的资源。 1234567891011121314protected Resource[] findAllClassPathResources(String location) throws IOException { String path = location; // 去除首个 / if (path.startsWith(\"/\")) { path = path.substring(1); } // 真正执行加载所有 classpath 资源 Set&lt;Resource&gt; result = doFindAllClassPathResources(path); if (logger.isTraceEnabled()) { logger.trace(\"Resolved classpath location [\" + location + \"] to resources \" + result); } // 转换成 Resource 数组返回 return result.toArray(new Resource[0]);} 真正执行加载的是在 #doFindAllClassPathResources(...) 方法，代码如下： 12345678910111213141516171819protected Set&lt;Resource&gt; doFindAllClassPathResources(String path) throws IOException { Set&lt;Resource&gt; result = new LinkedHashSet&lt;&gt;(16); ClassLoader cl = getClassLoader(); // &lt;1&gt; 根据 ClassLoader 加载路径下的所有资源 Enumeration&lt;URL&gt; resourceUrls = (cl != null ? cl.getResources(path) : ClassLoader.getSystemResources(path)); // &lt;2&gt; while (resourceUrls.hasMoreElements()) { URL url = resourceUrls.nextElement(); // 将 URL 转换成 UrlResource result.add(convertClassLoaderURL(url)); } // &lt;3&gt; 加载路径下得所有 jar 包 if (\"\".equals(path)) { // The above result is likely to be incomplete, i.e. only containing file system references. // We need to have pointers to each of the jar files on the classpath as well... addAllClassLoaderJarRoots(cl, result); } return result;} &lt;1&gt; 处，根据 ClassLoader 加载路径下的所有资源。在加载资源过程时，如果在构造 PathMatchingResourcePatternResolver 实例的时候如果传入了 ClassLoader，则调用该 ClassLoader 的 #getResources() 方法，否则调用 ClassLoader#getSystemResources(path) 方法。另外，ClassLoader#getResources() 方法，代码如下: 12345678910111213// java.lang.ClassLoader.javapublic Enumeration&lt;URL&gt; getResources(String name) throws IOException { @SuppressWarnings(\"unchecked\") Enumeration&lt;URL&gt;[] tmp = (Enumeration&lt;URL&gt;[]) new Enumeration&lt;?&gt;[2]; if (parent != null) { tmp[0] = parent.getResources(name); } else { tmp[0] = getBootstrapResources(name); } tmp[1] = findResources(name); return new CompoundEnumeration&lt;&gt;(tmp);} 看到这里是不是就已经一目了然了？如果当前父类加载器不为 null ，则通过父类向上迭代获取资源，否则调用 #getBootstrapResources() 。这里是不是特别熟悉。 &lt;2&gt; 处，遍历 URL 集合，调用 #convertClassLoaderURL(URL url) 方法，将 URL 转换成 UrlResource 对象。代码如下： 123protected Resource convertClassLoaderURL(URL url) { return new UrlResource(url);} &lt;3&gt; 处，若 path 为空（“”）时，则调用 #addAllClassLoaderJarRoots(...)方法。该方法主要是加载路径下得所有 jar 包，方法较长也没有什么实际意义就不贴出来了。感兴趣的胖友，自己可以去看看。当然，可能代码也比较长哈。 通过上面的分析，我们知道 #findAllClassPathResources(...) 方法，其实就是利用 ClassLoader 来加载指定路径下的资源，不论它是在 class 路径下还是在 jar 包中。如果我们传入的路径为空或者 /，则会调用 #addAllClassLoaderJarRoots(...) 方法，加载所有的 jar 包。 findPathMatchingResources当 locationPattern 中包含了通配符，则调用该方法进行资源加载。代码如下： 123456789101112131415161718192021222324252627282930313233343536protected Resource[] findPathMatchingResources(String locationPattern) throws IOException { // 确定根路径、子路径 String rootDirPath = determineRootDir(locationPattern); String subPattern = locationPattern.substring(rootDirPath.length()); // 获取根据路径下的资源 Resource[] rootDirResources = getResources(rootDirPath); // 遍历，迭代 Set&lt;Resource&gt; result = new LinkedHashSet&lt;&gt;(16); for (Resource rootDirResource : rootDirResources) { rootDirResource = resolveRootDirResource(rootDirResource); URL rootDirUrl = rootDirResource.getURL(); // bundle 资源类型 if (equinoxResolveMethod != null &amp;&amp; rootDirUrl.getProtocol().startsWith(\"bundle\")) { URL resolvedUrl = (URL) ReflectionUtils.invokeMethod(equinoxResolveMethod, null, rootDirUrl); if (resolvedUrl != null) { rootDirUrl = resolvedUrl; } rootDirResource = new UrlResource(rootDirUrl); } // vfs 资源类型 if (rootDirUrl.getProtocol().startsWith(ResourceUtils.URL_PROTOCOL_VFS)) { result.addAll(VfsResourceMatchingDelegate.findMatchingResources(rootDirUrl, subPattern, getPathMatcher())); // jar 资源类型 } else if (ResourceUtils.isJarURL(rootDirUrl) || isJarResource(rootDirResource)) { result.addAll(doFindPathMatchingJarResources(rootDirResource, rootDirUrl, subPattern)); // 其它资源类型 } else { result.addAll(doFindPathMatchingFileResources(rootDirResource, subPattern)); } } if (logger.isTraceEnabled()) { logger.trace(\"Resolved location pattern [\" + locationPattern + \"] to resources \" + result); } // 转换成 Resource 数组返回 return result.toArray(new Resource[0]);} 方法有点儿长，但是思路还是很清晰的，主要分两步： 确定目录，获取该目录下得所有资源。 在所获得的所有资源后，进行迭代匹配获取我们想要的资源。 在这个方法里面，我们要关注两个方法，一个是 #determineRootDir(String location) 方法，一个是 #doFindPathMatchingXXXResources(...) 等方法。 determineRootDirdetermineRootDir(String location) 方法，主要是用于确定根路径。代码如下： 1234567891011121314151617181920212223242526272829/** * Determine the root directory for the given location. * &lt;p&gt;Used for determining the starting point for file matching, * resolving the root directory location to a {@code java.io.File} * and passing it into {@code retrieveMatchingFiles}, with the * remainder of the location as pattern. * &lt;p&gt;Will return \"/WEB-INF/\" for the pattern \"/WEB-INF/*.xml\", * for example. * @param location the location to check * @return the part of the location that denotes the root directory * @see #retrieveMatchingFiles */protected String determineRootDir(String location) { // 找到冒号的后一位 int prefixEnd = location.indexOf(':') + 1; // 根目录结束位置 int rootDirEnd = location.length(); // 在从冒号开始到最后的字符串中，循环判断是否包含通配符，如果包含，则截断最后一个由”/”分割的部分。 // 例如：在我们路径中，就是最后的ap?-context.xml这一段。再循环判断剩下的部分，直到剩下的路径中都不包含通配符。 while (rootDirEnd &gt; prefixEnd &amp;&amp; getPathMatcher().isPattern(location.substring(prefixEnd, rootDirEnd))) { rootDirEnd = location.lastIndexOf('/', rootDirEnd - 2) + 1; } // 如果查找完成后，rootDirEnd = 0 了，则将之前赋值的 prefixEnd 的值赋给 rootDirEnd ，也就是冒号的后一位 if (rootDirEnd == 0) { rootDirEnd = prefixEnd; } // 截取根目录 return location.substring(0, rootDirEnd);} 方法比较绕，效果如下示例： 原路径 确定根路径 classpath*:test/cc*/spring-*.xml classpath*:test/ classpath*:test/aa/spring-*.xml classpath*:test/aa/ doFindPathMatchingXXXResources#doFindPathMatchingXXXResources(...) 方法，是个泛指，一共对应三个方法： #doFindPathMatchingJarResources(rootDirResource, rootDirUrl, subPatter) 方法 #doFindPathMatchingFileResources(rootDirResource, subPattern) 方法 VfsResourceMatchingDelegate#findMatchingResources(rootDirUrl, subPattern, pathMatcher) 方法 因为本文重在分析 Spring 统一资源加载策略的整体流程。相对来说，上面几个方法的代码量会比较多。所以本文不再追溯，感兴趣的胖友，推荐阅读如下文章： 《Spring源码情操陶冶-PathMatchingResourcePatternResolver路径资源匹配溶解器》 ，主要针对 #doFindPathMatchingJarResources(rootDirResource, rootDirUrl, subPatter) 方法。 《深入 Spring IoC 源码之 ResourceLoader》 ，主要针对 #doFindPathMatchingFileResources(rootDirResource, subPattern) 方法。 《Spring 源码学习 —— 含有通配符路径解析（上）》 😈 貌似没有下 小结至此 Spring 整个资源记载过程已经分析完毕。下面简要总结下： Spring 提供了 Resource 和 ResourceLoader 来统一抽象整个资源及其定位。使得资源与资源的定位有了一个更加清晰的界限，并且提供了合适的 Default 类，使得自定义实现更加方便和清晰。 AbstractResource 为 Resource 的默认抽象实现，它对 Resource 接口做了一个统一的实现，子类继承该类后只需要覆盖相应的方法即可，同时对于自定义的 Resource 我们也是继承该类。 DefaultResourceLoader 同样也是 ResourceLoader 的默认实现，在自定 ResourceLoader 的时候我们除了可以继承该类外还可以实现 ProtocolResolver 接口来实现自定资源加载协议。 DefaultResourceLoader 每次只能返回单一的资源，所以 Spring 针对这个提供了另外一个接口 ResourcePatternResolver ，该接口提供了根据指定的 locationPattern 返回多个资源的策略。其子类 PathMatchingResourcePatternResolver 是一个集大成者的 ResourceLoader ，因为它即实现了 Resource getResource(String location) 方法，也实现了 Resource[] getResources(String locationPattern) 方法。 另外，如果胖友认真的看了本文的包结构，我们可以发现，Resource 和 ResourceLoader 核心是在，spring-core 项目中。 如果想要调试本小节的相关内容，可以直接使用 Resource 和 ResourceLoader 相关的 API ，进行操作调试。 参考摘要: 原创出处 http://cmsblogs.com/?p=2656 「小明哥」，略作修改及补充","link":"/2020/08/22/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91IoC%E4%B9%8BSpring%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E5%8A%A0%E8%BD%BD%E7%AD%96%E7%95%A5/"},{"title":"【Spring源码】IoC之加载 BeanDefinition","text":"Ioc 容器的使用整个过程就分为三个步骤：资源定位、装在、注册，本文分享的就是装载这个步骤。 先看一段熟悉的代码： 1234ClassPathResource resource = new ClassPathResource(\"bean.xml\"); // &lt;1&gt;DefaultListableBeanFactory factory = new DefaultListableBeanFactory(); // &lt;2&gt;XmlBeanDefinitionReader reader = new XmlBeanDefinitionReader(factory); // &lt;3&gt;reader.loadBeanDefinitions(resource); // &lt;4&gt; 这段代码是 Spring 中编程式使用 IoC 容器，通过这四段简单的代码，我们可以初步判断 IoC 容器的使用过程。 获取资源 获取 BeanFactory 根据新建的 BeanFactory 创建一个 BeanDefinitionReader 对象，该 Reader 对象为资源的解析器 装载资源 整个过程就分为三个步骤：资源定位、装载、注册，如下： 资源定位。我们一般用外部资源来描述 Bean 对象，所以在初始化 IoC 容器的第一步就是需要定位这个外部资源。在上一篇博客（《【Spring源码】IoC之Spring统一资源加载策略》）已经详细说明了资源加载的过程。 装载。装载就是 BeanDefinition 的载入。BeanDefinitionReader 读取、解析 Resource 资源，也就是将用户定义的 Bean 表示成 IoC 容器的内部数据结构：BeanDefinition 。 在 IoC 容器内部维护着一个 BeanDefinition Map 的数据结构 在配置文件中每一个 &lt;bean&gt; 都对应着一个 BeanDefinition 对象。 本文，我们分享的就是装载这个步骤。 FROM 《Spring 源码深度解析》P16 页BeanDefinitionReader ，主要定义资源文件读取并转换为 BeanDefinition 的各个功能。 注册。向 IoC 容器注册在第二步解析好的 BeanDefinition，这个过程是通过 BeanDefinitionRegistry 接口来实现的。在 IoC 容器内部其实是将第二个过程解析得到的 BeanDefinition 注入到一个 HashMap 容器中，IoC 容器就是通过这个 HashMap 来维护这些 BeanDefinition 的。 在这里需要注意的一点是这个过程并没有完成依赖注入（Bean 创建），Bean 创建是发生在应用第一次调用 #getBean(...) 方法，向容器索要 Bean 时。 当然我们可以通过设置预处理，即对某个 Bean 设置 lazyinit = false 属性，那么这个 Bean 的依赖注入就会在容器初始化的时候完成。 FROM 本人 简单的说，上面步骤的结果是，XML Resource =&gt; XML Document =&gt; Bean Definition 。 loadBeanDefinitions资源定位在前面已经分析了，下面我们直接分析加载，上面看到的 reader.loadBeanDefinitions(resource) 代码，才是加载资源的真正实现，所以我们直接从该方法入手。代码如下： 12345// XmlBeanDefinitionReader.java@Overridepublic int loadBeanDefinitions(Resource resource) throws BeanDefinitionStoreException { return loadBeanDefinitions(new EncodedResource(resource));} 从指定的 xml 文件加载 Bean Definition ，这里会先对 Resource 资源封装成 org.springframework.core.io.support.EncodedResource 对象。这里为什么需要将 Resource 封装成 EncodedResource 呢？主要是为了对 Resource 进行编码，保证内容读取的正确性。 然后，再调用 #loadBeanDefinitions(EncodedResource encodedResource) 方法，执行真正的逻辑实现。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 当前线程，正在加载的 EncodedResource 集合。 */private final ThreadLocal&lt;Set&lt;EncodedResource&gt;&gt; resourcesCurrentlyBeingLoaded = new NamedThreadLocal&lt;&gt;(\"XML bean definition resources currently being loaded\");public int loadBeanDefinitions(EncodedResource encodedResource) throws BeanDefinitionStoreException { Assert.notNull(encodedResource, \"EncodedResource must not be null\"); if (logger.isTraceEnabled()) { logger.trace(\"Loading XML bean definitions from \" + encodedResource); } // &lt;1&gt; 获取已经加载过的资源 Set&lt;EncodedResource&gt; currentResources = this.resourcesCurrentlyBeingLoaded.get(); if (currentResources == null) { currentResources = new HashSet&lt;&gt;(4); this.resourcesCurrentlyBeingLoaded.set(currentResources); } if (!currentResources.add(encodedResource)) { // 将当前资源加入记录中。如果已存在，抛出异常 throw new BeanDefinitionStoreException(\"Detected cyclic loading of \" + encodedResource + \" - check your import definitions!\"); } try { // &lt;2&gt; 从 EncodedResource 获取封装的 Resource ，并从 Resource 中获取其中的 InputStream InputStream inputStream = encodedResource.getResource().getInputStream(); try { InputSource inputSource = new InputSource(inputStream); if (encodedResource.getEncoding() != null) { // 设置编码 inputSource.setEncoding(encodedResource.getEncoding()); } // 核心逻辑部分，执行加载 BeanDefinition return doLoadBeanDefinitions(inputSource, encodedResource.getResource()); } finally { inputStream.close(); } } catch (IOException ex) { throw new BeanDefinitionStoreException(\"IOException parsing XML document from \" + encodedResource.getResource(), ex); } finally { // 从缓存中剔除该资源 &lt;3&gt; currentResources.remove(encodedResource); if (currentResources.isEmpty()) { this.resourcesCurrentlyBeingLoaded.remove(); } }} &lt;1&gt;处，通过resourcesCurrentlyBeingLoaded.get()代码，来获取已经加载过的资源，然后将encodedResource加入其中，如果resourcesCurrentlyBeingLoaded中已经存在该资源，则抛出BeanDefinitionStoreException 异常。 为什么需要这么做呢？答案在 &quot;Detected cyclic loading&quot; ，避免一个 EncodedResource 在加载时，还没加载完成，又加载自身，从而导致死循环。 也因此，在 &lt;3&gt; 处，当一个 EncodedResource 加载完成后，需要从缓存中剔除。 &lt;2&gt; 处理，从 encodedResource 获取封装的 Resource 资源，并从 Resource 中获取相应的 InputStream ，然后将 InputStream 封装为 InputSource ，最后调用 #doLoadBeanDefinitions(InputSource inputSource, Resource resource) 方法，执行加载 Bean Definition 的真正逻辑。 doLoadBeanDefinitions123456789101112131415161718192021222324252627282930313233343536373839/** * Actually load bean definitions from the specified XML file. * @param inputSource the SAX InputSource to read from * @param resource the resource descriptor for the XML file * @return the number of bean definitions found * @throws BeanDefinitionStoreException in case of loading or parsing errors * @see #doLoadDocument * @see #registerBeanDefinitions */protected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException { try { // &lt;1&gt; 获取 XML Document 实例 Document doc = doLoadDocument(inputSource, resource); // &lt;2&gt; 根据 Document 实例，注册 Bean 信息 int count = registerBeanDefinitions(doc, resource); if (logger.isDebugEnabled()) { logger.debug(\"Loaded \" + count + \" bean definitions from \" + resource); } return count; } catch (BeanDefinitionStoreException ex) { throw ex; } catch (SAXParseException ex) { throw new XmlBeanDefinitionStoreException(resource.getDescription(), \"Line \" + ex.getLineNumber() + \" in XML document from \" + resource + \" is invalid\", ex); } catch (SAXException ex) { throw new XmlBeanDefinitionStoreException(resource.getDescription(), \"XML document from \" + resource + \" is invalid\", ex); } catch (ParserConfigurationException ex) { throw new BeanDefinitionStoreException(resource.getDescription(), \"Parser configuration exception parsing XML from \" + resource, ex); } catch (IOException ex) { throw new BeanDefinitionStoreException(resource.getDescription(), \"IOException parsing XML document from \" + resource, ex); } catch (Throwable ex) { throw new BeanDefinitionStoreException(resource.getDescription(), \"Unexpected exception parsing XML document from \" + resource, ex); }} 在 &lt;1&gt; 处，调用 #doLoadDocument(InputSource inputSource, Resource resource) 方法，根据 xml 文件，获取 Document 实例。 在 &lt;2&gt; 处，调用 #registerBeanDefinitions(Document doc, Resource resource) 方法，根据获取的 Document 实例，注册 Bean 信息。 doLoadDocument123456789101112131415/** * 获取 XML Document 实例 * * Actually load the specified document using the configured DocumentLoader. * @param inputSource the SAX InputSource to read from * @param resource the resource descriptor for the XML file * @return the DOM Document * @throws Exception when thrown from the DocumentLoader * @see #setDocumentLoader * @see DocumentLoader#loadDocument */protected Document doLoadDocument(InputSource inputSource, Resource resource) throws Exception { return this.documentLoader.loadDocument(inputSource, getEntityResolver(), this.errorHandler, getValidationModeForResource(resource), isNamespaceAware());} 调用 #getValidationModeForResource(Resource resource) 方法，获取指定资源（xml）的验证模式。详细解析，见 《【死磕 Spring】—— IoC 之获取验证模型》 。 调用 DocumentLoader#loadDocument(InputSource inputSource, EntityResolver entityResolver, ErrorHandler errorHandler, int validationMode, boolean namespaceAware) 方法，获取 XML Document 实例。详细解析，见 《【死磕 Spring】—— IoC 之获取 Document 对象》 。 registerBeanDefinitions该方法的详细解析，见 《【死磕 Spring】—— IoC 之注册 BeanDefinition》 。 参考原创出处 http://cmsblogs.com/?p=2658 「小明哥」，略作修改及补充","link":"/2020/10/05/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91IoC%E4%B9%8B%E5%8A%A0%E8%BD%BD-BeanDefinition/"},{"title":"Java实现批量去除文件、文件夹的名称中指定的字符","text":"有时候下载的文件/文件夹会带有广告后缀，通过Java实现批量去除 实现效果Before After 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.we;import java.io.File;/** * 批量去除文件、文件夹的名称中指定的字符 */public class ClearAdvert { //AD为广告内容 public static final String AD = \"要去除的字符\"; public static int fileNum = 0; public static void main(String[] args) { //文件夹路径名 String rootPath = \"指定的文件（夹）路径\"; scanFile(rootPath); System.out.println(\"共去广告\" + fileNum + \"个文件\"); } /* * 递归调用查找指定文件夹下所有文件 */ public static void scanFile(String path) { File dirFile = reName(new File(path)); System.out.println(dirFile.getAbsolutePath()); if (dirFile.isDirectory()){ String[] fileList = dirFile.list(); for (int i = 0; i &lt; fileList.length; i++) { // windows系统写法 path = dirFile.getAbsolutePath() + \"\\\\\" + fileList[i]; // macOS系统写法 // path = dirFile.getAbsolutePath() + \"/\" + fileList[i]; scanFile(path); } } } public static File reName(File oldFile) { //不带路径的文件名 String originalName = oldFile.getName(); if (originalName.contains(AD)) { //带路径的文件名 String oldFilePath = oldFile.getAbsolutePath();// 目录路径 String newFilePath = oldFilePath.replace(AD, \"\"); File newFile = new File(newFilePath); if (oldFile.renameTo(newFile)) { fileNum++; return newFile; } } return oldFile; }} 参考https://blog.csdn.net/qq_41616414/article/details/102793368 （增加macOS路径写法）","link":"/2020/08/22/Java%E5%AE%9E%E7%8E%B0%E6%89%B9%E9%87%8F%E5%8E%BB%E9%99%A4%E6%96%87%E4%BB%B6%E3%80%81%E6%96%87%E4%BB%B6%E5%A4%B9%E7%9A%84%E5%90%8D%E7%A7%B0%E4%B8%AD%E6%8C%87%E5%AE%9A%E7%9A%84%E5%AD%97%E7%AC%A6/"},{"title":"【Spring源码】IoC之注册BeanDefinitions","text":"在【Spring源码】IoC之加载 BeanDefinition中提到，在核心逻辑方法#doLoadBeanDefinitions(InputSource inputSource, Resource resource) 方法中，中主要是做三件事情： 调用 #getValidationModeForResource(Resource resource) 方法，获取指定资源（xml）的验证模式。 调用 DocumentLoader#loadDocument(InputSource inputSource, EntityResolver entityResolver,ErrorHandler errorHandler, int validationMode, boolean namespaceAware) 方法，获取 XML Document 实例。 调用 #registerBeanDefinitions(Document doc, Resource resource) 方法，根据获取的 Document 实例，注册 Bean 信息。 这篇博客主要第 3 步，分析注册 Bean 信息。 获取 XML Document 对象后，会根据该对象和 Resource 资源对象调用 XmlBeanDefinitionReader#registerBeanDefinitions(Document doc, Resource resource) 方法，开始注册 BeanDefinitions 之旅。代码如下： 123456789101112131415// AbstractBeanDefinitionReader.javaprivate final BeanDefinitionRegistry registry;// XmlBeanDefinitionReader.javapublic int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException { // &lt;1&gt; 创建 BeanDefinitionDocumentReader 对象 BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); // &lt;2&gt; 获取已注册的 BeanDefinition 数量 int countBefore = getRegistry().getBeanDefinitionCount(); // &lt;3&gt; 创建 XmlReaderContext 对象 // &lt;4&gt; 注册 BeanDefinition documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); // &lt;5&gt; 计算新注册的 BeanDefinition 数量 return getRegistry().getBeanDefinitionCount() - countBefore;} &lt;1&gt; 处，调用 #createBeanDefinitionDocumentReader() 方法，实例化 BeanDefinitionDocumentReader 对象。 FROM 《Spring 源码深度解析》P16页 定义读取 Document 并注册 BeanDefinition 功能 &lt;2&gt; 处，调用 BeanDefinitionRegistry#getBeanDefinitionCount() 方法，获取已注册的 BeanDefinition 数量。 &lt;3&gt; 处，调用 #createReaderContext(Resource resource) 方法，创建 XmlReaderContext 对象。 &lt;4&gt; 处，调用 BeanDefinitionDocumentReader#registerBeanDefinitions(Document doc, XmlReaderContext readerContext) 方法，读取 XML 元素，注册 BeanDefinition 们。 &lt;5&gt; 处，计算新注册的 BeanDefinition 数量。 createBeanDefinitionDocumentReader#createBeanDefinitionDocumentReader()，实例化 BeanDefinitionDocumentReader 对象。代码如下： 12345678910/** * documentReader 的类 * * @see #createBeanDefinitionDocumentReader() */private Class&lt;? extends BeanDefinitionDocumentReader&gt; documentReaderClass = DefaultBeanDefinitionDocumentReader.class;protected BeanDefinitionDocumentReader createBeanDefinitionDocumentReader() { return BeanUtils.instantiateClass(this.documentReaderClass);} documentReaderClass的默认值为 DefaultBeanDefinitionDocumentReader.class 。关于它，我们在后续的文章，详细解析。 registerBeanDefinitionsBeanDefinitionDocumentReader#registerBeanDefinitions(Document doc, XmlReaderContext readerContext) 方法，注册 BeanDefinition ，在接口 BeanDefinitionDocumentReader 中定义。代码如下： 123456789101112public interface BeanDefinitionDocumentReader { /** * Read bean definitions from the given DOM document and * register them with the registry in the given reader context. * @param doc the DOM document * @param readerContext the current context of the reader * (includes the target registry and the resource being parsed) * @throws BeanDefinitionStoreException in case of parsing errors */ void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) throws BeanDefinitionStoreException;} 从给定的 Document 对象中解析定义的 BeanDefinition 并将他们注册到注册表中。方法接收两个参数： doc 方法参数：待解析的 Document 对象。 readerContext 方法，解析器的当前上下文，包括目标注册表和被解析的资源。它是根据 Resource 来创建的，见下文。 DefaultBeanDefinitionDocumentReaderBeanDefinitionDocumentReader 有且只有一个默认实现类 DefaultBeanDefinitionDocumentReader 。它对 #registerBeanDefinitions(...) 方法的实现代码如下： DefaultBeanDefinitionDocumentReader 对该方法提供了实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@Nullableprivate XmlReaderContext readerContext;@Nullableprivate BeanDefinitionParserDelegate delegate; /** * This implementation parses bean definitions according to the \"spring-beans\" XSD * (or DTD, historically). * &lt;p&gt;Opens a DOM Document; then initializes the default settings * specified at the {@code &lt;beans/&gt;} level; then parses the contained bean definitions. */@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) { this.readerContext = readerContext; // 获得 XML Document Root Element // 执行注册 BeanDefinition doRegisterBeanDefinitions(doc.getDocumentElement());}/** * Register each bean definition within the given root {@code &lt;beans/&gt;} element. */@SuppressWarnings(\"deprecation\") // for Environment.acceptsProfiles(String...)protected void doRegisterBeanDefinitions(Element root) { // Any nested &lt;beans&gt; elements will cause recursion in this method. In // order to propagate and preserve &lt;beans&gt; default-* attributes correctly, // keep track of the current (parent) delegate, which may be null. Create // the new (child) delegate with a reference to the parent for fallback purposes, // then ultimately reset this.delegate back to its original (parent) reference. // this behavior emulates a stack of delegates without actually necessitating one. // 记录老的 BeanDefinitionParserDelegate 对象 BeanDefinitionParserDelegate parent = this.delegate; // &lt;1&gt; 创建 BeanDefinitionParserDelegate 对象，并进行设置到 delegate this.delegate = createDelegate(getReaderContext(), root, parent); // &lt;2&gt; 检查 &lt;beans /&gt; 根标签的命名空间是否为空，或者是 http://www.springframework.org/schema/beans if (this.delegate.isDefaultNamespace(root)) { // &lt;2.1&gt; 处理 profile 属性。可参见《Spring3自定义环境配置 &lt;beans profile=\"\"&gt;》http://nassir.iteye.com/blog/1535799 String profileSpec = root.getAttribute(PROFILE_ATTRIBUTE); if (StringUtils.hasText(profileSpec)) { // &lt;2.2&gt; 使用分隔符切分，可能有多个 profile 。 String[] specifiedProfiles = StringUtils.tokenizeToStringArray( profileSpec, BeanDefinitionParserDelegate.MULTI_VALUE_ATTRIBUTE_DELIMITERS); // &lt;2.3&gt; 如果所有 profile 都无效，则不进行注册 // We cannot use Profiles.of(...) since profile expressions are not supported // in XML config. See SPR-12458 for details. if (!getReaderContext().getEnvironment().acceptsProfiles(specifiedProfiles)) { if (logger.isDebugEnabled()) { logger.debug(\"Skipped XML bean definition file due to specified profiles [\" + profileSpec + \"] not matching: \" + getReaderContext().getResource()); } return; } } } // &lt;3&gt; 解析前处理 preProcessXml(root); // &lt;4&gt; 解析 parseBeanDefinitions(root, this.delegate); // &lt;5&gt; 解析后处理 postProcessXml(root); // 设置 delegate 回来的 BeanDefinitionParserDelegate 对象 this.delegate = parent;} &lt;1&gt; 处，创建 BeanDefinitionParserDelegate 对象，并进行设置到 delegate 。BeanDefinitionParserDelegate 是一个重要的类，它负责解析 BeanDefinition。代码如下： FROM 《Spring 源码深度解析》P16 定义解析 XML Element 的各种方法 12345678protected BeanDefinitionParserDelegate createDelegate( XmlReaderContext readerContext, Element root, @Nullable BeanDefinitionParserDelegate parentDelegate) { // 创建 BeanDefinitionParserDelegate 对象 BeanDefinitionParserDelegate delegate = new BeanDefinitionParserDelegate(readerContext); // 初始化默认 delegate.initDefaults(root, parentDelegate); return delegate;} &lt;2&gt; 处，检查 &lt;beans /&gt; 根标签的命名空间是否为空，或者是 http://www.springframework.org/schema/beans 。 &lt;2.1&gt; 处，判断是否 &lt;beans /&gt; 上配置了 profile 属性。不了解这块的胖友，可以看下 《Spring3自定义环境配置 》 。 &lt;2.2&gt; 处，使用分隔符切分，可能有多个 profile 。 &lt;2.3&gt; 处，判断，如果所有 profile 都无效，则 return 不进行注册。 &lt;4&gt; 处，调用 #parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) 方法，进行解析逻辑。 &lt;3&gt; / &lt;5&gt; 处，解析前后的处理，目前这两个方法都是空实现，交由子类来实现。代码如下： 123protected void preProcessXml(Element root) {}protected void postProcessXml(Element root) {} parseBeanDefinitions#parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) 方法，进行解析逻辑。代码如下： 12345678910111213141516171819202122232425262728/** * Parse the elements at the root level in the document: * \"import\", \"alias\", \"bean\". * @param root the DOM root element of the document */protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate) { // &lt;1&gt; 如果根节点使用默认命名空间，执行默认解析 if (delegate.isDefaultNamespace(root)) { // 遍历子节点 NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) { Node node = nl.item(i); if (node instanceof Element) { Element ele = (Element) node; // &lt;1&gt; 如果该节点使用默认命名空间，执行默认解析 if (delegate.isDefaultNamespace(ele)) { parseDefaultElement(ele, delegate); // 如果该节点非默认命名空间，执行自定义解析 } else { delegate.parseCustomElement(ele); } } } // &lt;2&gt; 如果根节点非默认命名空间，执行自定义解析 } else { delegate.parseCustomElement(root); }} Spring 有两种 Bean 声明方式： 配置文件式声明：&lt;bean id=&quot;studentService&quot; class=&quot;org.springframework.core.StudentService&quot; /&gt; 。对应 &lt;1&gt; 处。 自定义注解方式：&lt;tx:annotation-driven&gt; 。对应 &lt;2&gt; 处。 &lt;1&gt; 处，如果根节点或子节点使用默认命名空间，调用 #parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) 方法，执行默认解析。代码如下： 123456789101112private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) { if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) { // import importBeanDefinitionResource(ele); } else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) { // alias processAliasRegistration(ele); } else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) { // bean processBeanDefinition(ele, delegate); } else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) { // beans // recurse doRegisterBeanDefinitions(ele); }} &lt;2&gt; 处，如果根节点或子节点不使用默认命名空间，调用 BeanDefinitionParserDelegate#parseCustomElement(Element ele) 方法，执行自定义解析。详细的解析，见后续文章。 createReaderContext#createReaderContext(Resource resource) 方法，创建 XmlReaderContext 对象。代码如下： 12345678910111213141516private ProblemReporter problemReporter = new FailFastProblemReporter();private ReaderEventListener eventListener = new EmptyReaderEventListener();private SourceExtractor sourceExtractor = new NullSourceExtractor();@Nullableprivate NamespaceHandlerResolver namespaceHandlerResolver;/** * Create the {@link XmlReaderContext} to pass over to the document reader. */public XmlReaderContext createReaderContext(Resource resource) { return new XmlReaderContext(resource, this.problemReporter, this.eventListener, this.sourceExtractor, this, getNamespaceHandlerResolver());} 小结至此，XmlBeanDefinitionReader#doLoadBeanDefinitions(InputSource inputSource, Resource resource) 方法中，做的三件事情已经全部分析完毕，下面将对 BeanDefinition 的解析过程做详细分析说明。 另外，XmlBeanDefinitionReader#doLoadBeanDefinitions(InputSource inputSource, Resource resource) 方法，整体时序图如下： 红框部分，就是 BeanDefinition 的解析过程。 参考摘要: 原创出处 http://cmsblogs.com/?p=2697 「小明哥」，略作修改及补充","link":"/2020/10/12/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91IoC%E4%B9%8B%E6%B3%A8%E5%86%8CBeanDefinitions/"},{"title":"Mac使用Marginnote3时，启动会自动打开固定某一个笔记解决方案","text":"本文记录使用Marginnote3时，第一次启动会固定打开某一个笔记本的解决方案 这个问题是笔记本链接的问题 打开 12~/Library/Containers/QReader.MarginStudyMac/Data/Library/Preferences~/Library/Containers/QReader.MarginStudyMac/Data/Library/SyncedPreferences 这两个目录下， 删掉“QReader.MarginStudyMac.plist”这个文件。 打开云同步再同步一遍笔记即可。","link":"/2020/07/22/Mac%E4%BD%BF%E7%94%A8Marginnote3%E6%97%B6%EF%BC%8C%E5%90%AF%E5%8A%A8%E4%BC%9A%E8%87%AA%E5%8A%A8%E6%89%93%E5%BC%80%E5%9B%BA%E5%AE%9A%E6%9F%90%E4%B8%80%E4%B8%AA%E7%AC%94%E8%AE%B0%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"【Spring源码】IoC之解析Bean：解析import标签","text":"在【Spring源码】IoC之注册BeanDefinitions 提到，Spring 有两种解析 Bean 的方式： 如果根节点或者子节点采用默认命名空间的话，则调用 #parseDefaultElement(...) 方法，进行默认标签解析 否则，调用 BeanDefinitionParserDelegate#parseCustomElement(...) 方法，进行自定义解析。 本文就对这两个方法进行详细分析说明。 默认标签解析过程12345678910111213141516171819// DefaultBeanDefinitionDocumentReader.javapublic static final String IMPORT_ELEMENT = \"import\";public static final String ALIAS_ATTRIBUTE = \"alias\";public static final String BEAN_ELEMENT = BeanDefinitionParserDelegate.BEAN_ELEMENT;public static final String NESTED_BEANS_ELEMENT = \"beans\";private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) { if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) { // import importBeanDefinitionResource(ele); } else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) { // alias processAliasRegistration(ele); } else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) { // bean processBeanDefinition(ele, delegate); } else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) { // beans // recurse doRegisterBeanDefinitions(ele); }} 该方法的功能一目了然，分别是对四种不同的标签进行解析，分别是 import、alias、bean、beans 。咱门从第一个标签 import 开始。 import 示例经历过 Spring 配置文件的小伙伴都知道，如果工程比较大，配置文件的维护会让人觉得恐怖，文件太多了，想象将所有的配置都放在一个 spring.xml 配置文件中，哪种后怕感是不是很明显？ 所有针对这种情况 Spring 提供了一个分模块的思路，利用 import 标签，例如我们可以构造一个这样的 spring.xml 。 1234567891011&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;import resource=\"spring-student.xml\"/&gt; &lt;import resource=\"spring-student-dtd.xml\"/&gt;&lt;/beans&gt; spring.xml 配置文件中，使用 import 标签的方式导入其他模块的配置文件。 如果有配置需要修改直接修改相应配置文件即可。 若有新的模块需要引入直接增加 import 即可。 这样大大简化了配置后期维护的复杂度，同时也易于管理。 importBeanDefinitionResourceSpring 使用 #importBeanDefinitionResource(Element ele) 方法，完成对 import 标签的解析。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980// DefaultBeanDefinitionDocumentReader.java/** * Parse an \"import\" element and load the bean definitions * from the given resource into the bean factory. */protected void importBeanDefinitionResource(Element ele) { // &lt;1&gt; 获取 resource 的属性值 String location = ele.getAttribute(RESOURCE_ATTRIBUTE); // 为空，直接退出 if (!StringUtils.hasText(location)) { getReaderContext().error(\"Resource location must not be empty\", ele); // 使用 problemReporter 报错 return; } // &lt;2&gt; 解析系统属性，格式如 ：\"${user.dir}\" // Resolve system properties: e.g. \"${user.dir}\" location = getReaderContext().getEnvironment().resolveRequiredPlaceholders(location); // 实际 Resource 集合，即 import 的地址，有哪些 Resource 资源 Set&lt;Resource&gt; actualResources = new LinkedHashSet&lt;&gt;(4); // &lt;3&gt; 判断 location 是相对路径还是绝对路径 // Discover whether the location is an absolute or relative URI boolean absoluteLocation = false; try { absoluteLocation = ResourcePatternUtils.isUrl(location) || ResourceUtils.toURI(location).isAbsolute(); } catch (URISyntaxException ex) { // cannot convert to an URI, considering the location relative // unless it is the well-known Spring prefix \"classpath*:\" } // Absolute or relative? // &lt;4&gt; 绝对路径 if (absoluteLocation) { try { // 添加配置文件地址的 Resource 到 actualResources 中，并加载相应的 BeanDefinition 们 int importCount = getReaderContext().getReader().loadBeanDefinitions(location, actualResources); if (logger.isTraceEnabled()) { logger.trace(\"Imported \" + importCount + \" bean definitions from URL location [\" + location + \"]\"); } } catch (BeanDefinitionStoreException ex) { getReaderContext().error( \"Failed to import bean definitions from URL location [\" + location + \"]\", ele, ex); } // &lt;5&gt; 相对路径 } else { // No URL -&gt; considering resource location as relative to the current file. try { int importCount; // 创建相对地址的 Resource Resource relativeResource = getReaderContext().getResource().createRelative(location); // 存在 if (relativeResource.exists()) { // 加载 relativeResource 中的 BeanDefinition 们 importCount = getReaderContext().getReader().loadBeanDefinitions(relativeResource); // 添加到 actualResources 中 actualResources.add(relativeResource); // 不存在 } else { // 获得根路径地址 String baseLocation = getReaderContext().getResource().getURL().toString(); // 添加配置文件地址的 Resource 到 actualResources 中，并加载相应的 BeanDefinition 们 importCount = getReaderContext().getReader().loadBeanDefinitions( StringUtils.applyRelativePath(baseLocation, location) /* 计算绝对路径 */, actualResources); } if (logger.isTraceEnabled()) { logger.trace(\"Imported \" + importCount + \" bean definitions from relative location [\" + location + \"]\"); } } catch (IOException ex) { getReaderContext().error(\"Failed to resolve current resource location\", ele, ex); } catch (BeanDefinitionStoreException ex) { getReaderContext().error( \"Failed to import bean definitions from relative location [\" + location + \"]\", ele, ex); } } // &lt;6&gt; 解析成功后，进行监听器激活处理 Resource[] actResArray = actualResources.toArray(new Resource[0]); getReaderContext().fireImportProcessed(location, actResArray, extractSource(ele));} 解析 import 标签过程较为清晰，整个过程如下： &lt;1&gt; 处，获取 source 属性的值，该值表示资源的路径 &lt;2&gt; 处，解析路径中的系统属性，如 “${user.dir}” &lt;3&gt; 处，判断资源路径 location 是绝对路径还是相对路径。 &lt;4&gt; 处，如果是绝对路径，则递归调用 Bean 的解析过程，进行另一次的解析。 &lt;5&gt; 处，若果是相对路径，则先计算出绝对路径得到 Resource，然后进行解析。 &lt;6&gt; 处，通知监听器，完成解析。 判断路径通过以下代码，来判断 location 是为相对路径还是绝对路径 12absoluteLocation = ResourcePatternUtils.isUrl(location) // &lt;1&gt; || ResourceUtils.toURI(location).isAbsolute(); // &lt;2&gt; 判断绝对路径的规则如下 &lt;1&gt; 以 classpath*: 或者 classpath: 开头的为绝对路径 &lt;1&gt; 能够通过该 location 构建出 java.net.URL 为绝对路径 &lt;2&gt; 根据 location 构造 java.net.URI 判断调用 #isAbsolute() 方法，判断是否为绝对路径。 参考原创出处 http://cmsblogs.com/?p=2724 「小明哥」，略作修改及补充","link":"/2020/10/22/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91IoC%E4%B9%8B%E8%A7%A3%E6%9E%90Bean%EF%BC%9A%E8%A7%A3%E6%9E%90import%E6%A0%87%E7%AD%BE/"},{"title":"【Spring源码】IoC之获取验证模型","text":"在上篇【Spring源码】IoC之加载 BeanDefinition中提到，在核心逻辑方法#doLoadBeanDefinitions(InputSource inputSource, Resource resource) 方法中，中主要是做三件事情： 调用 #getValidationModeForResource(Resource resource) 方法，获取指定资源（xml）的验证模式。 调用 DocumentLoader#loadDocument(InputSource inputSource, EntityResolver entityResolver,ErrorHandler errorHandler, int validationMode, boolean namespaceAware) 方法，获取 XML Document 实例。 调用 #registerBeanDefinitions(Document doc, Resource resource) 方法，根据获取的 Document 实例，注册 Bean 信息。 这篇博客主要第 1 步，分析获取 xml 文件的验证模式。 为什么要获取验证模式呢？原因如下： XML 文件的验证模式保证了 XML 文件的正确性。 DTD 与 XSD 的区别DTDDTD (Document Type Definition)，即文档类型定义，为 XML 文件的验证机制，属于 XML 文件中组成的一部分。DTD 是一种保证 XML 文档格式正确的有效验证方式，它定义了相关 XML 文档的元素、属性、排列方式、元素的内容类型以及元素的层次结构。其实 DTD 就相当于 XML 中的 “词汇”和“语法”，我们可以通过比较 XML 文件和 DTD 文件 来看文档是否符合规范，元素和标签使用是否正确。 要在 Spring 中使用 DTD，需要在 Spring XML 文件头部声明： 12&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE beans PUBLIC &quot;-//SPRING//DTD BEAN//EN&quot; &quot;http://www.springframework.org/dtd/spring-beans.dtd&quot;&gt; DTD 在一定的阶段推动了 XML 的发展，但是它本身存在着一些缺陷： 它没有使用 XML 格式，而是自己定义了一套格式，相对解析器的重用性较差；而且 DTD 的构建和访问没有标准的编程接口，因而解析器很难简单的解析 DTD 文档。 DTD 对元素的类型限制较少；同时其他的约束力也叫弱。 DTD 扩展能力较差。 基于正则表达式的 DTD 文档的描述能力有限。 XSD针对 DTD 的缺陷，W3C 在 2001 年推出 XSD。XSD（XML Schemas Definition）即 XML Schema 语言。XML Schema 本身就是一个 XML文档，使用的是 XML 语法，因此可以很方便的解析 XSD 文档。相对于 DTD，XSD 具有如下优势： XML Schema 基于 XML ，没有专门的语法。 XML Schema 可以象其他 XML 文件一样解析和处理。 XML Schema 比 DTD 提供了更丰富的数据类型。 XML Schema 提供可扩充的数据模型。 XML Schema 支持综合命名空间。 XML Schema 支持属性组。 getValidationModeForResource12345678910111213141516171819202122232425262728293031323334// XmlBeanDefinitionReader.java// 禁用验证模式public static final int VALIDATION_NONE = XmlValidationModeDetector.VALIDATION_NONE;// 自动获取验证模式public static final int VALIDATION_AUTO = XmlValidationModeDetector.VALIDATION_AUTO;// DTD 验证模式public static final int VALIDATION_DTD = XmlValidationModeDetector.VALIDATION_DTD;// XSD 验证模式public static final int VALIDATION_XSD = XmlValidationModeDetector.VALIDATION_XSD; /** * 验证模式。默认为自动模式。 */private int validationMode = VALIDATION_AUTO; protected int getValidationModeForResource(Resource resource) { // &lt;1&gt; 获取指定的验证模式 int validationModeToUse = getValidationMode(); // 首先，如果手动指定，则直接返回 if (validationModeToUse != VALIDATION_AUTO) { return validationModeToUse; } // &lt;2&gt; 其次，自动获取验证模式 int detectedMode = detectValidationMode(resource); if (detectedMode != VALIDATION_AUTO) { return detectedMode; } // &lt;3&gt; 最后，使用 VALIDATION_XSD 做为默认 // Hmm, we didn't get a clear indication... Let's assume XSD, // since apparently no DTD declaration has been found up until // detection stopped (before finding the document's root tag). return VALIDATION_XSD;} &lt;1&gt; 处，调用 #getValidationMode() 方法，获取指定的验证模式( validationMode )。如果有手动指定，则直接返回。另外，对对于 validationMode 属性的设置和获得的代码，代码如下： 1234567public void setValidationMode(int validationMode) { this.validationMode = validationMode;}public int getValidationMode() { return this.validationMode;} &lt;2&gt; 处，调用 #detectValidationMode(Resource resource) 方法，自动获取验证模式。代码如下： 12345678910111213141516171819202122232425262728293031323334/** * XML 验证模式探测器 */private final XmlValidationModeDetector validationModeDetector = new XmlValidationModeDetector(); protected int detectValidationMode(Resource resource) {// 不可读，抛出 BeanDefinitionStoreException 异常 if (resource.isOpen()) { throw new BeanDefinitionStoreException( \"Passed-in Resource [\" + resource + \"] contains an open stream: \" + \"cannot determine validation mode automatically. Either pass in a Resource \" + \"that is able to create fresh streams, or explicitly specify the validationMode \" + \"on your XmlBeanDefinitionReader instance.\"); } // 打开 InputStream 流 InputStream inputStream; try { inputStream = resource.getInputStream(); } catch (IOException ex) { throw new BeanDefinitionStoreException( \"Unable to determine validation mode for [\" + resource + \"]: cannot open InputStream. \" + \"Did you attempt to load directly from a SAX InputSource without specifying the \" + \"validationMode on your XmlBeanDefinitionReader instance?\", ex); } // &lt;x&gt; 获取相应的验证模式 try { return this.validationModeDetector.detectValidationMode(inputStream); } catch (IOException ex) { throw new BeanDefinitionStoreException(\"Unable to determine validation mode for [\" + resource + \"]: an error occurred whilst reading from the InputStream.\", ex); }} 核心在于 &lt;x&gt; 处，调用 XmlValidationModeDetector#detectValidationMode(InputStream inputStream) 方法，获取相应的验证模式。详细解析，见下一节。 &lt;3&gt; 处，使用 VALIDATION_XSD 做为默认。 XmlValidationModeDetectororg.springframework.util.xml.XmlValidationModeDetector ，XML 验证模式探测器。 12345678910111213141516171819202122232425262728293031323334353637public int detectValidationMode(InputStream inputStream) throws IOException { // Peek into the file to look for DOCTYPE. BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream)); try { // 是否为 DTD 校验模式。默认为，非 DTD 模式，即 XSD 模式 boolean isDtdValidated = false; String content; // &lt;0&gt; 循环，逐行读取 XML 文件的内容 while ((content = reader.readLine()) != null) { content = consumeCommentTokens(content); // 跳过，如果是注释，或者 if (this.inComment || !StringUtils.hasText(content)) { continue; } // &lt;1&gt; 包含 DOCTYPE 为 DTD 模式 if (hasDoctype(content)) { isDtdValidated = true; break; } // &lt;2&gt; hasOpeningTag 方法会校验，如果这一行有 &lt; ，并且 &lt; 后面跟着的是字母，则返回 true 。 if (hasOpeningTag(content)) { // End of meaningful data... break; } } // 返回 VALIDATION_DTD or VALIDATION_XSD 模式 return (isDtdValidated ? VALIDATION_DTD : VALIDATION_XSD); } catch (CharConversionException ex) { // &lt;3&gt; 返回 VALIDATION_AUTO 模式 // Choked on some character encoding... // Leave the decision up to the caller. return VALIDATION_AUTO; } finally { reader.close(); }} &lt;0&gt; 处，从代码中看，主要是通过读取 XML 文件的内容，来进行自动判断。 &lt;1&gt; 处，调用 #hasDoctype(String content) 方法，判断内容中如果包含有 &quot;DOCTYPE“ ，则为 DTD 验证模式。代码如下： 123456789/** * The token in a XML document that declares the DTD to use for validation * and thus that DTD validation is being used. */private static final String DOCTYPE = \"DOCTYPE\";private boolean hasDoctype(String content) { return content.contains(DOCTYPE);} &lt;2&gt; 处，调用 #hasOpeningTag(String content) 方法，判断如果这一行包含 &lt; ，并且 &lt; 紧跟着的是字幕，则为 XSD 验证模式。代码如下： 1234567891011121314/** * Does the supplied content contain an XML opening tag. If the parse state is currently * in an XML comment then this method always returns false. It is expected that all comment * tokens will have consumed for the supplied content before passing the remainder to this method. */private boolean hasOpeningTag(String content) { if (this.inComment) { return false; } int openTagIndex = content.indexOf('&lt;'); return (openTagIndex &gt; -1 // &lt; 存在 &amp;&amp; (content.length() &gt; openTagIndex + 1) // &lt; 后面还有内容 &amp;&amp; Character.isLetter(content.charAt(openTagIndex + 1))); // &lt; 后面的内容是字幕} &lt;3&gt; 处，如果发生 CharConversionException 异常，则为 VALIDATION_AUTO 模式。 关于 #consumeCommentTokens(String content) 方法，代码比较复杂。感兴趣的胖友可以看看。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * The token that indicates the start of an XML comment. */private static final String START_COMMENT = \"&lt;!--\";/** * The token that indicates the end of an XML comment. */private static final String END_COMMENT = \"--&gt;\";/** * Consumes all the leading comment data in the given String and returns the remaining content, which * may be empty since the supplied content might be all comment data. For our purposes it is only important * to strip leading comment content on a line since the first piece of non comment content will be either * the DOCTYPE declaration or the root element of the document. */@Nullableprivate String consumeCommentTokens(String line) { // 非注释 if (!line.contains(START_COMMENT) &amp;&amp; !line.contains(END_COMMENT)) { return line; } String currLine = line; while ((currLine = consume(currLine)) != null) { if (!this.inComment &amp;&amp; !currLine.trim().startsWith(START_COMMENT)) { return currLine; } } return null;}/** * Consume the next comment token, update the \"inComment\" flag * and return the remaining content. */@Nullableprivate String consume(String line) { int index = (this.inComment ? endComment(line) : startComment(line)); return (index == -1 ? null : line.substring(index));}/** * Try to consume the {@link #START_COMMENT} token. * @see #commentToken(String, String, boolean) */private int startComment(String line) { return commentToken(line, START_COMMENT, true);}private int endComment(String line) { return commentToken(line, END_COMMENT, false);}/** * Try to consume the supplied token against the supplied content and update the * in comment parse state to the supplied value. Returns the index into the content * which is after the token or -1 if the token is not found. */private int commentToken(String line, String token, boolean inCommentIfPresent) { int index = line.indexOf(token); if (index &gt; - 1) { this.inComment = inCommentIfPresent; } return (index == -1 ? index : index + token.length());} 笔者没细看。。想细看的胖友可以看下面两篇文章，有一定的辅助作用 《spring源码（六）–XmlValidationModeDetector（获取xml文档校验模式）》 《XmlValidationModeDetector》 参考原创出处 http://cmsblogs.com/?p=2688 「小明哥」，略作修改及补充 https://blog.csdn.net/ljz2016/article/details/82686884 https://my.oschina.net/u/3579120/blog/1532852","link":"/2020/10/07/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91IoC%E4%B9%8B%E8%8E%B7%E5%8F%96%E9%AA%8C%E8%AF%81%E6%A8%A1%E5%9E%8B/"},{"title":"《Head First设计模式》脑图笔记","text":"读完了《Head First设计模式》，做了个脑图笔记总结输出，使用的是Marginnote 3，由于整本书的脑图太长，一章一章截下来放到笔记里的。 对标记颜色做了分类，红色为标题，黄色为重点，绿色为次重点，蓝色为补充 策略模式 观察者模式 装饰者模式 工厂方法模式 抽象工厂模式 单件模式 命令模式 适配器模式 外观模式 模板方法模式 迭代器模式 组合模式 状态模式 代理模式 复合模式","link":"/2020/07/22/%E3%80%8AHead-First%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E3%80%8B%E8%84%91%E5%9B%BE%E7%AC%94%E8%AE%B0/"},{"title":"【Spring源码】IoC之获取Document对象","text":"在【Spring源码】IoC之加载 BeanDefinition中提到，在核心逻辑方法#doLoadBeanDefinitions(InputSource inputSource, Resource resource) 方法中，中主要是做三件事情： 调用 #getValidationModeForResource(Resource resource) 方法，获取指定资源（xml）的验证模式。 调用 DocumentLoader#loadDocument(InputSource inputSource, EntityResolver entityResolver,ErrorHandler errorHandler, int validationMode, boolean namespaceAware) 方法，获取 XML Document 实例。 调用 #registerBeanDefinitions(Document doc, Resource resource) 方法，根据获取的 Document 实例，注册 Bean 信息。 这篇博客主要第 2 步，分析获取 XML Document 对象。 DocumentLoader获取 Document 的策略，由接口org.springframework.beans.factory.xml.DocumentLoader 定义。代码如下： 123456public interface DocumentLoader { Document loadDocument( InputSource inputSource, EntityResolver entityResolver, ErrorHandler errorHandler, int validationMode, boolean namespaceAware) throws Exception;} inputSource 方法参数，加载 Document 的 Resource 资源。 entityResolver 方法参数，解析文件的解析器。 errorHandler 方法参数，处理加载 Document 对象的过程的错误。 validationMode 方法参数，验证模式。 namespaceAware 方法参数，命名空间支持。如果要提供对 XML 名称空间的支持，则需要值为 true 。 FROM 《Spring 源码深度解析》P16 页 定义从资源文件加载到转换为 Document 的功能。 DefaultDocumentLoader该方法由 DocumentLoader 的默认实现类 org.springframework.beans.factory.xml.DefaultDocumentLoader 实现。代码如下： 1234567891011121314151617/** * Load the {@link Document} at the supplied {@link InputSource} using the standard JAXP-configured * XML parser. */@Overridepublic Document loadDocument(InputSource inputSource, EntityResolver entityResolver, ErrorHandler errorHandler, int validationMode, boolean namespaceAware) throws Exception { // &lt;1&gt; 创建 DocumentBuilderFactory DocumentBuilderFactory factory = createDocumentBuilderFactory(validationMode, namespaceAware); if (logger.isTraceEnabled()) { logger.trace(\"Using JAXP provider [\" + factory.getClass().getName() + \"]\"); } // &lt;2&gt; 创建 DocumentBuilder DocumentBuilder builder = createDocumentBuilder(factory, entityResolver, errorHandler); // &lt;3&gt; 解析 XML InputSource 返回 Document 对象 return builder.parse(inputSource);} 首先，调用 #createDocumentBuilderFactory(...) 方法，创建 javax.xml.parsers.DocumentBuilderFactory 对象。代码如下： 12345678910111213141516171819202122232425262728293031323334/** * JAXP attribute used to configure the schema language for validation. */private static final String SCHEMA_LANGUAGE_ATTRIBUTE = \"http://java.sun.com/xml/jaxp/properties/schemaLanguage\";/** * JAXP attribute value indicating the XSD schema language. */private static final String XSD_SCHEMA_LANGUAGE = \"http://www.w3.org/2001/XMLSchema\";protected DocumentBuilderFactory createDocumentBuilderFactory(int validationMode, boolean namespaceAware) throws ParserConfigurationException { // 创建 DocumentBuilderFactory DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance(); factory.setNamespaceAware(namespaceAware); // 设置命名空间支持 if (validationMode != XmlValidationModeDetector.VALIDATION_NONE) { factory.setValidating(true); // 开启校验 // XSD 模式下，设置 factory 的属性 if (validationMode == XmlValidationModeDetector.VALIDATION_XSD) { // Enforce namespace aware for XSD... factory.setNamespaceAware(true); // XSD 模式下，强制设置命名空间支持 // 设置 SCHEMA_LANGUAGE_ATTRIBUTE try { factory.setAttribute(SCHEMA_LANGUAGE_ATTRIBUTE, XSD_SCHEMA_LANGUAGE); } catch (IllegalArgumentException ex) { ParserConfigurationException pcex = new ParserConfigurationException( \"Unable to validate using XSD: Your JAXP provider [\" + factory + \"] does not support XML Schema. Are you running on Java 1.4 with Apache Crimson? \" + \"Upgrade to Apache Xerces (or Java 1.5) for full XSD support.\"); pcex.initCause(ex); throw pcex; } } } return factory;} 然后，调用#createDocumentBuilder(DocumentBuilderFactory factory, EntityResolver entityResolver,ErrorHandler errorHandler) 方法，创建 javax.xml.parsers.DocumentBuilder 对象。代码如下： 123456789101112131415protected DocumentBuilder createDocumentBuilder(DocumentBuilderFactory factory, @Nullable EntityResolver entityResolver, @Nullable ErrorHandler errorHandler) throws ParserConfigurationException { // 创建 DocumentBuilder 对象 DocumentBuilder docBuilder = factory.newDocumentBuilder(); // &lt;x&gt; 设置 EntityResolver 属性 if (entityResolver != null) { docBuilder.setEntityResolver(entityResolver); } // 设置 ErrorHandler 属性 if (errorHandler != null) { docBuilder.setErrorHandler(errorHandler); } return docBuilder;} 在 &lt;x&gt; 处，设置 DocumentBuilder 的 EntityResolver 属性。关于它，在下一节会详细解析。 最后，调用 DocumentBuilder#parse(InputSource) 方法，解析 InputSource ，返回 Document 对象。 EntityResolver通过 DocumentLoader#loadDocument(...) 方法来获取 Document 对象时，有一个方法参数 entityResolver 。该参数是通过 XmlBeanDefinitionReader#getEntityResolver() 方法来获取的。代码如下： #getEntityResolver() 方法，返回指定的解析器，如果没有指定，则构造一个未指定的默认解析器。 1234567891011121314151617181920// XmlBeanDefinitionReader.java/** * EntityResolver 解析器 */@Nullableprivate EntityResolver entityResolver;protected EntityResolver getEntityResolver() { if (this.entityResolver == null) { // Determine default EntityResolver to use. ResourceLoader resourceLoader = getResourceLoader(); if (resourceLoader != null) { this.entityResolver = new ResourceEntityResolver(resourceLoader); } else { this.entityResolver = new DelegatingEntityResolver(getBeanClassLoader()); } } return this.entityResolver;} 如果 ResourceLoader 不为 null，则根据指定的 ResourceLoader 创建一个 ResourceEntityResolver 对象。 如果 ResourceLoader 为 null ，则创建 一个 DelegatingEntityResolver 对象。该 Resolver 委托给默认的 BeansDtdResolver 和 PluggableSchemaResolver 。 子类上面的方法，一共涉及四个 EntityResolver 的子类： org.springframework.beans.factory.xm.BeansDtdResolver ：实现 EntityResolver 接口，Spring Bean dtd 解码器，用来从 classpath 或者 jar 文件中加载 dtd 。部分代码如下： 123private static final String DTD_EXTENSION = \".dtd\";private static final String DTD_NAME = \"spring-beans\"; org.springframework.beans.factory.xml.PluggableSchemaResolver ，实现 EntityResolver 接口，读取 classpath 下的所有 &quot;META-INF/spring.schemas&quot; 成一个 namespaceURI 与 Schema 文件地址的 map 。代码如下： 12345678910111213141516171819/** * The location of the file that defines schema mappings. * Can be present in multiple JAR files. * * 默认 {@link #schemaMappingsLocation} 地址 */public static final String DEFAULT_SCHEMA_MAPPINGS_LOCATION = \"META-INF/spring.schemas\";@Nullableprivate final ClassLoader classLoader;/** * Schema 文件地址 */private final String schemaMappingsLocation;/** Stores the mapping of schema URL -&gt; local schema path. */@Nullableprivate volatile Map&lt;String, String&gt; schemaMappings; // namespaceURI 与 Schema 文件地址的映射集合 org.springframework.beans.factory.xml.DelegatingEntityResolver ：实现 EntityResolver 接口，分别代理 dtd 的 BeansDtdResolver 和 xml schemas 的 PluggableSchemaResolver 。代码如下： 1234567891011121314151617181920212223/** Suffix for DTD files. */public static final String DTD_SUFFIX = \".dtd\";/** Suffix for schema definition files. */public static final String XSD_SUFFIX = \".xsd\";private final EntityResolver dtdResolver;private final EntityResolver schemaResolver;// 默认public DelegatingEntityResolver(@Nullable ClassLoader classLoader) { this.dtdResolver = new BeansDtdResolver(); this.schemaResolver = new PluggableSchemaResolver(classLoader);}// 自定义public DelegatingEntityResolver(EntityResolver dtdResolver, EntityResolver schemaResolver) { Assert.notNull(dtdResolver, \"'dtdResolver' is required\"); Assert.notNull(schemaResolver, \"'schemaResolver' is required\"); this.dtdResolver = dtdResolver; this.schemaResolver = schemaResolver;} org.springframework.beans.factory.xml.ResourceEntityResolver ：继承自 DelegatingEntityResolver 类，通过 ResourceLoader 来解析实体的引用。代码如下： 123456private final ResourceLoader resourceLoader;public ResourceEntityResolver(ResourceLoader resourceLoader) { super(resourceLoader.getClassLoader()); this.resourceLoader = resourceLoader;} 作用EntityResolver 的作用就是，通过实现它，应用可以自定义如何寻找【验证文件】的逻辑。 FROM 《Spring 源码深度解析》 在 loadDocument 方法中涉及一个参数 EntityResolver ，何为EntityResolver？官网这样解释：如果 SAX 应用程序需要实现自定义处理外部实体，则必须实现此接口并使用 setEntityResolver 方法向SAX 驱动器注册一个实例。也就是说，对于解析一个XML，SAX 首先读取该 XML 文档上的声明，根据声明去寻找相应的 DTD 定义，以便对文档进行一个验证。默认的寻找规则，即通过网络（实现上就是声明的DTD的URI地址）来下载相应的DTD声明，并进行认证。下载的过程是一个漫长的过程，而且当网络中断或不可用时，这里会报错，就是因为相应的DTD声明没有被找到的原因。 EntityResolver 的作用是项目本身就可以提供一个如何寻找 DTD 声明的方法，即由程序来实现寻找 DTD 声明的过程，比如我们将 DTD 文件放到项目中某处，在实现时直接将此文档读取并返回给 SAX 即可。这样就避免了通过网络来寻找相应的声明。 org.xml.sax.EntityResolver 接口，代码如下： 1234public interface EntityResolver { public abstract InputSource resolveEntity (String publicId, String systemId) throws SAXException, IOException;} 接口方法接收两个参数 publicId 和 systemId ，并返回 InputSource 对象。两个参数声明如下： publicId ：被引用的外部实体的公共标识符，如果没有提供，则返回 null 。 systemId ：被引用的外部实体的系统标识符。 这两个参数的实际内容和具体的验证模式的关系如下： XSD 验证模式 publicId：null systemId：http://www.springframework.org/schema/beans/spring-beans.xsd DTD 验证模式 publicId：-//SPRING//DTD BEAN 2.0//EN systemId：http://www.springframework.org/dtd/spring-beans.dtd DelegatingEntityResolver们知道在 Spring 中使用 DelegatingEntityResolver 为 EntityResolver 的实现类。#resolveEntity(String publicId, String systemId) 方法，实现如下： 1234567891011121314@Override@Nullablepublic InputSource resolveEntity(String publicId, @Nullable String systemId) throws SAXException, IOException { if (systemId != null) { // DTD 模式 if (systemId.endsWith(DTD_SUFFIX)) { return this.dtdResolver.resolveEntity(publicId, systemId); // XSD 模式 } else if (systemId.endsWith(XSD_SUFFIX)) { return this.schemaResolver.resolveEntity(publicId, systemId); } } return null;} 如果是 DTD 验证模式，则使用 BeansDtdResolver 来进行解析 如果是 XSD 验证模式，则使用 PluggableSchemaResolver 来进行解析。 BeansDtdResolverBeansDtdResolver 的解析过程，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * DTD 文件的后缀 */private static final String DTD_EXTENSION = \".dtd\";/** * Spring Bean DTD 的文件名 */private static final String DTD_NAME = \"spring-beans\";@Override@Nullablepublic InputSource resolveEntity(String publicId, @Nullable String systemId) throws IOException { if (logger.isTraceEnabled()) { logger.trace(\"Trying to resolve XML entity with public ID [\" + publicId + \"] and system ID [\" + systemId + \"]\"); } // 必须以 .dtd 结尾 if (systemId != null &amp;&amp; systemId.endsWith(DTD_EXTENSION)) { // 获取最后一个 / 的位置 int lastPathSeparator = systemId.lastIndexOf('/'); // 获取 spring-beans 的位置 int dtdNameStart = systemId.indexOf(DTD_NAME, lastPathSeparator); if (dtdNameStart != -1) { // 找到 String dtdFile = DTD_NAME + DTD_EXTENSION; if (logger.isTraceEnabled()) { logger.trace(\"Trying to locate [\" + dtdFile + \"] in Spring jar on classpath\"); } try { // 创建 ClassPathResource 对象 Resource resource = new ClassPathResource(dtdFile, getClass()); // 创建 InputSource 对象，并设置 publicId、systemId 属性 InputSource source = new InputSource(resource.getInputStream()); source.setPublicId(publicId); source.setSystemId(systemId); if (logger.isTraceEnabled()) { logger.trace(\"Found beans DTD [\" + systemId + \"] in classpath: \" + dtdFile); } return source; } catch (IOException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Could not resolve beans DTD [\" + systemId + \"]: not found in classpath\", ex); } } } } // 使用默认行为，从网络上下载 // Use the default behavior -&gt; download from website or wherever. return null;} 从上面的代码中，我们可以看到，加载 DTD 类型的 BeansDtdResolver#resolveEntity(...) 过程，只是对 systemId 进行了简单的校验（从最后一个 / 开始，内容中是否包含 spring-beans），然后构造一个 InputSource 对象，并设置 publicId、systemId 属性，然后返回。 PluggableSchemaResolverPluggableSchemaResolver 的解析过程，代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445@Nullableprivate final ClassLoader classLoader;/** * Schema 文件地址 */private final String schemaMappingsLocation;/** Stores the mapping of schema URL -&gt; local schema path. */@Nullableprivate volatile Map&lt;String, String&gt; schemaMappings; // namespaceURI 与 Schema 文件地址的映射集合@Override@Nullablepublic InputSource resolveEntity(String publicId, @Nullable String systemId) throws IOException { if (logger.isTraceEnabled()) { logger.trace(\"Trying to resolve XML entity with public id [\" + publicId + \"] and system id [\" + systemId + \"]\"); } if (systemId != null) { // 获得 Resource 所在位置 String resourceLocation = getSchemaMappings().get(systemId); if (resourceLocation != null) { // 创建 ClassPathResource Resource resource = new ClassPathResource(resourceLocation, this.classLoader); try { // 创建 InputSource 对象，并设置 publicId、systemId 属性 InputSource source = new InputSource(resource.getInputStream()); source.setPublicId(publicId); source.setSystemId(systemId); if (logger.isTraceEnabled()) { logger.trace(\"Found XML schema [\" + systemId + \"] in classpath: \" + resourceLocation); } return source; } catch (FileNotFoundException ex) { if (logger.isDebugEnabled()) { logger.debug(\"Could not find XML schema [\" + systemId + \"]: \" + resource, ex); } } } } return null;} 首先调用 #getSchemaMappings() 方法，获取一个映射表(systemId 与其在本地的对照关系)。代码如下： 1234567891011121314151617181920212223242526272829private Map&lt;String, String&gt; getSchemaMappings() { Map&lt;String, String&gt; schemaMappings = this.schemaMappings; // 双重检查锁，实现 schemaMappings 单例 if (schemaMappings == null) { synchronized (this) { schemaMappings = this.schemaMappings; if (schemaMappings == null) { if (logger.isTraceEnabled()) { logger.trace(\"Loading schema mappings from [\" + this.schemaMappingsLocation + \"]\"); } try { // 以 Properties 的方式，读取 schemaMappingsLocation Properties mappings = PropertiesLoaderUtils.loadAllProperties(this.schemaMappingsLocation, this.classLoader); if (logger.isTraceEnabled()) { logger.trace(\"Loaded schema mappings: \" + mappings); } // 将 mappings 初始化到 schemaMappings 中 schemaMappings = new ConcurrentHashMap&lt;&gt;(mappings.size()); CollectionUtils.mergePropertiesIntoMap(mappings, schemaMappings); this.schemaMappings = schemaMappings; } catch (IOException ex) { throw new IllegalStateException( \"Unable to load schema mappings from location [\" + this.schemaMappingsLocation + \"]\", ex); } } } } return schemaMappings;} 映射表如下（部分）: 然后，根据传入的 systemId 获取该 systemId 在本地的路径 resourceLocation 。 最后，根据 resourceLocation ，构造 InputSource 对象。 ResourceEntityResolverResourceEntityResolver 的解析过程，代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private final ResourceLoader resourceLoader;@Override@Nullablepublic InputSource resolveEntity(String publicId, @Nullable String systemId) throws SAXException, IOException { // 调用父类的方法，进行解析 InputSource source = super.resolveEntity(publicId, systemId); // 解析失败，resourceLoader 进行解析 if (source == null &amp;&amp; systemId != null) { // 获得 resourcePath ，即 Resource 资源地址 String resourcePath = null; try { String decodedSystemId = URLDecoder.decode(systemId, \"UTF-8\"); // 使用 UTF-8 ，解码 systemId String givenUrl = new URL(decodedSystemId).toString(); // 转换成 URL 字符串 // 解析文件资源的相对路径（相对于系统根路径） String systemRootUrl = new File(\"\").toURI().toURL().toString(); // Try relative to resource base if currently in system root. if (givenUrl.startsWith(systemRootUrl)) { resourcePath = givenUrl.substring(systemRootUrl.length()); } } catch (Exception ex) { // Typically a MalformedURLException or AccessControlException. if (logger.isDebugEnabled()) { logger.debug(\"Could not resolve XML entity [\" + systemId + \"] against system root URL\", ex); } // No URL (or no resolvable URL) -&gt; try relative to resource base. resourcePath = systemId; } if (resourcePath != null) { if (logger.isTraceEnabled()) { logger.trace(\"Trying to locate XML entity [\" + systemId + \"] as resource [\" + resourcePath + \"]\"); } // 获得 Resource 资源 Resource resource = this.resourceLoader.getResource(resourcePath); // 创建 InputSource 对象 source = new InputSource(resource.getInputStream()); // 设置 publicId 和 systemId 属性 source.setPublicId(publicId); source.setSystemId(systemId); if (logger.isDebugEnabled()) { logger.debug(\"Found XML entity [\" + systemId + \"]: \" + resource); } } } return source;} 首先，调用父类的方法，进行解析。 如果失败，使用 resourceLoader ，尝试读取 systemId 对应的 Resource 资源。 自定义 EntityResolver#getEntityResolver() 方法返回 EntityResolver 对象。那么怎么进行自定义 EntityResolver 呢? If a SAX application needs to implement customized handling for external entities, it must implement this interface and register an instance with the SAX driver using the setEntityResolver method. 就是说：如果 SAX 应用程序需要实现自定义处理外部实体，则必须实现此接口，并使用 #setEntityResolver(EntityResolver entityResolver) 方法，向 SAX 驱动器注册一个 EntityResolver 实例。 示例如下： 123456789101112public class MyResolver implements EntityResolver { @Override public InputSource resolveEntity(String publicId, String systemId) { if (systemId.equals(\"http://www.myhost.com/today\")) { MyReader reader = new MyReader(); return new InputSource(reader); } else { // use the default behaviour return null; } }} 我们首先将 &quot;spring-student.xml&quot; 文件中的 XSD 声明的地址改掉，如下： 如果我们再次运行，则会报如下错误： 从上面的错误可以看到，是在进行文档验证时，无法根据声明找到 XSD 验证文件而导致无法进行 XML 文件验证。在 《【Spring源码】IoC之获取验证模型》 中讲到，如果要解析一个 XML 文件，SAX 首先会读取该 XML 文档上的声明，然后根据声明去寻找相应的 DTD 定义，以便对文档进行验证。默认的加载规则是通过网络方式下载验证文件，而在实际生产环境中我们会遇到网络中断或者不可用状态，那么就应用就会因为无法下载验证文件而报错。 总结是不是看到此处，有点懵逼，不是说好了分享获取 Document 对象，结果内容主要是 EntityResolver 呢？因为，从 XML 中获取 Document 对象，已经有 javax.xml 库进行解析。而 EntityResolver 的重点，是在于如何获取【验证文件】，从而验证用户写的 XML 是否通过验证。 参考原创出处 http://cmsblogs.com/?p=2695 「小明哥」，略作修改及补充","link":"/2020/10/07/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91IoC%E4%B9%8B%E8%8E%B7%E5%8F%96Document%E5%AF%B9%E8%B1%A1/"},{"title":"在云服务器上安装配置Kafka遇到的问题及解决方案","text":"在学习Kafka，准备在服务器上搭一个使用Kafka的项目。对服务器的使用不是很熟练，在安装使用Kafka的时候遇到了一些问题，记录下来。 服务器情况腾讯云，标准型S2，1核，1GB，1Mbps，操作系统 CentOS 7.6 64位，坐标香港 其实我之前买过阿里云的服务器，但是当时选的服务器位置在境内，由于工信部要求如果要在境内服务器上部署网站需要备案，现在我在香港，就各种不方便。因此另外买了一台香港的服务器，刚好腾讯十周年活动，腾讯云的香港服务器比其他几家大厂都便宜得多，于是选了腾讯云的。但是毕竟是境外的服务器，比境内的还是要贵，所以选的是1核1G的，想了想穷学生学习应该也够，结果就遇上问题了orz Kafka运行内存不足问题在安装好Kafka准备启动的时候，报如下错误 12345678[root@VM-0-6-centos kafka_2.11-2.4.0]# bin/kafka-server-start.sh config/server.properties &amp;[1] 5158[root@VM-0-6-centos kafka_2.11-2.4.0]# Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) failed; error='Cannot allocate memory' (errno=12)## There is insufficient memory for the Java Runtime Environment to continue.# Native memory allocation (mmap) failed to map 1073741824 bytes for committing reserved memory.# An error report file with more information is saved as:# /opt/install/kafka_2.11-2.4.0/hs_err_pid5158.log 看报错情况应该是内存不足，果然1G还是有点小，但没办法只能修改配置了。 修改启动脚本，Kafka安装目录下：/bin/kafka-server-start.sh 修改 export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot; 为 export KAFKA_HEAP_OPTS=&quot;-Xmx256M -Xms128M&quot; （也可以修改为其它合适的大小） ip地址绑定错误问题启动Kafka的时候报如下错误 kafka.common.KafkaException: Socket server failed to bind to xx:9092: Cannot assign requested address 解决方法参考：https://qii404.me/2018/02/02/kafka-cluster.html 该情况下的虚拟机对外ip（暴露的ip，也就是服务器提供的公网ip）和真实ip（ifconfig显示的ip，也就是服务器提供的内网ip）可能只是映射关系，用户访问对外ip时，OpenStack会转发到对应的真实ip实现访问。但此时如果 server.properties 123listeners=PLAINTEXT://xxx.xxx.xxx.xxx:9092 advertised.listeners=PLAINTEXT://xxx.xxx.xxx.xxx:9092 中的ip配置为（对外ip）的时候无法启动，socket无法绑定监听 解决方法也很简单，将ip改为真实ip（ifconfig中显示的ip，也就是内网ip）即可，其他使用时正常使用对外ip即可，跟真实ip就没有关系了。 上述改完Kafka就能成功启动了，但会出现下一个问题。 Kafka连接服务器出现”Connection to node 1 (localhost/127.0.0.1:9092) could not be established”在Kafka成功启动后，启动项目项目，会报这个错误。 在本地测试的时候安装运行好Kafka后就能成功运行项目了，但是放到服务器上就报错了。从错误信息可以看到，SpringBoot开启后连接的是127.0.0.1，也就是本地的Kafka broker，但是配置文件中配置的是服务器的IP地址。 先来一点点排查错误。 安全组是否开放&amp;防火墙是否拦截请求一般数据库或Redis连接不可用，都有可能是安全组没有开放或者防火墙拦截了外来连接导致。登陆腾讯云服务器管理台后发现，9092端口和2181端口都是开放的，说明腾讯云这边并没有关闭了Kafka和Zookeeper的外部连接权限，那么尝试下端口扫描，发现这两个端口依然可以被外部访问。 登陆服务器查看Kafka broker是否可用12345[root@VM-0-6-centos wechat_template]# lsof -i:9092COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 26077 root 123u IPv6 501530 0t0 TCP VM-0-6-centos:XmlIpcRegSvc (LISTEN)java 26077 root 139u IPv6 501545 0t0 TCP VM-0-6-centos:43702-&gt;VM-0-6-centos:XmlIpcRegSvc (ESTABLISHED)java 26077 root 140u IPv6 501546 0t0 TCP VM-0-6-centos:XmlIpcRegSvc-&gt;VM-0-6-centos:43702 (ESTABLISHED) 通过查询Kafka broker默认端口9092发现，服务运行状态良好，排除掉服务下线的可能性。 查看Kafka运行日志1234567891011[root@VM-0-6-centos kafka_2.11-2.4.0]# tail -f logs/server.log[2020-08-26 14:47:04,804] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)[2020-08-26 14:47:04,876] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)[2020-08-26 14:47:04,930] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)[2020-08-26 14:47:04,998] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)[2020-08-26 14:47:05,046] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)[2020-08-26 14:47:05,093] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)[2020-08-26 14:47:05,095] INFO Kafka version: 2.4.0 (org.apache.kafka.common.utils.AppInfoParser)[2020-08-26 14:47:05,095] INFO Kafka commitId: 77a89fcf8d7fa018 (org.apache.kafka.common.utils.AppInfoParser)[2020-08-26 14:47:05,095] INFO Kafka startTimeMs: 1598424425093 (org.apache.kafka.common.utils.AppInfoParser)[2020-08-26 14:47:05,096] INFO [KafkaServer id=0] started (kafka.server.KafkaServer) 运行状态良好，没有出现WARN和ERROR信息，从日志看不出问题。 网上找资料没能力找到问题所在，那么问问google，总有一些解决问题的方式，虽然并不一定能直接解决问题，但是在查找问题的过程中还是能学到很多，例如这篇博客也提到了Kafka外网连接问题，但按照这个方式修改后并不能解决我的问题： kafka连接问题 这里面提到了一个很重要的概念： kafka启动后会在zookeeper的/brokers/ids下注册监听协议，包括IP和端口号，客户端连接的时候，会取得这个IP和端口号。后来查看了kafka的配置，原来我忽视了listeners和advertised.listeners的区别，advertised.listeners才是真正暴露给外部使用的连接地址，会写入到zookeeper节点中的。于是再次进行修改，把IP配置到advertised.listeners中，问题再一次解决。 advertised.listeners才是真正的对外代理地址！那么listeners的作用就不是对外提供服务代理，而是监听！ 解决问题修改server.properties的两行默认配置： 1234# 允许外部端口连接 listeners=PLAINTEXT://0.0.0.0:9092 # 外部代理地址（如已修改为真实ip，也就是服务器的内网ip，则不变） advertised.listeners=PLAINTEXT://xxx.xxx.xxx.xxx:9092","link":"/2020/08/26/%E5%9C%A8%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEKafka%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"title":"安装Homebrew遇到connection refused问题解决及Proxifier配置","text":"问题描述刚刚从window转为macOS，很多东西需要重新配置以及熟悉。听说Homebrew是个强大的包管理器，所以用来试试看。在Homebrew官网主页复制了安装命令在终端运行时遇到了connection refused问题 问题解决由于我在境内，需要使用VPN连接，因此我使用的是V2rayU这款开源软件（github地址：https://github.com/yanue/V2rayU），查阅了用户手册后，在代理模式部分看到 全局模式: 有别于vpn,只是将代理信息更新到系统代理http,https,socks,若需要真正全局模式, 推荐搭配使用Proxifier 因此需要Proxifier搭配才能进行全局代理。 添加代理信息安装好Proxifier后还需要进行配置，打开软件，添加代理信息，选择：菜单栏–&gt;&gt;Profile–&gt;&gt;Proxy Servers。 选择add添加服务器，地址填127.0.0.1，端口一般为1080 配置完成在终端即可实现全局代理 远程DNS设置为了防止DNS污染，一般使用代理的时候都会使用远程服务器的DNS设置，具体设置方法是，菜单栏–&gt;&gt;Profile–&gt;&gt;Name Resolution。 然后选择“Resolve hostnames through proxy”即可。（一开始这个选项可能是灰色不能点，将默认的“Detect DNS seetings automatically”点掉即可） 至此，再次尝试在终端安装Homebrew即可。","link":"/2020/06/10/%E5%AE%89%E8%A3%85Homebrew%E9%81%87%E5%88%B0connection-refused%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E5%8F%8AProxifier%E9%85%8D%E7%BD%AE/"},{"title":"【Spring源码】深入理解Spring IoC","text":"一开始学习Spring的时候就接触IoC了，作为Spring第一个最核心的概念，我们在解读它的源码之前必须要对其有深入的认识，本篇主要介绍IoC基本概念和各个组件。 本文主要基于 Spring 5.0.6.RELEASE IoC介绍IoC全称为Inversion of Control即“控制反转”，它还有一个别名DI（Dependency Injection），即“依赖注入”。 如何理解“控制反转”？关键在于我们需要回答下面四个问题： 谁控制谁 控制什么 为什么是反转 哪些方面反转了 回答这四个问题前，我们先看IoC的定义： 所谓IoC，就是由Spring IoC容器来负责对象的生命周期和对象之间的关系。 这句话是IoC理论的核心，如何来理解这句话？我们引用一个例子来阐述。 找女朋友，一般情况下是如何找的呢？首先我们要根据自己的喜好（漂亮、身材好、性格好）找一个妹子，然后到处打听她的兴趣爱好、电话号码，然后各种投其所好追到手。如下： 123456789101112131415161718192021222324/** * 年轻小伙子 */public class YoungMan { private BeautifulGirl beautifulGirl; YoungMan(){ // 可能你比较厉害，指腹为婚 // beautifulGirl = new BeautifulGirl(); } public void setBeautifulGirl(BeautifulGirl beautifulGirl) { this.beautifulGirl = beautifulGirl; } public static void main(String[] args){ YoungMan you = new YoungMan(); BeautifulGirl beautifulGirl = new BeautifulGirl(\"你的各种条件\"); beautifulGirl.setxxx(\"各种投其所好\"); // 然后你有女票了 you.setBeautifulGirl(beautifulGirl); }} 这就是我们通常做事的方式，如果我们需要某个对象，一般都是采用这种直接创建的方式（new BeautifulGirl()），这个过程复杂而又繁琐，而且我们必须要面对每个环节，在使用完成之后还需要复杂地销毁它，这种情况下我们的对象与它所依赖的对象耦合在一起。 其实我们需要思考一个问题，我们每次用到自己依赖的对象真的需要自己去创建吗？我们知道，我们依赖的对象其实并不是依赖该对象本身，而是依赖它所提供的服务，只要在我们需要它的时候，它能够及时提供服务即可，至于它是我们主动去创建的还是别人送给我们的，其实并不是那么重要。 这个给我们送东西的“人”就是IoC，在上面的例子中，它就相当于一个婚介公司，作为一个婚介公司它管理者很多男男女女的资料，当我们需要一个女朋友的时候，直接跟婚介公司提出我们的需求，婚介公司则会根据我们的需求提供一个妹子给我们，我们只需要负责谈恋爱就行了。 作为婚介公司的IoC帮我们省略了找女朋友的繁琐过程，将原来的主动寻找变成了现在的被动接受，更加简介轻便。原来需要各种鞍前马后亲力亲为的事，现在直接有人把现成的送过来，多么美妙的事情啊。所以，IoC的理念就是让别人为你服务，如下图（摘自Spring揭秘）： 在没有引入IoC的时候，被引入的对象直接依赖于被依赖的对象，有了Ioc后，两者及他们的关系都是通过Ioc Service Provider来统一管理维护的。被注入的对象需要什么，直接跟IoC Service Provider打声招呼，后者就会把相应的被依赖对象注入到被注入的对象中，从而达到IoC Service Provider为被注入对象服务的目的。所以有了IoC，原来需要什么东西自己去拿，现在是需要什么让别人（IoC Service Provider）送过来。 现在看看上面那四个问题，答案就很明显了： 谁控制谁：在传统的开发模式下，我们都是采用直接new一个对象的方式来创建对象，也就是说你依赖的对象直接由你自己控制，但是有了IoC容器后，则直接由IoC容器来控制。所以“谁控制谁”，当然是IoC容器控制对象。 控制什么：控制对象 为什么是反转：没有IoC的时候我们都是在自己对象中主动区创建被依赖的对象，这就是正转。但是有了IoC后，所依赖的对象直接由IoC容器创建后注入到被注入的对象中，依赖的对象由原来的主动获取变成被动接受，所以是反转。 哪些方面反转了：所依赖对象的获取被反转了。 妹子有了，但是如何拥有妹子呢？这也是一门学问。 可能你比较厉害，刚出生就指腹为婚了 大多数情况我们还是会考虑自己想要什么样的妹子，所以还是需要向婚介公司打招呼的 还有一种情况就是，你根本不知道自己想要什么样的妹子，直接跟婚介公司说，我想要一个妹子 注入形式所以，IoC Service Provider为被注入对象提供被依赖对象也有如下几种方式：构造方法注入、setter方法注入、接口注入 构造方法注入顾名思义就是被注入的对象通过在其构造方法中声明依赖对象的参数列表，让外部知道它需要哪些依赖对象。 123YoungMan(BeautifulGirl beautifulGirl) { this.beautifulGirl = beautifulGirl;} 构造方法注入比较直观，对象构造完毕后就可以直接使用，这就好比你出生你家里就给你制定了媳妇 setter方法注入对于JavaBean对象而言，我们一般都是通过getter和setter方法来访问和设置对象的属性。所以，当前对象只需要为其所依赖的对象提供相对应的setter方法，就可以通过该方法将相应的依赖对象设置到被注入对象中。 12345678public class YoungMan { private BeautifulGirl beautifulGirl; public void setBeautifulGirl(BeautifulGirl beautifulGirl) { this.beautifulGirl = beautifulGirl; }} 相比于构造器注入，setter方式注入会显得比较宽松灵活一些，它可以在任何时候进行注入（当然是在使用依赖对象之前），这就好比你可以先把自己想要的妹子想好了，然后再跟婚介公司打招呼。 接口方式注入接口方式注入显得比较霸道，因为它需要被依赖的对象实现不必要的接口，带有侵入性。一般不推荐这种方式。感兴趣的可以看看《依赖注入的三种实现形式 —— 接口注入（Interface Injection）》 推荐文章关于 IoC 理论部分，笔者不在阐述，这里推荐几篇博客阅读： 《谈谈对 Spring IoC 的理解》 《Spring 的 IoC 原理[通俗解释一下]》 《Spring IoC 原理（看完后大家可以自己写一个spring）》 各个组件先看下图（摘自：http://singleant.iteye.com/blog/1177358） 该图为ClassPathXmlApplicationContext的类继承体系结构，虽然只有一部分，但是它基本上包含了IoC体系中大部分的核心类和接口。 下面我们就针对这个图进行简单的拆分和补充说明。 Resource 体系org.springframework.core.io.Resource，对资源的抽象。它的每一个实现类都代表了一种资源的访问策略，如ClassPathResource、RLResource、FileSystemResource 等。 ResourceLoader 体系有了资源，就应该有资源加载，Spring利用org.springframework.core.io.ResourceLoader来进行统一资源加载，类图如下： （关于Resource和ResourceLoader的源码解析，见…） BeanFactory 体系org.springframework.beans.factory.BeanFactory，是一个非常纯粹的bean容器，它是IoC必备的数据结构，其中BeanDefinition是它的基本结构。BeanFactory内部维护着一个BeanDefinition map，并可根据BeanDefinition的描述进行bean的创建和管理。 BeanFactory 有三个直接子类ListableBeanFactory、HierarchicalBeanFactory 和 AutowireCapableBeanFactory 。 DefaultListableBeanFactory 为最终默认实现，它实现了所有接口。 BeanDefinition 体系org.springframework.beans.factory.config.BeanDefinition ，用来描述 Spring 中的 Bean 对象。 BeanDefinitionReader 体系org.springframework.beans.factory.support.BeanDefinitionReader 的作用是读取 Spring 的配置文件的内容，并将其转换成 Ioc 容器内部的数据结构 ：BeanDefinition 。 关于BeanDefinitionReader 的源码解析，见如下文章： ApplicationContext 体系org.springframework.context.ApplicationContext ，这个就是大名鼎鼎的 Spring 容器，它叫做应用上下文，与我们应用息息相关。它继承 BeanFactory ，所以它是 BeanFactory 的扩展升级版，如果BeanFactory 是屌丝的话，那么 ApplicationContext 则是名副其实的高富帅。由于 ApplicationContext 的结构就决定了它与 BeanFactory 的不同，其主要区别有： 继承 org.springframework.context.MessageSource 接口，提供国际化的标准访问策略。 继承 org.springframework.context.ApplicationEventPublisher 接口，提供强大的事件机制。 扩展 ResourceLoader ，可以用来加载多种 Resource ，可以灵活访问不同的资源。 对 Web 应用的支持。 下图来源：https://blog.csdn.net/yujin753/article/details/47043143 小结上面的五个体系可以说是 Spring IoC 中最核心的部分，后面的博文也是针对这五个部分进行源码分析。其实IoC乍一看还是挺简单的，无非就是将配置文件（暂且认定是xml文件）进行解析，然后放到一个Map里面就差不多了。初看有道理，其实要面临的问题还是有很多的。 此系列博文为笔者学习、研究 Spring 机制和源码的学习笔记，会涉及参考别人的博文和书籍内容，如有雷同，纯属借鉴，当然 LZ 会标明参考来源。同时由于知识面和能力的问题，文章中难免会出现错误之处，如有，望各位大佬指出，不胜感激。 参考原创出处 http://cmsblogs.com/?p=2652 「小明哥」，略作修改及补充","link":"/2020/08/21/%E3%80%90Spring%E6%BA%90%E7%A0%81%E3%80%91%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Spring-IoC/"},{"title":"你应该如何正确健壮后端服务？","text":"对每一个程序员而言，故障都是悬在头上的达摩克利斯之剑，都唯恐避之不及，如何避免故障是每一个程序员都在苦苦追寻希望解决的问题。 对于这一问题，大家都可以从需求分析、架构设计 、代码编写、测试、code review、上线、线上服务运维等各个视角给出自己的答案。本人结合自己两年有限的互联网后端工作经验，从某几个视角谈谈自己对这一问题的理解，不足之处，望大家多多指出。 我们大部分服务都是如下的结构，既要给使用方使用，又依赖于他人提供的第三方服务，中间又穿插了各种业务、算法、数据等逻辑，这里面每一块都可能是故障的来源。如何避免故障？我用一句话概括，“怀疑第三方，防备使用方，做好自己”。 1. 怀疑第三方 坚持一条信念：“所有第三方服务都不可靠”，不管第三方什么天花乱坠的承诺。基于这样的信念，我们需要有以下行动。 1.1 有兜底，制定好业务降级方案 如果第三方服务挂掉怎么办？我们业务也跟着挂掉？显然这不是我们希望看到的结果，如果能制定好降级方案，那将大大提高服务的可靠性。举几个例子以便大家更好的理解。 比如我们做个性化推荐服务时，需要从用户中心获取用户的个性化数据，以便代入到模型里进行打分排序，但如果用户中心服务挂掉，我们获取不到数据了，那么就不推荐了？显然不行，我们可以在cache里放置一份热门商品以便兜底； 又比如做一个数据同步的服务，这个服务需要从第三方获取最新的数据并更新到mysql中，恰好第三方提供了两种方式：1）一种是消息通知服务，只发送变更后的数据；2）一种是http服务，需要我们自己主动调用获取数据。我们一开始选择消息同步的方式，因为实时性更高，但是之后就遭遇到消息迟迟发送不过来的问题，而且也没什么异常，等我们发现一天时间已过去，问题已然升级为故障。合理的方式应该两个同步方案都使用，消息方式用于实时更新，http主动同步方式定时触发（比如1小时）用于兜底，即使消息出了问题，通过主动同步也能保证一小时一更新。 有些时候第三方服务表面看起来正常，但是返回的数据是被污染的，这时还有什么方法兜底吗？有人说这个时候除了通知第三方快速恢复数据，基本只能干等了。举个例子，我们做移动端的检索服务，其中需要调用第三方接口获取数据来构建倒排索引，如果第三方数据出错，我们的索引也将出错，继而导致我们的检索服务筛选出错误的内容。第三方服务恢复数据最快要半小时，我们构建索引也需要半小时，即可能有超过1个多小时的时间检索服务将不能正常使用，这是不可接受的。如何兜底呢？我们采取的方法是每隔一段时间保存全量索引文件快照，一旦第三方数据源出现数据污染问题，我们先按下停止索引构建的开关，并快速回滚到早期正常的索引文件快照，这样尽管数据不是很新（可能1小时之前），但是至少能保证检索有结果，不至于对交易产生特别大的影响。 1.2 遵循快速失败原则，一定要设置超时时间 某服务调用的一个第三方接口正常响应时间是50ms，某天该第三方接口出现问题，大约有15%的请求响应时间超过2s，没过多久服务load飙高到10以上，响应时间也非常缓慢，即第三方服务将我们服务拖垮了。 为什么会被拖垮？没设置超时！我们采用的是同步调用方式，使用了一个线程池，该线程池里最大线程数设置了50，如果所有线程都在忙，多余的请求就放置在队列里中。如果第三方接口响应时间都是50ms左右，那么线程都能很快处理完自己手中的活，并接着处理下一个请求，但是不幸的是如果有一定比例的第三方接口响应时间为2s，那么最后这50个线程都将被拖住，队列将会堆积大量的请求，从而导致整体服务能力极大下降。 正确的做法是和第三方商量确定个较短的超时时间比如200ms，这样即使他们服务出现问题也不会对我们服务产生很大影响。 1.3 适当保护第三方，慎重选择重试机制 需要结合自己的业务以及异常来仔细斟酌是否使用重试机制。比如调用某第三方服务，报了个异常，有些同学就不管三七二十一就直接重试，这样是不对的，比如有些业务返回的异常表示业务逻辑出错，那么你怎么重试结果都是异常；又如有些异常是接口处理超时异常，这个时候就需要结合业务来判断了，有些时候重试往往会给后方服务造成更大压力，启到雪上加霜的效果。 2. 防备使用方 这里又要坚持一条信念：“所有的使用方都不靠谱”，不管使用方什么天花乱坠的保证。基于这样的信念，我们需要有以下行动。 2.1 设计一个好的api（RPC、Restful），避免误用 过去两年间看过不少故障，直接或间接原因来自于糟糕的接口。如果你的接口让很多人误用，那要好好反思自己的接口设计了，接口设计虽然看着简单，但是学问很深，建议大家好好看看Joshua Bloch的演讲《How to Design a Good API &amp; Why it Matters（如何设计一个好的API及为什么这很重要）》以及《Java API 设计清单》（原地址已无法访问，可以查看其他转载）。 下面简单谈谈我的经验。 a） 遵循接口最少暴露原则 使用方用多少接口我们就提供多少，因为提供的接口越多越容易出现乱用现象，言多必失嘛。此外接口暴露越多自己维护成本就越高。 b） 不要让使用方做接口可以做的事情 如果使用方需要调用我们接口多次才能进行一个完整的操作，那么这个接口设计就可能有问题。比如获取数据的接口，如果仅仅提供getData(int id);接口，那么使用方如果要一次性获取20个数据，它就需要循环遍历调用我们接口20次，不仅使用方性能很差，也无端增加了我们服务的压力，这时提供getDataList(List idList);接口显然是必要的。 c）避免长时间执行的接口 还是以获取数据方法为例：getDataList(List idList); 假设一个用户一次传1w个id进来，我们的服务估计没个几秒出不来结果，而且往往是超时的结果，用户怎么调用结果都是超时异常，那怎么办？限制长度，比如限制长度为100，即每次最多只能传100个id，这样就能避免长时间执行，如果用户传的id列表长度超过100就报异常。 加了这样限制后，必须要让使用方清晰地知道这个方法有此限制。之前就遇到误用的情况，某用户一个订单买了超过100个商品，该订单服务需要调用商品中心接口获取该订单下所有商品的信息，但是怎么调用都失败，而且异常也没打出什么有价值的信息，后来排查好久才得知是商品中心接口做了长度限制。 怎么才能做到加了限制，又不让用户误用呢？ 两种思路：1）接口帮用户做了分割调用操作，比如用户传了1w个id，接口内部分割成100个id列表（每个长度100），然后循环调用，这样对使用方屏蔽了内部机制，对使用方透明；2）让用户自己做分割，自己写循环显示调用，这样需要让用户知道我们方法做了限制，具体方法有：1）改变方法名，比如getDataListWithLimitLength(List idList); ；2）增加注释；3）如果长度超过 100，很明确地抛出异常，很直白地进行告知。 d）参数易用原则 避免参数长度太长，一般超过3个后就较难使用，那有人说了我参数就是这么多，那怎么办？写个参数类嘛！ 此外避免连续的同类型的参数，不然很容易误用。 能用其它类型如int等的尽量不要用String类型，这也是避免误用的方法。 e）异常 接口应当最真实的反应出执行中的问题，更不能用聪明的代码做某些特别处理。经常看到一些同学接口代码里一个try catch，不管内部抛了什么异常，捕获后返回空集合。 1234567public List&lt;Integer&gt; test() { try { ... } catch (Exception e) { return Collections.emptyList(); }} 这让使用方很无奈，很多时候不知道是自己参数传的问题，还是服务方内部的问题，而一旦未知就可能误用了。 2.2 流量控制，按服务分配流量，避免滥用 相信很多做过高并发服务的同学都碰到类似事件：某天A君突然发现自己的接口请求量突然涨到之前的10倍，没多久该接口几乎不可使用，并引发连锁反应导致整个系统崩溃。 为什么会涨10倍，难道是接口被外人攻击了，以我的经验看一般内部人“作案”可能性更大。之前还见过有同学mapreduce job调用线上服务，分分钟把服务搞死。 如何应对这种情况？生活给了我们答案：比如老式电闸都安装了保险丝，一旦有人使用超大功率的设备，保险丝就会烧断以保护各个电器不被强电流给烧坏。同理我们的接口也需要安装上“保险丝”，以防止非预期的请求对系统压力过大而引起的系统瘫痪，当流量过大时，可以采取拒绝或者引流等机制。具体限流算法参见《接口限流实践》一文。 3. 做好自己 做好自己是个非常大的话题，从需求分析、架构设计 、代码编写、测试、code review、上线、线上服务运维等阶段都可以重点展开介绍，这次简单分享下架构设计、代码编写上的几条经验原则。 3.1 单一职责原则 对于工作了两年以上的同学来说，设计模式应该好好看看，我觉得各种具体的设计模式其实并不重要，重要的是背后体现的原则。比如单一职责原则，在我们的需求分析、架构设计、编码等各个阶段都非常有指导意义。 在需求分析阶段，单一职责原则可以界定我们服务的边界，如果服务边界如果没界定清楚，各种合理的不合理的需求都接，最后导致服务出现不可维护、不可扩展、故障不断的悲哀结局。 对于架构来讲，单一职责也非常重要。比如读写模块放置在一起，导致读服务抖动非常厉害，如果读写分离那将大大提高读服务的稳定性（读写分离）；比如一个服务上同时包含了订单、搜索、推荐的接口，那么如果推荐出了问题可能影响订单的功能，那这个时候就可以将不同接口拆分为独立服务，并独立部署，这样一个出问题也不会影响其他服务（资源隔离）；又比如我们的图片服务使用独立域名、并放置到cdn上，与其它服务独立（动静分离）。 从代码角度上讲，一个类只干一件事情，如果你的类干了多个事情，就要考虑将他分开。这样做的好处是非常清晰，以后修改起来非常方便，对其它代码的影响就很小。再细粒度看类里的方法，一个方法也只干一个事情，即只有一个功能，如果干两件事情，那就把它分开，因为修改一个功能可能会影响到另一个功能。 3.2 控制资源的使用 写代码脑子一定要绷紧一根弦，认知到我们所在的机器资源是有限的。机器资源有哪些？cpu、内存、网络、磁盘等，如果不做好保护控制工作，一旦某一资源满负荷，很容易导致出现线上问题。 3.2.1 CPU资源怎么限制？a）计算算法优化 如果服务需要进行大量的计算，比如推荐排序服务，那么务必对你的计算算法进行优化。 b）锁 对于很多服务而言，没有那么多耗费计算资源的算法，但cpu使用率也很高，这个时候需要看看锁的使用情况，我的建议是如无必要，尽量不用显式使用锁。 c） 习惯问题 比如写循环的时候，千万要检查看看是否能正确退出，有些时候一不小心，在某些条件下就成为死循环，很著名的案例就是《多线程下HashMap的死循环问题》。比如集合遍历时候使用性能较差的遍历方式、String +检查，如果有超过多个String相加，是否使用StringBuffer.append？ d）尽量使用线程池 通过线程池来限制线程的数目，避免线程过多造成的线程上下文切换的开销。 e）jvm参数调优 jvm参数也会影响cpu的使用，如《发布或重启线上服务时抖动问题解决方案》。 3.2.2 内存资源怎么限制？a）Jvm参数设置 通过JVM参数的设置来限制内存使用，jvm参数调优比较靠经验。 b）初始化java集合类大小 使用java集合类的时候尽量初始化大小，在长连接服务等耗费内存资源的服务中这种优化非常重要； c）使用内存池/对象池 d）使用线程池的时候一定要设置队列的最大长度 之前看过好多起故障都是由于队列最大长度没有限制最后导致内存溢出。 e）如果数据较大避免使用本地缓存 如果数据量较大，可以考虑放置到分布式缓存如redis、tair等，不然gc都可能把自己服务卡死； f）对缓存数据进行压缩 比如之前做推荐相关服务时，需要保存用户偏好数据，如果直接保存可能有12G，后来采用短文本压缩算法直接压缩到6G，不过这时一定要考虑好压缩解压缩算法的cpu使用率、效率与压缩率的平衡，一些压缩率很高但是性能很差的算法，也不适合线上实时调用。 有些时候直接使用probuf来序列化之后保存，这样也能节省内存空间。 g）清楚第三方软件实现细节，精确调优 在使用第三方软件时，只有清楚细节后才知道怎么节约内存，这点我在实际工作中深有体会，比如之前在阅读过lucene的源码后发现我们的索引文件原来是可以压缩的，而这在说明文档中都找不到，具体参考《lucene索引文件大小优化小结》一文。 3.2.3 网络资源怎么限制？a）减少调用的次数 减少调用的次数？经常看到有同学在循环里用redis/tair的get，如果意识到这里面的网络开销的话就应该使用批量处理；又如在推荐服务中经常遇到要去多个地方去取数据，一般采用多线程并行去取数据，这个时候不仅耗费cpu资源，也耗费网络资源，一种在实际中常常采用的方法就是先将很多数据离线存储到一块 ，这时候线上服务只要一个请求就能将所有数据获取。 b）减少传输的数据量 一种方法是压缩后传输，还有一种就是按需传输，比如经常遇到的getData(int id)，如果我们返回该id对应的Data所有信息，一来人家不需要，二来数据量传输太大，这个时候可以改为getData(int id, List fields)，使用方传输相应的字段过来，服务端只返回使用方需要的字段即可。 3.2.4 磁盘资源怎么限制？ 打日志要控制量，并定期清理。1）只打印关键的异常日志；2）对日志大小进行监控报警。我有一次就遇到了第三方服务挂了，然后我这边就不断打印调用该第三方服务异常的日志，本来我的服务有降级方案，如果第三方服务挂了会自动使用其它服务，但是突然收到报警说我服务挂了，登上机器一看才知道是磁盘不够导致的崩溃；3）定期对日志进行清理，比如用crontab，每隔几天对日志进行清理；4）打印日志到远端，对于一些比较重要的日志可以直接将日志打印到远端HDFS文件系统里； 3.3 避免单点 不要把鸡蛋放在一个篮子上！从大层次上讲服务可以多机房部署、异地多活；从自己设计角度上讲，服务应该能做到水平扩展。 对于很多无状态的服务，通过nginx、zookeeper能轻松实现水平扩展； 对数据服务来说，怎么避免单点呢？简而言之、可以通过分片、分层等方式来实现，后面会有个博文总结。 4. 小结 如何避免故障？我的经验浓缩为一句：“怀疑第三方，防备使用方，做好自己”，大家也可以思考、总结并分享下自己的经验。 参考原创出处 https://www.cnblogs.com/LBSer/p/4753112.html 「zhanlijun」欢迎转载，保留摘要，谢谢！","link":"/2020/10/05/%E4%BD%A0%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%81%A5%E5%A3%AE%E5%90%8E%E7%AB%AF%E6%9C%8D%E5%8A%A1%EF%BC%9F/"}],"tags":[{"name":"数据挖掘与知识发现","slug":"数据挖掘与知识发现","link":"/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%8F%91%E7%8E%B0/"},{"name":"项目管理","slug":"项目管理","link":"/tags/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"name":"决策论与博弈论","slug":"决策论与博弈论","link":"/tags/%E5%86%B3%E7%AD%96%E8%AE%BA%E4%B8%8E%E5%8D%9A%E5%BC%88%E8%AE%BA/"},{"name":"网络与网页编程","slug":"网络与网页编程","link":"/tags/%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%BD%91%E9%A1%B5%E7%BC%96%E7%A8%8B/"},{"name":"Homebrew","slug":"Homebrew","link":"/tags/Homebrew/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"MacOS","slug":"MacOS","link":"/tags/MacOS/"},{"name":"Kafka","slug":"Kafka","link":"/tags/Kafka/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Spring","slug":"Spring","link":"/tags/Spring/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"工程师经验","slug":"工程师经验","link":"/tags/%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%BB%8F%E9%AA%8C/"}],"categories":[{"name":"课程笔记","slug":"课程笔记","link":"/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"},{"name":"工具安装及配置","slug":"工具安装及配置","link":"/categories/%E5%B7%A5%E5%85%B7%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE/"},{"name":"读书笔记","slug":"读书笔记","link":"/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"命令","slug":"命令","link":"/categories/%E5%91%BD%E4%BB%A4/"},{"name":"消息队列","slug":"消息队列","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"工具使用问题与解决方案","slug":"工具使用问题与解决方案","link":"/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"源码","slug":"源码","link":"/categories/%E6%BA%90%E7%A0%81/"},{"name":"黑科技","slug":"黑科技","link":"/categories/%E9%BB%91%E7%A7%91%E6%8A%80/"},{"name":"转载","slug":"转载","link":"/categories/%E8%BD%AC%E8%BD%BD/"}]}